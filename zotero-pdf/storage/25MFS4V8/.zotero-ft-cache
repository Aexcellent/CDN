PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

arXiv:2105.07612v1 [cs.CR] 17 May 2021

Xiaoyu Fan
fanxy20@mails.tsinghua.edu.cn IIIS Tsinghua University Beijing, China

Guosai Wang
wanggs@pbcsf.tsinghua.edu.cn PBC School of Finance, Tsinghua
University Beijing, China

Kun Chen
chenk@pbcsf.tsinghua.edu.cn PBC School of Finance, Tsinghua
University Beijing, China

Xu He
Larryhe@tsingj.com Huakong Tsingjiao Information
Technology Inc. Beijing, China

Wei Xu
weixu@tsinghua.edu.cn IIIS Tsinghua University
Beijing, China

ABSTRACT
Privacy-preserving data mining has become an important topic. People have built several multi-party-computation (MPC)-based frameworks to provide theoretically guaranteed privacy, the poor performance of real-world algorithms have always been a challenge. Using Principal Component Analysis (PCA) as an example, we show that by considering the unique performance characters of the MPC platform, we can design highly effective algorithm-level optimizations, such as replacing expensive operators and batching up. We achieve about 200Ã— performance boost over existing privacy-preserving PCA algorithms with the same level of privacy guarantee. Also, using real-world datasets, we show that by combining multi-party data, we can achieve better training results.
CCS CONCEPTS
â€¢ Security and privacy â†’ Usability in security and privacy.
KEYWORDS
Privapy-preserving, MPC, PCA, Sort, Eigen-decomposition
1 INTRODUCTION
Mining datasets distributed across many parties without leaking extra private information has become an important topic recently. Integrating data from multiple parties increases the overall training dataset and provides an opportunity to train on datasets with different distributions and dimensions. In fact, researchers have shown that by integrating data from multiple parties, we can improve model prediction accuracy [52], and even train models that we have not been able to [24].
However, privacy concerns have been a big hurdle in data integration. Privacy-preserving algorithms offer some promising solutions. Private information to protect goes beyond personal data (a.k.a. personal identifiable information, or PII) and includes other less obvious private information, such as data distribution, as adversaries may infer important business information from it. For example, [51, 53] show attacks in which the adversaries can obtain the information and defeat the purpose of privacy protection, just by analyzing the intermediate results without seeing the original

data. Note that these leaks depend on the data and model, and their security properties are sometimes hard to reason about.
Secure Multi-Party-Computation (MPC) [49] is a class of cryptographic techniques that allow multiple parties to jointly compute on their data without leaking any information other than the final results. Although it provides a theoretically promising solution, its performance is far from useful in analyzing large-scale datasets even using simple data mining algorithms.
Recently, people have proposed MPC frameworks that greatly improve the computation efficiency [10, 25, 31, 40]. Most importantly, these frameworks can theoretically support any algorithm on MPC by offering high-level programming languages, such as Python [25, 40]. However, performance remains a challenge for sophisticated algorithms, even for small datasets.
We believe there are three main challenges to solve the problem: Firstly, these MPC frameworks mainly focus on the performance of basic operations such as multiplications and comparisons, but the performance of data mining algorithms depends on complex operations such as division and square-root [18, 37, 45]. In MPC, multiplication and comparison require communications among the parties, so they are slower than addition. Also, people use numerical methods (such as Newtonâ€™s method) to implement complex operations, and thus the time cost can be orders-of-magnitude higher than multiplication. Table 1 in Section 3.2 shows the comparison among different operations in an MPC system. Secondly, using the provided high-level language, the naive way to implement algorithms is to directly port the algorithm designed for plain-text onto MPC platforms. The naive porting often leads to poor performance due to the huge performance gap among different basic operations. Also, there may be some asymptotic complexity differences in algorithms like sorting [22], as we have to hide the comparison results among elements. Last but not least, many of these frameworks rely on data partition and parallelism to provide large-scale data processing in a reasonable time, but some data mining algorithms, such as PCA, are non-trivial to partition into independent tasks. In this paper, using the principal component analysis (PCA) as an example, we demonstrate an end-to-end optimization of a data mining algorithm to run on the MPC framework. We choose to use PCA not only because it is a popular algorithm but also because it

pre-print, 2021,
involves several steps that require a variety of performance optimization techniques to illustrate an end-to-end data mining method. Although we implemented our algorithm on a single MPC framework, the optimization techniques and tradeoffs are independent of the specific framework.
The first step of PCA is to calculate the covariance matrix from raw data from the participating parties. The dataset can be horizontally partitioned (i.e., each party has the same feature space with different samples) or vertically partitioned (i.e., each party has the same sample space with different features). As the raw data can be large, it is important to avoid too many computations on cipher-text. We apply some transformations in covariance matrix computation to avoid encrypting all raw data by locally computing the partial results and only aggregate the partial results using MPC.
The next step is the core step of PCA, in which we perform eigen-decomposition on the covariance matrix. Previous work [2] provides an MPC-based approach to compute it on cipher-text, but the performance is unacceptable (even a 50 Ã— 50 matrix takes 126.7 minutes). The slow performance leads to proposals to reveal the covariance matrix as plain-text for decomposition, arguing that it does not contain private information [28]. However, as our analysis in Section 4.1 shows, the covariance matrix does leak much information about the distribution of each partyâ€™s dataset in many cases. Also, we need to sort on the eigenvalues to select the largest ğ¾ corresponding eigenvectors to construct the projection matrix. We carefully designed the decomposition and sort algorithms based on the MPC platformâ€™s performance characters, avoiding expensive operations and fully exploiting the opportunities to batch up operations, thus greatly reduced the computation time. In our work, we have improved performance by 200Ã— comparing with [2] on similar scale matrices and have achieved the entire PCA on the 7, 062, 606 Ã— 115 dataset from 9 parties within 3 minutes.
Using real-world datasets, we show that performing PCA from integrated datasets from multiple parties can benefit downstream tasks. We have adopted two real-world datasets for demonstration in Section 5.3.
In summary, there are four contributions of this paper:
â€¢ We designed an end-to-end privacy-preserving PCA algorithm implementation optimized for MPC frameworks;
â€¢ We proposed an optimized eigen-decomposition algorithm and sort algorithm fully taking into account the basic operation cost of MPC algorithms;
â€¢ We demonstrated that we could perform the entire PCA algorithm on a 7, 062, 606 Ã— 115 dataset within 3 minutes.
â€¢ Using real-world datasets, we show that integrating data from different parties can significantly improve the downstream modelsâ€™ quality.
2 RELATED WORK
We first compare different underlying techniques that achieve general privacy-preserving data mining and then review the studies on privacy-preserving PCA algorithms.
Privacy-preserving techniques. There are four popular lines to achieve privacy-preserving data mining: MPC, differential privacy (DP) [13], federated learning (FL) [46], and trusted execution environment (TEE) [41]. DP-based techniques introduce random noises to

Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, and Wei Xu
defend against differential queries or attacks, but the side effect is the precision loss of the computation results [13, 14]. FL exchanges intermediate results such as the gradients instead of the raw data among multiple data providers to reduce privacy leakage when training models [46]. For PCA, FL methods have to exchange parts of the raw data to compute the covariance matrix, and expose the matrix to decompose it, breaking the privacy requirement. TEE runs algorithms in an isolated secure area (a.k.a. enclave) in the processor [41], but its security depends on the hardware manufacturer, and sometimes is vulnerable to side-channel attacks [44].
MPC achieves secure computation using cryptographic techniques while performance remains a challenge. There are many MPC frameworks trying to improve basic MPC operation performance. However, we still need algorithm-level optimizations to make algorithms like PCA practical on large datasets.
Privacy-preserving algorithms. Researchers have studied various privacy-preserving data mining algorithms. Including privacypreserving classification [5, 12, 47], regression [33, 43] and unsupervised algorithms like K-Means[21] and EM clustering [26].
People have provided several privacy-preserving PCA designs too. [27] uses a well-designed perturbation to the original dataâ€™s covariance matrix to keep it secret. In addition to the inaccurate results, it does not provide a way to jointly compute the covariance matrix. [28] supports joining horizontally-partitioned raw data, but it exposes the covariance matrix to perform the eigendecomposition. As we will show in Section 4.1, the covariance matrix may leak important information about each partyâ€™s raw data. [2] handles horizontally-partitioned dataset and allows keeping the covariance matrix private. However, it takes 127.4 minutes even on a small dataset with only 50 features. We adopt the pre-computation techniques in Section 4.3 for the horizontally partitioned data, but provide orders-of-magnitude performance improvement.
Some studies focus on privacy-preserving matrix decomposition, the slowest step in PCA [19, 37]. [37] uses an MPC-based power iteration method to calculate only the larggest eigenvalue and eigenvector. [19] uses the QR algorithm to implement a privacypreserving SVD algorithm of input matrix ğ‘€ that can be written in the form ğµğµğ‘‡ or ğµğ‘‡ ğµ where ğµ is some matrix that is horizontally or vertically partitioned among exactly two parties. Also, it takes over 166.67 minutes to decompose a 50 Ã— 50 matrix.
Compared to previous studies, our method offers the following desirable properties: 1) keeping everything private, including the covariance matrix. 2) supporting any number of parties that hold the partitioned data, 3) supporting both horizontally and vertically partitioned dataset, and 4) running fast and outputting all the eigenvalues and eigenvectors.
3 BACKGROUND
In this section, we provide some background information. Specifically, we summarize important performance characteristics of MPC protocols, which leads to our optimization design.
3.1 Secure Multi-Party Computation (MPC)
MPC has a long history in the cryptography community. It enables a group of data owners to jointly compute a function without disclosing their data input.

PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

pre-print, 2021,

Basic MPC protocols. To achieve the generality of MPC, i.e., to support arbitrary functions, people usually start by designing basic operations using cryptographic protocols.
There are protocols focusing on supporting a single important operation. For example, oblivious transfer (OT) protocol allows a receiver ğ‘… to select one element from an array that the sender ğ‘† holds, without letting ğ‘† know which one she selects [15]. Secure multi-party shuffling (MPS) [32] allows parties to jointly permute their inputs without disclosing the input itself. The protocol is useful to implement functions like argsort.
There are also protocols allowing general MPC. Secret sharing (SS) protocol allows splitting a secret input ğ‘  âˆˆ R into ğ‘› pieces (ğ‘ 1, ğ‘ 2, ..., ğ‘ ğ‘›), and let each computation server to hold one of the pieces [3]. We can perform basic operation protocols over these shared secrets, such as multiplication and comparison, without allowing any of the parties to see plain-text data, under certain security assumptions [23]. Garbled Circuits (GC) [48] treats the function to compute as a look-up table, and each partyâ€™s input as keys addressing into the table using protocols like oblivious transfer. GC is proven to be general to any computation; however, its efficiency and scalability to a large number of parties remain a challenge. Thus, most practical frameworks are based on SS [6, 7].
Composed protocols. Although we can perform any computation using the MPC protocols like SS or GC in theory, one obstacle is that designing a more complex operation directly based on these protocols are both difficult to program and inefficient. Thus, most practical MPC frameworks only implement basic operations using these protocols, and then compose them to implement more complex functions [25, 31], just like running programs on an instruction set of a processor. The security guarantee of composing these operations are not obvious, but many of them are proven to be composable in cryptography and adopted in existing MPC frameworks [10, 25, 31].
Obviously, all these operations are slower than the plain-text version, mainly because 1) the data size in cipher-text is larger than plain-text; and 2) even basic operations, such as multiplication, involves one or more rounds of communications among the computation nodes [9, 10, 25]. These optimizations usually focus on single operations but not complex algorithms.
Practical MPC frameworks. As the computation involves multiple parties, people have built practical MPC frameworks to reduce the algorithm development and system management overhead. These frameworks provide a programming interface, allowing users to specify data mining algorithms on MPC using a high-level language for the computation platform to execute the composed protocols. A typical computation engine includes 1) modules that run at each data owner party to encrypt the data; 2) a set of MPC servers to execute the MPC protocols, and 3) a module to decrypt the results in the designated receiving party.
Many frameworks improve basic operation performance (throughput or latency, or both) by leveraging modern technology such as multi-core processors, dedicated hardware accelerators [36], low latency networks, as well as programming models like data and task parallelism.
As an example, PrivPy [25] is a MPC framework designed for data mining. It is based on the 2-out-of-4 secret sharing protocol,

Table 1: Time cost of basic operations

Input size add mul gt eq sqrt reciprocal

104

1/50 1 10 20 80

90

105

2/50 4 20 60 240

270

106

13/50 9 80 180 1600

1350

and it offers a Python programming interface with high-level data types like NumPy [35]. It treats each joint party as a client. Each client sends a secretly shared data to four calculation servers. Then the servers perform the SS-based privacy-preserving algorithms.
3.2 Operation Characteristics of MPC
The cipher-to-plain-text performance difference is still large, and several MPC frameworks are trying to improve it. In this paper, we only focus on the relative performance characteristics of different MPC operations that are not likely to change with framework optimizations yet have a big impact on algorithm design.
Using [25] as an example, we compare the relative time for each secure operations useful in PCA, normalizing to the time taken to compute 10K multiplications (MULs). Table 1 shows a summary. Actually, the relations among each operation are general across many MPC frameworks, and we will explain the main characteristics in the following.
1) Addition is almost free comparing to MUL, while comparison is over 10Ã— more expensive. This is because all compelling frameworks use protocols that avoid communication in addition, and only a single round of communication for MUL. For comparison, however, we need to perform many rounds of bit-wise operations, and the exact number depends on the design protocol. Normally 10+ rounds are required for practical MPC frameworks [4, 25, 34].
2) Non-linear mathematical functions are close to 100Ã— slower than MUL. Numerical method is a typical way to implement these functions. For example, We use Newton approximation to compute reci (reciprocal) and sqrt (square root), and we illustrate the algorithms in Appendix A. One observation is that these numerical methods require multiple iterations. As the input is in cipher-text, many early-termination optimizations in plain-text no longer apply, and thus these operations are much slower than MUL.
3) The speed-up from batch processing is significant. As most operations require communications, there is a fixed overhead to establish the connection, encode the data and generate keys. Thus it is important to batch-up operations to amortize this overhead. We observe the same speed-up of batch processing, as reported in [9, 10, 25, 31]. For example, in Table 1 we observe a speed-up of 9Ã— for EQ when we increase input batch size from 104 to 106.
These characteristics we observe in MPC are apparently different from the relative performance in plain-text, which leads to a different set of algorithm design choices.
4 PRIVACY-PRESERVING PCA DESIGN
We introduce the optimizations on the PCA algorithm in this section. We first formalize the privacy-preserving PCAâ€™s objective, and then describe the detailed design and highlight our optimizations.

pre-print, 2021,

4.1 Problem Definition

A group of ğ‘ data providers ğ‘1, ğ‘2, . . . , ğ‘ğ‘ jointly hold the original dataset, a matrix ğ‘‹ of size ğ‘› Ã— ğ‘‘ (ğ‘› samples of ğ‘‘ features). Each data

provider ğ‘ğ‘– holds some rows or some columns of ğ‘‹ , represented

by ğ‘‹ğ‘– . The groupâ€™s target is to collaboratively perform a principal

component analysis (PCA) and compute the projection matrix ğ‘ƒ of

size ğ‘‘ Ã— ğ¾, where 1 â‰¤ ğ¾ â‰¤ ğ‘› is an input to the algorithm, such that

the ğ‘–-th column of ğ‘ƒ is the eigenvector corresponding to the ğ‘–-th

largest

eigenvalues

of

the

covariance

matrix ğ¶

=

1 ğ‘›âˆ’1

ğ‘‹

ğ‘‡

ğ‘‹

,

where

ğ‘‹ğ‘– = ğ‘‹ğ‘– âˆ’ ğœ‡ğ‘–, 1 â‰¤ ğ‘– â‰¤ ğ‘‘.

Each party wants to protect her own original data, and meet

the following requirements: 1) No other party ğ‘ğ‘˜ (ğ‘˜ â‰  ğ‘–) and any computation server ğ¸ can infer the privacy of data ğ‘‹ğ‘– held by ğ‘ğ‘–

during the algorithm execution; 2) No party ğ‘ğ‘– and any computa-

tion server can infer the information about the covariance matrix

ğ¶; and 3) ğ‘ƒ is a correct projection matrix of ğ‘‹ under normal PCA

definition. Besides, the algorithm needs to run fast on a modern

MPC framework. These guarantees should hold with no extra secu-

rity assumptions other than those required by the underlying MPC

framework.

We emphasis that requirement 2) is necessary to achieve 1). Pre-

vious work [28] proposes to reveal ğ¶ to accelerate the computation.

We show that a plain-text ğ¶ can cause leaking information of ğ‘‹ . Given the covariance matrix ğ¶ âˆˆ Rğ‘‘Ã—ğ‘‘ :

ğ¶ğ‘–, ğ‘—

=

ğ‘›

1 âˆ’1

ğ‘›
âˆ‘ï¸ (ğ‘¥ ğ‘—ğ‘˜

âˆ’ ğ‘¥ ğ‘— )(ğ‘¥ğ‘–ğ‘˜

âˆ’ ğ‘¥ğ‘– ),

ğ‘–, ğ‘— = 1, 2..., ğ‘‘,

(1)

ğ‘˜ =1

where ğ‘– = ğ‘—, ğ¶ğ‘–,ğ‘– is the variance of the ğ‘–-th feature. Using a gradient descent with the objective function

min ğ½ = ||ğ‘‹ğ‘‡ ğ‘‹ âˆ’ (ğ‘› âˆ’ 1)2ğ¶ ||2 ,

(2)

ğ¹

ğ‘‹

we can obtain a number of candidate ğ‘‹ â€²s, all of them would produce the same covariance matrix ğ¶. When other information are available and depending on the distribution of the original dataset ğ‘‹ , an adversary can sometimes find out a ğ‘‹ â€² that is very close to ğ‘‹ , causing a failure to meet requirement 1).
We highlight the reason why we choose privacy-preserving PCA as an example to illustrate our optimization framework. 1) The PCA process contains many expensive steps like eigen-decomposition and sort, 2) local preprocess is an important optimization in both scalability and efficiency, 3) the optimized building blocks like eigendecomposition and sort are common in many other applications.

4.2 Method Overview
Figure 1 illustrates the four major steps.
Step 1) Covariance matrix construction. To reduce the amount of computation on cipher-text, we first let each party preprocess the data before she encrypts and sends out the data to compute the covariance matrix in the MPC framework. The computation depends on how the dataset is partitioned at each party, and we support both horizontal and vertical partitions.
Step 2) Eigen-decomposition. This is the most time consuming step. We use an optimized Jacobi method to perform the computation and avoid expensive operations on MPC framework.

Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, and Wei Xu

Step 3) Projection matrix construction. We need to choose the largest ğ¾ eigenvalues. To sort the eigenvalues fast, we developed a ğ‘ğ‘ğ‘¡ğ‘â„_ğ‘ ğ‘œğ‘Ÿğ‘¡ algorithm that significantly improved efficiency through batch operations.
Step 4) Inference. Optionally, we can keep the projection matrix in cipher-text, and perform downstream tasks like dimension reduction and anomaly detection without decrypting the data.

4.3 Covariance Matrix Construction
In the first step, we perform local preprocessing on each data owner, and jointly compute the covariance matrix, depending on the partation of the dataset.

Horizationally Partitioned Datasets. In this common case, each data provider ğ‘ğ‘– owns a subset of rows (i.e. samples) of the origional dataset ğ‘‹ , i.e.,

ğ‘‹=

ğ‘‹1, ğ‘‹2, ..., ğ‘‹ğ‘

ğ‘‡

âˆˆ

ğ‘›Ã—ğ‘‘
R,

(3)

where ğ‘‹ğ‘– âˆˆ Rğ‘›ğ‘– Ã—ğ‘‘ is the subset of rows of ğ‘ğ‘– of size ğ‘›ğ‘– . The key

optimization is to allow each party process as many samples as

possible locally to avoid expensive MPC operations for covariance

matrix computation. For ğ‘ parties, we let each party ğ‘ğ‘– locally

compute partial results ğ‘…ğ‘– =

ğ‘›ğ‘– ğ‘— =1

ğ‘¥

ğ‘—

ğ‘¥ğ‘‡
ğ‘—

and ğ‘£ğ‘–

=

ğ‘›ğ‘– ğ‘— =1

ğ‘¥

ğ‘—

.

Then

we

can jointly compute the covariance matrix of ğ‘‹ , ğ¶, using the same

method as [2]:

ğ¶

=

ğ‘›

1 âˆ’

1

ğ‘
âˆ‘ï¸ ğ‘…ğ‘–
ğ‘— =1

âˆ’

1 ğ‘›(ğ‘› âˆ’

ğ‘‡
ğ‘£ğ‘£ , 1)

(4)

where ğ‘£ =

ğ‘ ğ‘– =1

ğ‘£ğ‘–

and

ğ‘›

=

ğ‘ ğ‘– =1

ğ‘›ğ‘–

.

Note

that

the

data

we

need

to

compute on MPC platform for each party ğ‘ğ‘– reduces from ğ‘›ğ‘– Ã— ğ‘‘

(i.e., the dataset ğ‘‹ğ‘– ) to (ğ‘‘ Ã— ğ‘‘ + ğ‘‘) (i.e., ğ‘…ğ‘– and ğ‘£ğ‘– ), making the MPC

complexity independent of the sample size.

Vertically Partitioned Datasets. Each ğ‘ğ‘– owns some columns (or features) of the dataset ğ‘‹ . Typically a common id exists to align the data rows. As the covariance matrix computation requires the operations using different columns, we need to perform it on the MPC platform. If the id is from a small namespace of size ğ›¾ (i.e. ğ›¾ is close to number of samples ğ‘›), we map each sample to the namespace, filling missing samples with zeros. Thus, each ğ‘‹ğ‘– has dimension of ğ›¾ Ã— ğ‘‘ğ‘– . In the cases when ğ›¾ >> ğ‘›, we use a common cryptography protocol private set intersection (PSI) [11] to compute the joint dataset ğ‘‹ .
In either case, in order to scale the computation to support ğ›¾ or ğ‘› samples, we use data parallelism by partitioning the dataset into ğ‘€ separate pieces, and perform an MPC task independently on each piece before merging the result as

ğ‘€

âˆ‘ï¸

ğ¶

=

ğ‘‡
ğ‘‹ğ‘‹

=

ğ‘‡

ğ‘‹
ğ‘—

ğ‘‹ğ‘—,

(5)

ğ‘— =1

where each piece ğ‘‹ ğ‘—

has shape of

ğ‘› ğ‘€

Ã— ğ‘‘,

ğ‘› ğ‘€

âˆˆ

Z.

4.4 Eigen-decomposition
The eigen-decomposition is the most time-consuming step for PCA in both plain-text and cipher-text. In this section, we introduce how

PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

pre-print, 2021,

Figure 1: End-to-end Privacy-preserving PCAâ€™s framework

(a) Horizational

(b) Vertical

Figure 2: Data partitions and Local preprocess

we select the appropriate algorithm prototype and how to optimize it based on the characteristics of MPC frameworks.
Eigen-decomposition is a well-studied topic. There are three popular designs: Power Iterations, QR shift and Jacobiâ€™s method. Power Iteration [2, 37] is efficient, but it does not find all eigenvalues and eigenvectors and thus not suitable in our case. QR shift [17, 39], or QR decomposition with element shift for acceleration, based on householder reflection, is one of the most popular methods in the plain-text implementations, and previous privacy-preserving algorithms adopted this method [16, 19]. Jacobi method, with a parallel version, Parallel Jacobi [42], is based on givens rotation. It is promising because it allows vectorizing many operations, thus may be suitable in many vector-operation-friendly MPC platforms. We evaluate the latter two methods in this paper.
In both algorithms, the operations that caused the huge computational overhead is the iteratively orthogonal transformations, i.e., Householder Reflection and Givens Rotation. Appendix B shows the details of their computation. We introduce the privacy-preserving QR Shift and Vectorized Jacobi algorithm and show how to select the better algorithm in cipher-text next.
QR Shift is based on the householder reflection (HR) orthogonal transformations that finds an orthogonal matrix ğ» = ğ¼ âˆ’ 2ğ‘¤ğ‘¤ğ‘‡ . ğ» reflects the vector ğ‘¤ âˆˆ Rğ‘› to u âˆˆ Rğ‘›, where ğ‘¢0 = ||ğ‘¤ ||2 and ğ‘¢ğ‘– = 0, for 0 < ğ‘– < ğ‘›.
To compute QR Shift on a ğ‘‘ Ã— ğ‘‘ matrix, we first apply (ğ‘‘ âˆ’ 2) HRs to reduce the original matrix to tridiagonal and then perform QR transformations iteratively until it converges to diagonal. Each QR transformation requires (ğ‘‘ âˆ’ 1) HRs with a Rayleigh-quotientshift. Appendix C shows the algorithm for both plain-text and MPC frameworks.

A variation of QR shift, the DP QR shift uses divide-and-conquer to scale up the computation and thus popular on the plain-text platform. However, it requires very expensive operations on MPCs, such as full permutation and solving secular equations. Thus, we do not consider this DP approach.

Parallel Jacobi is based on givens rotation (GR) orthogonal trans-

formations

that

finds

the

orthogonal

matrix ğ‘„ğ‘˜ğ‘™

(ğœƒ )

=

{ğ‘ğ‘– ğ‘—

(ğœƒ

)

}ğ‘›
ğ‘–,

ğ‘—

=1,

1 â‰¤ ğ‘˜ < ğ‘™ â‰¤ ğ‘‘, where ğ‘ğ‘˜ğ‘˜ (ğœƒ ) = ğ‘ğ‘™ğ‘™ (ğœƒ ) = cos(ğœƒ ), ğ‘ğ‘˜ğ‘™ = âˆ’ sin(ğœƒ ), ğ‘ğ‘™ğ‘˜ =

sin(ğœƒ ), ğ‘ğ‘–ğ‘– = 1 for ğ‘– â‰  ğ‘˜, ğ‘™ with other elements all zero. For any real-

value

symmetric

matrix

ğ´

=

{ğ‘ğ‘–

ğ‘—

}ğ‘›
ğ‘–,

ğ‘—

=1

âˆˆ

Rğ‘‘Ã—ğ‘‘ ,

we

can

transform

the off-diagonal elememt ğ‘ğ‘˜ğ‘™ , ğ‘˜ < ğ‘™ to zero by rotation angle ğœƒ with

the cotangent value statisfying

cot(2ğœƒ )

=

|ğ‘ğ‘˜ğ‘˜

âˆ’ ğ‘ğ‘™ğ‘™ | .

(6)

2 âˆ— ğ‘ğ‘˜ğ‘™

Jacobiâ€™s Method produces a sequence ğ´ğ‘–, ğ‘– = 0, 1, ..., of orthogonally similar matrices for a given real symmetric matrix ğ´ = ğ´0 that converges to a diagonal matrix. We then compute the next matrix ğ´ğ‘–+1 iteratively with

ğ´ğ‘–+1

=

ğ‘‡
ğ‘„
ğ‘–

ğ´ğ‘– ğ‘„ğ‘– ,

(7)

where ğ‘„ğ‘– is the orthogonal matrix determined using GR with the

largest off-diagonal element of ğ´ğ‘– .

Given the largest off-diagonal element ğ‘ğ‘˜ğ‘™ , the rotation only affects the ğ‘˜-th and ğ‘™-th rows and columns. There is an inherent par-

allelism

property

of

Jacobiâ€™s

method

[42]:

at

most

âŒŠ

ğ‘› 2

âŒ‹

off-diagonal

elements in the upper triangle can be eliminated to zero together

in each round. There are two conditions to select elements in each

round to improve the performance. We illustrate the selection strat-

egy with the following 7 Ã— 7 matrix as an example. Elements with

same number are in the same batch for one iteration.

ï£®ğ‘0,0

ï£¹

ï£¯ ï£¯

3

ğ‘1,1

7

ï£º ï£º

ï£¯ ï£¯

6

2 ğ‘2,2

7

3

ï£º ï£º

ï£¯ ï£¯

2

5

1 ğ‘3,3 7

3

6

ï£º ï£º.

ï£¯ ï£¯

5

1

4

ğ‘4,4 6

2

ï£º ï£º

ï£¯ ï£¯

1

4

ğ‘5,5

5

ï£º ï£º

ï£¯ ï£¯

4

ï£º ğ‘6,6ï£º

ï£°

ï£»

We can aggregate the computation of the rotation matrix for a batch of elements. Here we use the first batch as an example, where ğ‘ğ‘–,ğ‘—

pre-print, 2021,

and ğ‘ ğ‘–,ğ‘— refers to cos ğœƒğ‘–,ğ‘— and sin ğœƒğ‘–,ğ‘— for the elements numbered 1.

ï£®ğ‘0,5 0 0

0

0 âˆ’ğ‘ 0,5 0ï£¹

ï£¯ ï£¯

0

ğ‘1,4

0

0 âˆ’ğ‘ 1,4 0 0ï£ºï£º

ï£¯ ï£¯

0

0 ğ‘2,3 âˆ’ğ‘ 2,3

0

ğ‘„1

=

ğ‘„11ğ‘„12ğ‘„13

=

ï£¯ ï£¯ ï£¯

0

0 ğ‘ 2,3 ğ‘2,3

0

ï£¯ 0 ğ‘ 1,4 0

0

ğ‘1,4

ï£¯

ï£¯ğ‘ 0,5 0 0

0

0

ï£¯

ï£¯0 0 0 0

0

0 0ï£ºï£º
0 0ï£ºï£º . 0 0ï£ºï£º
ï£º ğ‘0,5 0ï£º
ï£º 0 1ï£º

ï£°

ï£»

We leverage the parallel feature of Jacobi to implement inde-

pendent operations in each iteration as a single vector operation,

illustrated in Algorithm 1. Specifically, we first extract the indepen-

dent elements to an array, and then calculate the rotation angles

as a vector operation (Line 8). The algorithm converges for any

symmetric matâˆšrix, as there is a strong bound ğ‘† (ğ´â€²) â‰¤ ğ‘ğ‘† (ğ´) where

ğ‘ < 1, ğ‘† (ğ´) =

2 2

|

|ğ´

âˆ’ ğ‘‘ğ‘–ğ‘ğ‘”(ğ´)||ğ¹

and

â€²
ğ´

is

the

transformation

of

ğ´ after updating all off-diagonal elements [20].

We iterate until reaching the convergence threshold, 1ğ‘’ âˆ’ 5.

The threshold ensures each of the calculated eigenvalue ğœ†ğ‘– and eigenvector ğ‘£ğ‘– can achieve the accuracy of three significant numbers for ğ´ğ‘£ğ‘– == ğœ†ğ‘–ğ‘£ğ‘–, 0 < ğ‘– < ğ‘‘ for the input ğ´. The reason for choosing the mean value as the stoping criterion is to avoid accumulating the

accuracy error in cipher-text. On line 23, in order to check whether

the algorithm has converged, we reveal the comparison result (0

Algorithm 1: Vectorized Jacobiâ€™s method

Input : symmetric matrix ğ´ âˆˆ Rğ‘‘Ã—ğ‘‘ Output : The eigenvalues and eigenvectors of input ğ´

1

ğ‘€â†

âŒŠ

ğ‘‘ +1 2

âŒ‹

;

2 while True do

3 ğ‘– â†0;

4

ğ½ğ‘– â† ğ¼ğ‘‘ ;

5 ğ‘„ â† ğ¼ğ‘‘ ;

6 for i < 2*M do

7

ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğ¸ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘–) ;

8

ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  â† ğ´ [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] ;

9

ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ â† ğ´ [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ] âˆ’ ğ´ [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] ;

10

ğœ_ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ /(2 âˆ— ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ 2) ;

11

ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ â† ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  == 0;

12

ğ‘¡ğ‘ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ â† |ğœ_ğ‘™ğ‘–ğ‘ ğ‘¡ | + âˆšï¸(ğœ_ğ‘™ğ‘–ğ‘ ğ‘¡ 2 + 1) ;

13

ğ‘¡ğ‘ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ â† (1 âˆ’ ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ) âˆ— 1/ğ‘¡ğ‘ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ ;

14

ğ‘ğ‘œğ‘  _ğ‘™ ğ‘–ğ‘  ğ‘¡

â†

1/

(

âˆšï¸ ğ‘¡

ğ‘ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡

2

+

1)

;

15

ğ‘ ğ‘–ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğ‘¡ğ‘ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ âˆ— ğ‘ğ‘œğ‘ _ğ‘™ğ‘–ğ‘ ğ‘¡ ;

16

ğ½ğ‘– [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ], ğ½ğ‘– [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† ğ‘ğ‘œğ‘ _ğ‘™ğ‘–ğ‘ ğ‘¡ ;

17

ğ½ğ‘– [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† âˆ’ğ‘ ğ‘–ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ ;

18

ğ½ğ‘– [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† ğ‘ ğ‘–ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ ;

19

ğ´

â†

ğ½ğ‘‡
ğ‘–

ğ´

ğ½ğ‘–

;

20

ğ‘„ â† ğ½ğ‘‡ğ‘„ ;

ğ‘–

21

ğ‘–+ = 1;

/* Check for convergence every ğ‘Ÿ rounds.

*/

22

if Reveal(ğ‘šğ‘’ğ‘ğ‘› (ğ´ğ‘˜ğ‘˜,â‰ ğ‘™ğ‘™ ) < ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘) then

23

return ğ‘‘ğ‘–ğ‘ğ‘” (ğ´), ğ‘„;

24

end

25

end

26 end

Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, and Wei Xu
Table 2: EOs for Each Iteration

comp eq sqrt reciprocal

HR in QR Shift 1 Ã— (ğ‘‘ âˆ’ 1) 0 2 Ã— (ğ‘‘ âˆ’ 1) 1 Ã— (ğ‘‘ âˆ’ 1)

GR in Jacobi

1

1

2

3

Optimized Jacobi

2

1

2

1

Algorithm 2: Transformed Rotation Calculation
1 cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ â† |ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ |/(âˆšï¸4 âˆ— ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ 2 + ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ 2) ; 2 cos2 _ğ‘™ğ‘–ğ‘ ğ‘¡ â† 0.5 + 0.5 âˆ— cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ ; 3 sin2 _ğ‘™ğ‘–ğ‘ ğ‘¡ â† 0.5 âˆ’ 0.5 âˆ— cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ ; 4 ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ â† âˆšï¸[cos2 _ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘ ğ‘–ğ‘›2_ğ‘™ğ‘–ğ‘ ğ‘¡ ]; 5 cos _ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ [0] âˆ— (1 âˆ’ ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ) + ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ ; 6 sin _ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ [1] âˆ— ( (ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  âˆ— ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ > 0) âˆ— 2 âˆ’ 1);

or 1) to the computation servers running MPC protocol every ğ‘Ÿ iterations. The servers only gain a bit sequence ğ‘  = {0, 0, ..., 0, 1}, revealing the length of ğ‘ , |ğ‘  |. The length |ğ‘  | only roughly reveals the dimensions of the covariance matrix, which is already known to the servers. Thus, we do not think it is a privacy risk.
Performance Comparison between QR and Jacobi. The main computation cost for both algorithms in cipher-text comes from the total numbers of the expensive operations (EOs) based on the analysis in Section 3.2. The total number of EOs in QR shift and Jacobi is mainly determined by the per-iteration EO count and the number of iterations to convergence.
In each HR in QR shift, there are four EOs, including two sqrt, one reciprocal, and one comparison. In each iteration of QR shift, we need (ğ‘‘ âˆ’ 1) HRs (ğ‘‘ is the dimension of the covariance matrix), with vector dimension changing from ğ‘‘ to 1 sequentially. In each GR in Jacobi, there are at least three reciprocal, one comparison, one sqrt, and an additional equal to avoid the data overflow in MPC. The first two rows of Table 2 summarize EOs per iteration in each algorithm.
We explore the convergence iterations empirically and Section 5.2 summarizes the reaults. From the results, we observe that with the setting of our threshold, 1ğ‘’ âˆ’ 5, the convergence iterations of QR shift is ğ‘‚ (ğ‘‘ğ›¼ ) where 0.74 < ğ›¼ < 0.80, combining with the (ğ‘‘ âˆ’ 1) HR per iteration, the total number of EOs is around ğ‘‚ (ğ‘‘ğ›¼+1) which is higher than ğ‘‚ (ğ‘‘) on Jacobi. We want to point out that although Jacobi takes more iterations to converge, it is able to batch up more operations in each iteration. The overall performance is actually faster than QR shift.
EO-Reduction for Jacobi. We observe that we can further optimize Jacobi by replacing more expensive EOs with cheaper ones. As we discussed, reciprocal is over 10Ã— more expensive than comparison due to the numerical algorithm. We replace line 10 to 15 with the following code in Algorithm 2. The resulting algorithm reduces reciprocal per iteration from three to one, at the cost of one extra comparison. The last row in Table 2 summarizes the EOs in this algorithm.
In line 4 in Algorithm 2, we concatenate cos 2ğœƒ and sin 2ğœƒ vector to perform the sqrt operation as a single vector operation. Then

PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

pre-print, 2021,

Algorithm 3: Batch sort algorithm

Input : one-dimension column vector ğ‘¥ = (ğ‘¥0, ğ‘¥1, ..., ğ‘¥ğ‘›âˆ’1)ğ‘‡ Output : The sorted array ğ‘¥ with the argsort result ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥

1 ğ‘‹ â† ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘ğ‘¡ (ğ‘¥, ğ‘›) ;

2 ğ¶ â† ğ‘‹ğ‘‡ < ğ‘‹ ;

/* Comparision matrix */

3 ğ¸ â† (ğ‘‹ğ‘‡ == ğ‘‹ ) ;

/* Equal matrix */

4 ğ¸ â† (ğ‘ğ‘¢ğ‘šğ‘ ğ‘¢ğ‘š (ğ¸, ğ‘ğ‘¥ğ‘–ğ‘  = 0) == ğ¸) Ã— ğ¸ ;

5 ğµ â† (ğ‘ğ‘¢ğ‘šğ‘ ğ‘¢ğ‘š (ğ¸, ğ‘ğ‘¥ğ‘–ğ‘  = 0) âˆ’ ğ¸) Ã— ğ¸ ;

6 ğ‘‘ â† ğ‘ ğ‘¢ğ‘š (ğ¶, ğ‘ğ‘¥ğ‘–ğ‘  = 1) + ğ‘ ğ‘¢ğ‘š (ğµ, ğ‘ğ‘¥ğ‘–ğ‘  = 1) ; /* Sorted index */

7 ğ· â† ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘ğ‘¡ (ğ‘‘, ğ‘›) ;

8 ğ‘Ÿ â† ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (0, ğ‘›)ğ‘‡ ;

/* Range */

9 ğ‘… â† ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘ğ‘¡ (ğ‘Ÿ, ğ‘›) ;

10 ğ‘€ â† (ğ·ğ‘‡ == ğ‘…) ;

/* Mask matrix */

11 return ğ‘ ğ‘¢ğ‘š (ğ‘‹ğ‘‡ Ã— ğ‘€, ğ‘ğ‘¥ğ‘–ğ‘  = 1), d

on Line 5 and 6, we separate the cos ğœƒ and sin ğœƒ . This is beneficial given the observations of batching up in Table 1.
4.5 Projection Matrix and Inference
After the eigen-decomposition step, we sort all the eigenvalues to select the largest ğ¾ eigenvalues and the corresponding eigenvectors to construct the projection matrix ğ‘ƒ. The key step is to sort the eigenvalues for selection. As sorting requires many relatively expensive comparisonâ€™s, we want to batch up as much as possible. Thus, we design the batch_sort algorithm that combines ğ‘‚ (ğ‘‘2) comparisons into a single vector comparison of size ğ‘‘2. Note that it is a tradeoff between memory space and batch size. Using ğ‘‚ (ğ‘‘2) memory, we can greatly accelerate the comparison just by batching up. Algorithm 3 shows the batch_sort algorithm.
We use matrix ğ¸ in Algorithm 3 to handle elements with the same value. Lines 4 and 5 update the indices for each same-value element sequentially. Finally, we return the sorted array with its corresponding index in cipher-text. Note that during the entire process, we leak no information about the elements. All the position updates are achieved with the comparison result as an indicator array in cipher-text. We re-arrange each element with the result of argsort based on the MPS protocol in Section 3.1. Then we can take the largest ğ¾ eigenvalues and the corresponding eigenvectors to form the projection matrix ğ‘ƒ âˆˆ Rğ‘‘Ã—ğ¾ .
After computing the projection matrix ğ‘ƒ, we can perform a series of downstream tasks such as dimension reduction and anomaly detection, without decrypting ğ‘ƒ. The dimension reduction is a matrix multiplication of ğ‘‹ âˆˆ Rğ‘›Ã—ğ‘‘ and the projection matrix ğ‘ƒ, which can be done in MPC fast.
5 EVALUATION
We evaluate our design in two aspects, performance and effectiveness. In Section 5.1 and 5.2, we use several open datasets and synthetic data to demonstrate the performance of each step of the PCA process. We also use the real-world datasets to evaluate the effects when integrating the data from multiple parties.
We conduct all evaluations on a four-server PrivPy [25] deployment. Four independent servers are the minimal configuration for PrivPyâ€™s (4,2)-secret sharing scheme. All servers contain two 20-core 2.0 GHz Intel Xeon processors.

Table 3: Datasets in Evaluation and their running time (sec)

Name

Size

Cov Decomp Sort Inference

Wine [8]

6, 497 Ã— 11

0.56 2.27 0.04 0.01

Insurance [30]

9, 822 Ã— 85

2.97 56.05 0.11 0.07

Musk [1]

3, 062 Ã— 166 3.29 300.76 0.29 0.12

IoT-3 [29]

3, 803, 677 Ã— 115 3.59 136.50 0.16 0.01

IoT-5 [29]

5, 866, 616 Ã— 115 3.62 142.98 0.15 0.01

IoT-9 [29]

7, 062, 606 Ã— 115 3.64 142.80 0.17 0.01

MOOC-3 [50] 195, 177 Ã— 60 13.26 44.59 0.10 0.01

MOOC-7 [50] 195, 177 Ã— 140 31.55 295.03 0.23 0.02

MOOC-10 [50] 195, 177 Ã— 200 50.20 808.53 0.40 0.03

We have used several datasets for performance evaluation. The

first two columns in Table 3 summarizes the size of the datasets. We

choose the Wine dataset [8] because it is used in related work [28]

and others because they represent different data sizes and work-

loads for dimension reduction. The details of IoT [29] and MOOC [50]

datasetsâ€™s multi-party construation are in the followings. On each

of the dataset, we compared the explained variance ratio (EVR) with

plain-text PCA implementation in Scikit-learn [38] of the computed

principal components ğœ†ğ‘– , where ğ¸ğ‘‰ ğ‘…(ğœ†ğ‘– ) =

ğœ†ğ‘–
ğ‘‘

, 0 < ğ‘– â‰¤ 10, the

ğ‘—=1 ğœ†ğ‘—

mean precision error is within 1ğ‘’ âˆ’ 3, thus confirming the correct-

ness of our implementation.

5.1 Performance Overview
Micro-benchmarks. The first three rows of Table 3 summarizes the results of performance micro-benchmarks. In these benchmarks, we omit the time to send the data from multiple parties and just focus on the computation time. We report time cost in the four steps. The Cov step includes the time to encrypt the entire dataset and perform operations to compute the covariance matrix. We have the following observations:
1) Our method achieves reasonable time for the PCA task. In fact, our method is much faster than previously reported results. Comparing to [2] that does the decomposition in full cipher-text and takes 126.7 minutes on a 50 Ã— 50 matrix, our method achieves a 200Ã— speed-up on matrix with a similar scale. Even comparing to [28] that reveals the covariance matrix as plain-text, takes around three seconds on the same Wine dataset, and we can do everything in cipher-text within 5 seconds.
2) Matrix decomposition is the most time-consuming step. With the dimension of feature space ğ‘‘ increasing, the time for matrix decomposition is between ğ‘‚ (ğ‘‘2) and ğ‘‚ (ğ‘‘3). The reason for this cost is higher than ğ‘‚ (ğ‘‘2) is because iterations for convergence is also at least linear to ğ‘‘, which we will discuss in the next section. However, as we can aggressively batch up operations in Jacobi, we obtain a final execution time less than ğ‘‚ (ğ‘‘3).
Horizontally-partitioned IoT dataset. We use a larger scale dataset [29] with a dimension of 7, 062, 606 Ã— 115 to evaluate the scalability on horizontally partitioned dataset. We emulate horizontal partition by letting each IoT device be a separate party, and we need to allow each party to preprocess the data before combining them locally (Eq. 4.3 in Section 4.3). Row 4 to 6 in Table 3 shows the performance. The preprocessing time includes a local

pre-print, 2021,

(a) Iterations for QR shift

(b) Iterations for Parallel Jacobi

Figure 3: Convergence Illustration

preprocess time (same as discussed previously) of 0.76 seconds and a combination time on cipher-text of around 2.8 seconds.
Vertically-partitioned MOOCCube dataset. We use a derived dataset from MOOCCube [50] as an example of a vertically partitioned dataset. Each row represents a student (identified by an integer ID) and each column represents a concept (i.e., a topic in the course, such as â€œbinary treeâ€ in a programming course). We choose 200 concepts and separate them into ten courses. Treating these courses are from different institutions that cannot share the data, but we want to use the studentâ€™s learning record on all 200 concepts for analysis. Thus, it becomes a vertically partitioned situation. There are in total 195, 177 unique student IDs, and thus each party has a dataset with size 195, 177 Ã— 20.
The last three rows in Table 3 shows the performance. The Cov time contains the local arrangement which is around 0.21 seconds and the remaining is the combination time using MPC. The entire preprocessing time is much higher than other experiments as we have to encrypt all the samples from vertical partitions. Note that as more parties result in more dimensions, both decomposition and the preprocessing time increase with the number of vertically partitioned parties, as expected.
5.2 Analysis of Optimizations
Iterations to converge is a very important metric affecting performance. We experimentally evaluate the convergence iterations with different dimensions as well as data distributions using synthetic datasets for both QR shift and Jacobi repeating 50 times each with different random seeds. Figure 3 shows the results. We observe that 1) For both algorithms, the number of iteration mainly depends on the matrix dimension ğ‘‘, where QR shift is about ğ‘‚ (ğ‘‘ğ›¼ ) where 0.74 < ğ›¼ < 0.8 and Jacobi is about ğ‘‚ (ğ‘‘). 2) The data distribution can affect the convergence at some level. One reason causing this phenomenon is because our threshold in Algorithm 1 is relatively looser than the plain-text version to avoid accumulating the accuracy error. Thus matrices with smaller off-diagonal elements converge earlier than those with larger elements.
Jacobiâ€™s method vs. QR shift. The first two rows in Table4 shows the comparison between QR Shift and Jacobi on cipher-text. This is because each iteration of QR uses (ğ‘‘ âˆ’ 1) HRs that need to run sequentially without the benefit of batching up, which is consistent with the analysis in Section 4.4.
Benefits of EO Reduction. We have compared the time for the same matrix decomposition before and after the EO reduction in

Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, and Wei Xu
Table 4: Time consumption(s) comparison

50

70

100

120

QR Shift

325.49 648.03 1367.63 1981.08

Jacobi

48.53 95.51 180.17 253.35

Jacobi w/ EO-reduction 36.01 71.63 148.14 215.82

Table 5: Benefits for data integration

IoT

MOOC

N=1 N=3 N=5 N=9 N=1 N=3 N=7 N=10

Precision 0.79 0.80 0.99 1.0 0.74 0.79 0.80 0.87 Recall 0.74 0.92 0.98 1.0 0.81 0.82 0.83 0.83 f1-score 0.71 0.89 0.99 1.0 0.73 0.74 0.77 0.77

the last two rows of Table 4. Replacing 2 reciprocal with one comp in each iteration, we show that we can obtain a 18% performance gain.
5.3 Benefits of Data Integration
Back to the motivation why we need computing PCA with multiparty datasets, using the horizontally partitioned IoT [29] and vertically partitioned MOOC [50] datasets, we show that joint PCA does improve downstream task performance.
We perform a similar task on both datasets: first, we use our privacy-preserving PCA method to reduce the feature dimension to ğ¾. Then we train a classifier to perform the classification task on the ğ¾-dimension feature matrix and compare the precision, recall and F1-score. Table 5 summarizes results on both datasets.
Horizontally partitioned IoT dataset. Recall that the dataset is partitioned into 9 different IoT devices, each with the same 115 feature dimensions. The classification task is to determine whether the device is under attack by botnet gafgyt, miari or not, the hyperparameter of ğ¾ = 20 and the classifier is AdaBoost. Table 5 shows that integrating data from more parties (i.e., device types) significantly boosts the downstream classification performance, because actually many attacks happen on either Ennio_doorbell or Samsung_Webcam, without using data from these parties, the PCA algorithm fails to pickup feature dimensions required to capture the miari botnet.
Vertically partitioned MOOC dataset. We show the effects of vertically partitioned data integration using the MOOCCube dataset[50]. Each course constructs the feature space with 20 dimensions using assigned concepts. The prediction task we constructed is to predict whether a student will enroll in some courses (top 400 who has more relations with the selected concepts). The hyperparameter ğ¾ = 7 and the classifier here is GradientBoosting. Table 5 shows that with related features expanded, the model can gain better performance.
6 CONCLUSION AND FUTURE WORK
Privacy has become a major concern in data mining, and MPC seems to provide a technically sound solution to the privacy problem. However, existing basic-operation-level optimizations in MPC still do not provide sufficient performance for complex algorithms. Using PCA as an example, we are among the first work to show that

PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

pre-print, 2021,

by carefully choosing the algorithm (e.g., Jacobi vs. QR), replacing
individual operations based on the MPC performance characteris-
tics (e.g. replacing reciprocal with comparison), and batching up as
much as possible (e.g., batch_sort), we can provide an algorithmlevel performance boost, running 200Ã— faster over existing work
with similar privacy guarantee.
As future work, we will verify our approach on more MPC plat-
forms, especially those with different basic operator performance,
and expand the methodology to other data mining algorithms.
REFERENCES
[1] Charu C Aggarwal and Saket Sathe. 2015. Theoretical foundations and algorithms for outlier ensembles. Acm sigkdd explorations newsletter 17, 1 (2015), 24â€“47.
[2] Mohammad Al-Rubaie, Pei-yuan Wu, J Morris Chang, and Sun-Yuan Kung. 2017. Privacy-preserving PCA on horizontally-partitioned data. In 2017 IEEE Conference on Dependable and Secure Computing. IEEE, 280â€“287.
[3] Amos Beimel and Benny Chor. 1993. Interaction in key distribution schemes. In Annual International Cryptology Conference. Springer, 444â€“455.
[4] Dan Bogdanov, Sven Laur, and Jan Willemson. 2008. Sharemind: A framework for fast privacy-preserving computations. In European Symposium on Research in Computer Security. Springer, 192â€“206.
[5] HervÃ© Chabanne, Amaury de Wargny, Jonathan Milgram, Constance Morel, and Emmanuel Prouff. 2017. Privacy-Preserving Classification on Deep Neural Network. IACR Cryptol. ePrint Arch. (2017).
[6] Chaochao Chen, Liang Li, Bingzhe Wu, Cheng Hong, Li Wang, and Jun Zhou. 2020. Secure social recommendation based on secret sharing. arXiv preprint arXiv:2002.02088 (2020).
[7] Chaochao Chen, Jun Zhou, Bingzhe Wu, Wenjing Fang, Li Wang, Yuan Qi, and Xiaolin Zheng. 2020. Practical Privacy Preserving POI Recommendation. ACM Trans. Intell. Syst. Technol., Article 52 (July 2020).
[8] Paulo Cortez, AntÃ³nio Cerdeira, Fernando Almeida, Telmo Matos, and JosÃ© Reis. 2009. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems (2009).
[9] Ivan DamgÃ¥rd, Valerio Pastro, Nigel Smart, and Sarah Zakarias. 2012. Multiparty computation from somewhat homomorphic encryption. In Annual Cryptology Conference. Springer, 643â€“662.
[10] Daniel Demmler, Thomas Schneider, and Michael Zohner. 2015. ABY-A framework for efficient mixed-protocol secure two-party computation. In NDSS.
[11] Changyu Dong, Liqun Chen, and Zikai Wen. 2013. When private set intersection meets big data: an efficient and scalable protocol. In Proceedings of the 2013 ACM SIGSAC. 789â€“800.
[12] Wenliang Du, Yunghsiang S Han, and Shigang Chen. 2004. Privacy-preserving multivariate statistical analysis: Linear regression and classification. In Proceedings of the 2004 SDM. SIAM, 222â€“233.
[13] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science 9 (2014).
[14] Arik Friedman and Assaf Schuster. 2010. Data Mining with Differential Privacy. In Proceedings of the 16th ACM SIGKDD.
[15] Oded Goldreich, Silvio Micali, and Avi Wigderson. 2019. Proofs that yield nothing but their validity and a methodology of cryptographic protocol design. In Providing Sound Foundations for Cryptography: On the Work of Shafi Goldwasser and Silvio Micali. 285â€“306.
[16] Andreas Grammenos, Rodrigo Mendoza Smith, Jon Crowcroft, and Cecilia Mascolo. 2020. Federated Principal Component Analysis. Advances in NeurIPS 33 (2020), 989â€“997.
[17] A Greenbaum and Jack J Dongarra. 1989. Experiments with QR/QL methods for the symmetric tridiagonal eigenproblem. University of Tennessee. Computer Science Department.
[18] Shuguo Han and Wee Keong Ng. 2008. Privacy-preserving linear fisher discriminant analysis. In PAKDD. Springer, 136â€“147.
[19] S. Han, W. K. Ng, and P. S. Yu. 2009. Privacy-Preserving Singular Value Decomposition. In 2009 IEEE 25th ICDE. 1267â€“1270.
[20] Vjeran Hari and Erna Begovic. 2016. Convergence of the cyclic and quasi-cyclic block Jacobi methods. arXiv preprint arXiv:1604.05825 (2016).
[21] Geetha Jagannathan and Rebecca N Wright. 2005. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data. In Proceedings of the eleventh ACM SIGKDD. 593â€“599.
[22] KristjÃ¡n Valur JÃ³nsson, Gunnar Kreitz, and Misbah Uddin. 2011. Secure MultiParty Sorting and Applications. IACR Cryptol. ePrint Arch. 2011 (2011).
[23] Hugo Krawczyk. 1994. Secret Sharing Made Short. In Advances in Cryptology â€” CRYPTOâ€™ 93, Douglas R. Stinson (Ed.). Springer Berlin Heidelberg, 136â€“146.
[24] Maurizio Lenzerini. 2002. Data Integration: A Theoretical Perspective (PODS â€™02). Association for Computing Machinery, 122.

[25] Yi Li and Wei Xu. 2019. PrivPy: General and scalable privacy-preserving data mining. In Proceedings of the 25th ACM SIGKDD. 1299â€“1307.
[26] Xiaodong Lin, Chris Clifton, and Michael Zhu. 2005. Privacy-preserving clustering with distributed EM mixture modeling. Knowledge and information systems (2005).
[27] X. Liu, Y. Lin, Q. Liu, and X. Yao. 2018. A Privacy-Preserving Principal Component Analysis Outsourcing Framework. In 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications.
[28] Yingting Liu, Chaochao Chen, Longfei Zheng, Li Wang, Jun Zhou, and Guiquan Liu. 2020. Privacy preserving pca for multiparty modeling. arXiv preprint arXiv:2002.02091 (2020).
[29] Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Asaf Shabtai, Dominik Breitenbacher, and Yuval Elovici. 2018. N-baiotâ€”network-based detection of iot botnet attacks using deep autoencoders. IEEE Pervasive Computing (2018).
[30] Lingjun Meng, Peter van der Putten, and Haiyang Wang. 2005. A comprehensive benchmark of the artificial immune recognition system (AIRS). In International Conference on Advanced Data Mining and Applications. Springer.
[31] Payman Mohassel and Yupeng Zhang. 2017. Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE S&P. IEEE, 19â€“38.
[32] Mahnush Movahedi, Jared Saia, and Mahdi Zamani. 2015. Secure multi-party shuffling. In International Colloquium on Structural Information and Communication Complexity. Springer, 459â€“473.
[33] Valeria Nikolaenko, Stratis Ioannidis, Udi Weinsberg, Marc Joye, Nina Taft, and Dan Boneh. 2013. Privacy-preserving matrix factorization. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security. 801â€“812.
[34] Takashi Nishide and Kazuo Ohta. 2007. Multiparty computation for interval, equality, and comparison without bit-decomposition protocol. In International Workshop on Public Key Cryptography. Springer, 343â€“360.
[35] Travis E Oliphant. 2006. A guide to NumPy. Vol. 1. Trelgol Publishing USA. [36] Amos R Omondi and Jagath Chandana Rajapakse. 2006. FPGA implementations
of neural networks. [37] Manas Pathak and Bhiksha Raj. 2010. Privacy preserving protocols for eigenvector
computation. In International Workshop on Privacy and Security Issues in Data Mining and Machine Learning. Springer, 113â€“126. [38] Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the Journal of machine Learning research 12 (2011), 2825â€“2830. [39] Jeffery D Rutter. 1994. A serial implementation of Cuppenâ€™s divide and conquer algorithm for the symmetric eigenvalue problem. (1994). [40] Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, and Jonathan Passerat-Palmbach. 2018. A generic framework for privacy preserving deep learning. arXiv preprint arXiv:1811.04017 (2018). [41] M. Sabt, M. Achemlal, and A. Bouabdallah. 2015. Trusted Execution Environment: What It is, and What It is Not. In 2015 IEEE Trustcom/BigDataSE/ISPA. [42] Ahmed H. Sameh. 1971. On Jacobi and Jacobi-Like Algorithms for a Parallel Computer. Math. Comp. 25, 115 (1971). [43] Ashish P Sanil, Alan F Karr, Xiaodong Lin, and Jerome P Reiter. 2004. Privacy preserving regression modelling via distributed computation. In Proceedings of the tenth ACM SIGKDD. [44] Ming-Wei Shih, Sangho Lee, Taesoo Kim, and Marcus Peinado. 2017. T-SGX: Eradicating Controlled-Channel Attacks Against Enclave Programs.. In NDSS. [45] Jaideep Vaidya and Chris Clifton. 2003. Privacy-preserving k-means clustering over vertically partitioned data. In Proceedings of the ninth ACM SIGKDD. [46] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications. ACM TIST 10, 2 (2019), 1â€“19. [47] Zhiqiang Yang, Sheng Zhong, and Rebecca N Wright. 2005. Privacy-preserving classification of customer data without loss of accuracy. In Proceedings of the 2005 SDM. SIAM, 92â€“102. [48] Andrew C Yao. 1982. Protocols for secure computations. In 23rd annual symposium on foundations of computer science (sfcs 1982). IEEE, 160â€“164. [49] A. C. Yao. 1986. How to generate and exchange secrets. In 27th Annual Symposium on Foundations of Computer Science (sfcs 1986). 162â€“167. [50] Jifan Yu, Gan Luo, Tong Xiao, Qingyang Zhong, Yuquan Wang, Junyi Luo, Chenyu Wang, Lei Hou, Juanzi Li, Zhiyuan Liu, et al. 2020. MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 575â€“582. [51] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. idlg: Improved deep leakage from gradients. arXiv preprint arXiv:2001.02610 (2020). [52] Longfei Zheng, Chaochao Chen, Yingting Liu, Bingzhe Wu, Xibin Wu, Li Wang, Lei Wang, Jun Zhou, and Shuang Yang. 2020. Industrial scale privacy preserving deep neural network. arXiv preprint arXiv:2003.05198 (2020). [53] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep Leakage from Gradients. In Advances in NeurIPS. Curran Associates, Inc.

pre-print, 2021,

A PRIVACY-PRESERVING SQRT AND RECIPROCAL OPERATIONS
The calculation for sqrt and reciprocal is based on Newton-Raphson iterations, here we have designed some transformations to accelerate the convergence rate.

A.1 Privacy-preserving Sqrt

The square root of value ğ´ can be estimated through the solution

of

function

ğ‘“

(ğ‘¥ )

=

1 ğ‘¥2

âˆ’ğ´

=

0,

where the solution ğ‘¥

â†’

âˆš1 .
ğ´

The

iteration of ğ‘¥ was:

ğ‘¥0 â† ğ‘ğ‘ğ‘ğ‘Ÿğ‘œ,

ğ‘¥ğ‘˜ +1

=

1 2

ğ‘¥ğ‘˜

(3

âˆ’

ğ´ğ‘¥ 2
ğ‘˜

),

(8)

The ğ‘ğ‘ğ‘ğ‘Ÿğ‘œ we choose here equals 0.48. It is estimated through

âˆšï¸ƒ

the expection of âˆš1 for ğ´ âˆ¼ ğ‘ˆ [1, 10] while less than 3 which the

ğ´

ğ´

convergence condition. In our algorithm, we will scale the ğ‘ğ‘ğ‘ğ‘Ÿğ‘œ

for input value ğ´ not in [1, 10]. From experiments, we can get

convergence with around 7 itertaions with less than 1ğ‘’ âˆ’ 14 errors.

A.2 Privacy-preserving Reciprocal
The reciprocal of input value ğ´ can be estimated through the solution of function ğ‘“ (ğ‘¥) = 1 âˆ’ ğ´ = 0. Here we using the second-order
ğ‘¥
Newton-Raphson approximation with iterations reduced. The update equation of ğ‘¥ was:

ğ‘¥0 â† ğ‘ğ‘ğ‘ğ‘Ÿğ‘œ,

ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜ + ğ‘¥ğ‘˜ (1 âˆ’ ğ´ğ‘¥ğ‘˜ ) + ğ‘¥ğ‘˜ (1 âˆ’ ğ´ğ‘¥ğ‘˜ )2,

(9)

The initial value is defined through a wide range of possible val-
ues. We will firstly compare the input value ğ´ with vector a = [10âˆ’11, ...1, ..., 1012] and select the nearest estimation reciprocal in [1011, ...1, ..., 10âˆ’12] with the encrypted {0, 1} arrays through Obliv-
ious Transfer protocol. From experiments, we can get convergence
with around 6 iterations ess than 1ğ‘’ âˆ’ 14 errors.

B ORTHOGONAL TRANSFORMATIONS
Here we introduce the details of two orthogonal transformations where we can see the number of EOs each for Section 4.4. The sign function requires one time comp.

Orthogonal Transformation 4: Householder Reflection
Input : vector ğ‘¤ âˆˆ Rğ‘› Output : Orthogonal matrix ğ‘ƒ âˆˆ Rğ‘›Ã—ğ‘› where ğ‘ƒ ğ‘¤ = ğ‘˜ğ‘’, ğ‘˜ = | |ğ‘¤ | |2
1 Function Householder(w) : P is 2 ğ‘¢â†ğ‘¤; 3 ğ‘¢ [1] â† ğ‘¢ [1] + ğ‘ ğ‘–ğ‘”ğ‘› (ğ‘¢ [1]) | |ğ‘¢ | |2 ; 4 ğ‘¢ â† ğ‘¢/ | |ğ‘¢ | |2 ; 5 ğ‘ƒ â† ğ¼ğ‘› âˆ’ 2 âˆ— ğ‘¢ğ‘¢ğ‘‡ ; 6 return P 7 end

Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, and Wei Xu

Orthogonal Transformation 5: Givens Rotation
Input : symmetric matrix ğ´ âˆˆ Rğ‘›Ã—ğ‘› and index ğ‘˜, ğ‘™ Output : Orthogonal matrix ğ½ğ‘˜ğ‘™ âˆˆ Rğ‘›Ã—ğ‘› which can turn elements
ğ‘ğ‘˜ğ‘™ and ğ‘ğ‘™,ğ‘˜ to zero

1 Function GivensRotate(A, k, l) : ğ½ğ‘˜ğ‘™ is

2

ğ½ğ‘˜ğ‘™ â† ğ¼ğ‘› ;

3 ğœ â† (ğ´ [ğ‘˜, ğ‘˜ ] âˆ’ ğ´ [ğ‘™,âˆšğ‘™ ])/(2 âˆ— ğ´ [ğ‘˜, ğ‘™ ]) ;

4

ğ‘¡ â† ğ‘ ğ‘–ğ‘”âˆšğ‘› (ğœ)/( |ğœ | + 1 + ğœ2) ;

5 ğ‘ â† 1/ 1 + ğœ2 ;

6 ğ‘  â† ğ‘¡ğ‘ ;

7 ğ½ğ‘˜ğ‘™ [ğ‘˜, ğ‘™ ] â† âˆ’ğ‘ ; ğ½ğ‘˜ğ‘™ [ğ‘™, ğ‘˜ ] â† ğ‘  ;

8 ğ½ğ‘˜ğ‘™ [ğ‘˜, ğ‘˜ ], ğ½ğ‘˜ğ‘™ [ğ‘™, ğ‘™ ] â† ğ‘ ;

9 return ğ½ğ‘˜ğ‘™

10 end

C PRIVACY-PRESERVING QR SHIFT ALGORITHM
The implementation for our cipher-text QR shift algorithm including two phase and based on householder reflection, first is to reduce the origional ğ‘ Ã— ğ‘ symmetric matrix to tradtional form implementing thourgh ğ‘ âˆ’ 2 step householder reflections; second is to reduce the tridiagonal matrix into diagonal using QR decomposition with Rayleigh shift.

Algorithm 6: QR shift with Rayleigh quotient

Input : ğ‘ Ã— ğ‘ symmetric encrypted matrix ğ´ Output : Upper tradtional ğ‘‡ with ğ‘„ such that ğ´ = ğ‘„ğ‘‡ ğ‘‡ ğ‘„ where
ğ‘„0 is an orthogonal matrix

1 ğ´0 â† ğ´ ; ğ‘„ â† ğ¼ğ‘ ;

2 for ğ‘– â† 0 to ğ‘ âˆ’ 2 do

3 ğ‘¢ğ‘– â† ğ´ğ‘– [ğ‘– + 1 : ğ‘ , ğ‘˜ ] ;

4 ğ‘ƒğ‘– = ğ¼ğ‘ ;

5 Pi [i + 1 : N, i + 1 : N] = Householder(ui)

6 ğ‘„ [ğ‘– + 1 : ğ‘ , ğ‘– + 1 : ğ‘ ] â† ğ‘ƒğ‘–ğ‘„ [ğ‘– + 1 : ğ‘ , ğ‘– + 1 : ğ‘ ];

7

ğ´ğ‘– +1

â†

ğ‘ƒğ‘–

ğ´ğ‘–

ğ‘ƒğ‘‡
ğ‘–

;

8 end

9 ğ‘‡ â† ğ´ğ‘ âˆ’1 ;

D PRIVACY-PRESERVING SVD BASED ON VECTORIZED JACOBIâ€™S METHOD
Just as we mentioned in section4.4, based on One-Sided Jacobi, we can using the same optimization to implement the privacypreserving SVD, showin in Algorithm 8.

PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation (MPC)

pre-print, 2021,

Algorithm 7: QR shift - Second Phase: Rayleigh quotient shift QR decomposition

Input : Tridiagonal matrix ğ‘‡ and orthogonal ğ‘„ returned from

algorithm1

Output : eigenvalues and eigenvectors of ğ‘‡ which is the same as ğ´

1 Function QR(A) : Q, R where A = QR is 2 ğ‘„ â† ğ¼ğ‘ ;

3 ğ‘… â† ğ‘ğ‘œğ‘ğ‘¦ (ğ´) ;

4 for ğ‘– â† 0 to ğ‘ âˆ’ 1 do

5

ğ‘¢ğ‘– â† ğ‘… [ğ‘– :, ğ‘– ] ;

6

ğ‘ƒğ‘– = ğ¼ğ‘ ;

7

Pi [i : N, i : N] = Householder(ui) ;

8

ğ‘… â† ğ‘ƒğ‘– ğ‘… ;

9

ğ‘„ â† ğ‘„ğ‘ƒğ‘– ;

10

end

11 end

Algorithm 8: Privacy-preserving vectirized SVD

Input : real-value matrix ğº âˆˆ Rğ‘šÃ—ğ‘› Output : The singular values Î£ with the left singular vector matrix

ğ‘ˆ and the right singular vector matrix ğ‘‰

1

ğ‘€â†

âŒŠ

ğ‘›+1 2

âŒ‹

;

2 while True do

3 ğ‘– â† 0, ğ½ğ‘– â† ğ¼ğ‘› ;

4 ğ‘„ â† ğ¼ğ‘›, ğ´ â† ğºğ‘‡ ğº;

5 for i < 2*M do

6

ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğ¸ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘–) ;

7

ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  â† ğ´ [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] ;

8

ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ â† ğ´ [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ] âˆ’ ğ´ [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] ;

9

cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ â†

|ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ |/(âˆšï¸4 âˆ— ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ 2 + ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ 2) ;

10

cos2 _ğ‘™ğ‘–ğ‘ ğ‘¡ â† 0.5 + 0.5 âˆ— cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ ;

11

sin2 _ğ‘™ğ‘–ğ‘ ğ‘¡ â† 0.5 âˆ’ 0.5 âˆ— cos 2ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ ;

12

ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ â† âˆšï¸[cos2 _ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘ ğ‘–ğ‘›2_ğ‘™ğ‘–ğ‘ ğ‘¡ ];

13

cos _ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ [0] âˆ— (1 âˆ’ ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ) + ğ‘“ ğ‘™ğ‘ğ‘”_ğ‘§ğ‘’ğ‘Ÿğ‘œ ;

14

sin _ğ‘™ğ‘–ğ‘ ğ‘¡ â† ğœƒ _ğ‘™ğ‘–ğ‘ ğ‘¡ [1] âˆ— ( (ğ‘¡ğ‘ğ‘Ÿ _ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  âˆ— ğ‘¡ğ‘ğ‘Ÿ _ğ‘‘ğ‘– ğ‘“ ğ‘“ >

0) âˆ— 2 âˆ’ 1);

15

ğ½ğ‘– [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ], ğ½ğ‘– [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† ğ‘ğ‘œğ‘ _ğ‘™ğ‘–ğ‘ ğ‘¡ ;

16

ğ½ğ‘– [ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† âˆ’ğ‘ ğ‘–ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ ;

17

ğ½ğ‘– [ğ‘™_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ ] â† ğ‘ ğ‘–ğ‘›_ğ‘™ğ‘–ğ‘ ğ‘¡ ;

18

ğº â† ğº ğ½ğ‘– ;

19

ğ‘„ â† ğ‘„ ğ½ğ‘– ; /* for the right singular vector */

20

ğ‘–+ = 1 ;

/* Check for convergence every ğ‘Ÿ rounds

*/

21

if Reveal(ğ‘šğ‘’ğ‘ğ‘› (ğ´ğ‘˜ğ‘˜,â‰ ğ‘™ğ‘™ ) < ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘) then

22

Î£ â† [ğœ1, ..., ğœğ‘› ] ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğœğ‘– = | |ğº [:, ğ‘– ] | |2 ;

23

ğ‘ˆ â† [ğ‘¢1, ...ğ‘¢ğ‘› ] ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘¢ğ‘– = ğº [:, ğ‘– ]/ğœğ‘– ;

24

ğ‘‰ â†ğ‘„;

25

return Î£, ğ‘ˆ , ğ‘‰

26

end

27 end

28 end

