Personalized Federated Learning: A Meta-Learning Approach
Alireza Fallah∗, Aryan Mokhtari†, Asuman Ozdaglar∗

arXiv:2002.07948v1 [cs.LG] 19 Feb 2020

Abstract
The goal of federated learning is to design algorithms in which several agents communicate with a central node, in a privacy-protecting manner, to minimize the average of their loss functions. In this approach, each node not only shares the required computational budget but also has access to a larger data set, which improves the quality of the resulting model. However, this method only develops a common output for all the agents, and therefore, does not adapt the model to each user data. This is an important missing feature especially given the heterogeneity of the underlying data distribution for various agents. In this paper, we study a personalized variant of the federated learning in which our goal is to ﬁnd a shared initial model in a distributed manner that can be slightly updated by either a current or a new user by performing one or a few steps of gradient descent with respect to its own loss function. This approach keeps all the beneﬁts of the federated learning architecture while leading to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we propose a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is aﬀected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.

1 Introduction

In Federated Learning (FL), we consider a network of n users that are all connected to a central node (i.e., a star connectivity graph) where each user has access only to its local data (Konečny` et al., 2016). In this setting, the goal of the users is to come up with a model that is trained over all the data points in the network without exchanging their local data with other users or the central node, i.e., the server, due to privacy issues or communication limitations.
More formally, the classic FL setting studies a star-shaped network with n users and one server, and they all coordinate to solve the following optimization problem:

1n

min
w∈Rd

f (w)

:=

n

i=1

fi(w),

(1)

where fi : Rd → R denotes the loss function corresponding to user i. In particular, consider a

supervised learning application, where fi represents expected loss over the data distribution of user

i, i.e.,

fi(w) := E(x,y)∼pi [li(w; x, y)] ,

(2)

where li(w; x, y) measures the error of model w in predicting the true label y ∈ Yi given the input x ∈ Xi, and pi is the distribution over Xi × Yi. We would like to emphasize that in this paper we study the case that the probability distribution pi of users in the network are not identical and we face a heterogeneous data probability distribution.
To illustrate this formulation, as an example, consider the problem of training a Natural Language Processing (NLP) model over the devices of a set of users. In this problem, pi represenrts the

∗Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA. {afallah@mit.edu, asuman@mit.edu}.
†Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA. mokhtari@austin.utexas.edu.

1

empirical distribution of words and expressions used by user i, and hence, fi(w) can be expressed as

fi(w) =

pi(x, y)li(w; x, y),

(3)

(x,y)∈Si

where Si is the data set corresponding to user i and pi(x, y) is the probability that user i assigns to a speciﬁc word which is proportional to the frequency of using this word by user i.
In most algorithms designed for FL, the problem in (1) is solved in multiple rounds, where at each round the center sends the current model to a fraction of the users and those users update the model with respect to their own loss functions, usually by performing a few steps of a gradient-based method. Then, these users return their updated models to the center, and the center combines the received models to update the global model (for example by averaging, as in FedAvg Algorithm (McMahan et al., 2017a)) and sends the updated model to a (possibly diﬀerent) fraction of the users for the next round. This way, the computational power of all the users in the network are used to train the global model. In addition, the shared model is trained over a larger data set which could lead to a better model. Indeed, this approach leads to a model that solves the problem in (1) and the resulted solution w∗ performs well over all users on average.
Closeness of data distributions of users is crucial for the success of the federated learning framework. However, it is not necessarily the case that the data samples of all users are drawn from a common underlying distribution. This heterogeneity leads to an issue with formulation (1) in that the resulting model is only good on average and it does not take into account the heterogeneity of data distribution of users. In other words, the solution of problem (1) is not personalized for each user. To better highlight this point, recall the NLP example above, where although the distribution over the words and expressions varies from one person to another, the solution to problem (1) only provides a shared answer for all users, and therefore, it is not fully capable of achieving a user-adapted model.
Hence, in the setting that the underlying distribution of data points of the users are not identical, solving the average problem deﬁned in (1) could lead to poor local performance for each user. In this paper, we overcome this issue by considering a new problem formulation. We further introduce an eﬃcient method for solving the proposed formulation and characterize its convergence properties. A detailed list of our contributions follows:

1. We consider a modiﬁed formulation of the federated learning problem which incorporates personalization (Section 2). Building on the Model-Agnostic Meta-Learning (MAML) problem formulation introduced by Finn et al. (2017), the goal of our formulation is to ﬁnd an initial point shared between all users which performs well after each user updates it with respect to its own loss function, potentially by performing a few steps of a gradient-based method. This way, while the initial model is derived in a distributed manner over the whole network (same as the classic FL setting), the ﬁnal model implemented by each user diﬀers from other ones based on his or her own data.
2. We also propose a Personalized variant of the FedAvg algorithm, called Per-FedAvg, designed for solving the proposed personalized FL problem (Section 3). In particular, we elaborate on its connections with the original FedAvg algorithm (McMahan et al., 2017a), and also, discuss a number of considerations that one need to take into account for implementing Per-FedAvg.
3. We study the convergence properties of the proposed Per-FedAvg algorithm for solving nonconvex loss functions in terms of the objective function gradient norm (Section 4). In particular, we characterize the role of data heterogeneity and closeness of data distribution of diﬀerent users, measured by distribution distances, such as Total Variation (TV) or 1Wasserstein, on convergence of Per-FedAvg method.

1.1 Related Work
As mentioned earlier, McMahan et al. (2017a) proposed the FedAvg algorithm, where the global model is updated by averaging local SGD updates. Later, Guha et al. (2019) proposed oneshot Federated Learning (FL) in which the master node learns the model after a single round of communication. Also, several approaches have been used to address the communication limitations in FL. This includes quantization and compression ideas (Reisizadeh et al., 2019; Dai et al., 2019) as well as performing multiple local updates before communicating with the master (Stich, 2018; Lin

2

et al., 2018; Wang and Joshi, 2018). Several works have studied the problem of preserving privacy in federated learning (Duchi et al., 2014; McMahan et al., 2017b; Agarwal et al., 2018; Zhu et al., 2019). More related to our paper, there are several works that study statistical heterogeneity of users’ data points in FL (Zhao et al., 2018; Sahu et al., 2018; Karimireddy et al., 2019; Haddadpour and Mahdavi, 2019; Khaled et al., 2019; Li et al., 2019), but they do not attempt to ﬁnd a personalized solution for each user. In addition, Smith et al. (2017) used multi-task learning framework and proposed a new method, MOCHA, to address these statistical and systems challenges (including data heterogeneity as well as communication eﬃciency).
The idea of personalization in FL and its connections with meta-learning has recently gained attention in a number of papers. Khodak et al. (2019) proposed ARUBA, a meta-learning algorithm inspired by online convex optimization, and showed how applying it to FedAvg method improves its performance empirically. Jiang et al. (2019) proposed a personalized FedAvg algorithm in which the classic FedAvg is ﬁrst deployed, and then they switch to Reptile, a meta-learning algorithm proposed in (Nichol et al., 2018), and ﬁnally run local updates to achieve personalization. Note that this approach is diﬀerent from our proposed framework, as in this paper we do not perform the classic FedAvg and instead we look for a good initial point which performs well after it is ﬁne-tuned for each user. Moreover, Chen et al. (2018) focused on recommendation systems and proposed a meta-federated learning framework in which a parameterized meta-algorithm is used to train parameterized recommendation models and both meta-algorithm and local models’ parameters need to be optimized. For the special case that the meta-algorithm parameter is its initialization, this framework reduces to our formulation. The authors evaluated the success of this framework empirically over various data sets and by taking diﬀerent meta-algorithms. However, in our work, we speciﬁcally focus on the case that the meta-algorithm parameter is the initial point, and characterize its convergence theoretically, and highlight the role of diﬀerent parameters including heterogeneity of data distributions. We further provide empirical results for our proposed method. For a detailed survey on the connections of FL and multi-task and meta-learning check Section 3.3 of (Kairouz et al., 2019).

2 Personalized Federated Learning via Model-Agnostic MetaLearning (MAML)

As we stated in Section 1, our goal in this section is to show how the fundamental idea behind

the Model-Agnostic Meta-Learning (MAML) framework in (Finn et al., 2017) can be exploited to

design a personalized variant of the FL problem. To do so, let us ﬁrst brieﬂy recap the MAML

formulation. Given a set of tasks drawn from an underlying distribution, in MAML, in contrast

to the traditional supervised learning setting, the goal is not ﬁnding a model which performs well

on all the tasks in expectation. Instead, in MAML, we assume we have a limited computational

budget to update our model after a new task arrives, and in this new setting, we look for an

initialization which performs well after it is updated with respect to this new task, possibly by one

or a few steps of gradient descent. In particular, if we assume each user takes the initial point and

updates it using one step of gradient descent with respect to its own loss function, then problem

(1) changes to

1n

min
w∈Rd

F (w)

:=

n

i=1

fi(w

−

α∇fi(w))

(4)

where α ≥ 0 is the learning rate (stepsize). The strength of this formulation is that, not only it allows us to maintain the advantages of FL (limited communication), but also it captures the diﬀerence between users as either existing or new users can take the solution of this new problem as an initial point and slightly update it with respect to their own data. Going back to the NLP example (3), this means that each users i could take this resulting initialization and update it by going over her/his own data Si and performing just one or few steps of gradient descent to obtain a model that works well for her/his own dataset.
As we mentioned earlier, for the considered heterogeneous model of data distribution, solving problem (1) is not the ideal choice as it returns a single model that even after a few steps of local gradient may not quickly adjust to each users local data, but by solving (4) we ﬁnd an initial model (Meta-model) which is trained in a way that after one step of local gradient leads to a good model for each individual user. Indeed, this formulation can also be extended to the case that each

3

user runs a few steps of gradient update, but to simplify our notation we only focus on the single gradient update case.
The centralized version of this formulation was ﬁrst proposed by Finn et al. (2017) and followed by a number of papers studying its empirical characteristics (Antoniou et al., 2019; Li et al., 2017; Grant et al., 2018; Nichol et al., 2018; Zintgraf et al., 2019; Behl et al., 2019) as well as its convergence properties (Fallah et al., 2019). In this work, we focus on exploiting the MAML formulation to introduce a personalized solution for the federated learning setting. The analysis of the proposed algorithm for the FL setting is more challenging than the centralized case as we discuss in Section 4.

3 Personalized FedAvg

In this section, we introduce our proposed Personalized FedAvg method for solving problem (4).

This algorithm is inspired by the FedAvg algorithm originally proposed for the classic federated

learning problem (1), but it has been modiﬁed in a way that the resulting method ﬁnds the optimal

solution of (4) instead of (1). To better highlight this connection, let us recap the main steps of the

FedAvg algorithm. In FedAvg, at each round, server chooses a fraction of users with size rn (with

1 ≥ r > 0) and sends its current model to these users. Each selected user i updates this model

according to its own loss function fi and by running τ ≥ 1 steps of stochastic gradient descent.

Then, the users return their updated models to the server. Finally, the server updates the global

model by computing the average of the models received from these selected users, and then the

next round follows.

The proposed personalized FedAvg method follows the same principle and it aims to implement

a similar algorithm for minimizing the function F deﬁned in (4). Before formally stating the update

of personalized FedAvg let us mention that the global objective function F in (4) can be written

as the average of meta-functions F1, . . . , Fn where the meta-function Fi associated with user i is

deﬁned as

Fi(w) := fi(w − α∇fi(w)).

(5)

In other words, in this case, each local function is deﬁned as the value of the local loss function after running one step of gradient descent.
To follow a similar scheme as FedAvg for solving problem (4), the ﬁrst step is to compute the gradient of local functions, which in this case, the gradient ∇Fi, that is given by

∇Fi(w) = I − α∇2fi(w) ∇fi(w − α∇fi(w)).

(6)

Note that, computing the exact gradient ∇fi(w) at every round is not usually computationally tractable, and we therefore, take a batch of data Di with respect to distribution pi to obtain an unbiased estimate ∇˜ fi(w, Di) given by

∇˜ fi(w, Di)

:=

1 |Di|

∇li(w; x, y).

(7)

(x,y)∈Di

Similarly, we could replace the Hessian ∇2fi(w) in (6) by its unbiased estimate ∇˜ 2fi(w, Di) over the batch Di.

At round k of Personalized FedAvg algorithm, similar to FedAvg, ﬁrst the server sends the

current global model wk to a fraction of users Ak chosen uniformly at random with size rn. Each

user i ∈ Ak performs τ steps of stochastic gradient descent locally and with respect to Fi. In

particular, these local updates generates a local sequence {wki +1,t}τt=0 where wki +1,0 = wk and, for

τ ≥ t ≥ 1,

wki +1,t = wki +1,t−1 − β∇˜ Fi(wki +1,t−1)

(8)

where β is the local learning rate (stepsize) and ∇˜ Fi(wki +1,t−1) is an estimate of ∇Fi(wki +1,t−1) in (6). Note that the stochastic gradient ∇˜ Fi(wki +1,t−1) for all local iterates is computed using
independent batches Dti, Dti, and Dt i as follows

∇˜ Fi(wki +1,t−1) := I − α∇˜ 2fi(wki +1,t−1, Dt i) ∇˜ fi wki +1,t−1 − α∇˜ fi(wki +1,t−1, Dti), Dti . (9)

4

Algorithm 1: The proposed Personalized FedAvg (Per-FedAvg) Algorithm

Input:Initial iterate w0, fraction of active users r. for k : 0 to K − 1 do

Server chooses a subset of users Ak uniformly at random and with size rn;
Server sends wk to all users in Ak;
for all i ∈ Ak do Set wki +1,0 = wk; for t : 1 to τ do Compute the stochastic gradient ∇˜ fi(wki +1,t−1, Dti) using dataset Dti; Set w˜ki +1,t = wki +1,t−1 − α∇˜ fi(wki +1,t−1, Dti); Set wki +1,t = wki +1,t−1 − β(I − α∇˜ 2fi(wki +1,t−1, Dt i))∇˜ fi(w˜ki +1,t, Dti) using Dti and
Dt i; end for

Agent i sends wki +1,τ back to server; end for

Server

updates

its

model

by

averaging

over

received

models:

wk+1

=

1 rn

i∈Ak wki +1,τ ;

end for

We would like to emphasize that ∇˜ Fi(wki +1,t−1) is a biased estimator of ∇Fi(wki +1,t−1) due to the fact that ∇˜ fi(wki +1,t−1 − α∇˜ fi(wki +1,t−1, Dti), Dti) is a stochastic gradient that contains another stochastic gradient inside.
Once, the local updates are evaluated, all users send their updated models wki +1,τ to the server, and the server updates its global model by averaging over the received models, i.e.,

1 wk+1 = rn

wki +1,τ .

i∈Ak

(10)

These steps are depicted in Algorithm 1. Note that as in other MAML Algorithms (Finn et al.,

2017; Fallah et al., 2019), the update in (8) which exploits the stochastic gradient estimation in

(9) can be implemented in two levels: (i) First for each user i and each iteration t we perform the

following update

w˜ki +1,t = wki +1,t−1 − α∇˜ fi(wki +1,t−1, Dti)

and then evaluate wki +1,t by following the update

wki +1,t = wki,+t−11 − β(I − α∇˜ 2fi(wki +1,t−1, Dt i))∇˜ fi(w˜ki +1,t, Dti).

Indeed, it can be veriﬁed the outcome of the these two steps is equivalent to the update in (8). To simplify the notation, throughout the paper, we assume that the size of Dti, Dti, and Dt i is equal to D, D , and D , respectively, and for any i and t.

4 Theoretical Results
In this section, we study the convergence properties of our proposed Personalized FedAvg (PerFedAvg) method. We focus on nonconvex settings, and characterize the overall communication rounds between server and users for achieving ﬁrst-order stationarity. To do so, we ﬁrst formally deﬁne the notion of an -approximate ﬁrst-order stationary point.
4.1 Deﬁnitions and Assumptions
Deﬁnition 4.1. A random vector w ∈ Rd is called an -approximate First-Order Stationary Point (FOSP) for problem (4) if it satisﬁes
E[ ∇F (w ) ] ≤ .
Next, we formally state the assumptions required for proving our main results.

5

Assumption 1. Function F is bounded below, i.e., minw∈Rd F (w) > −∞.

Assumption 2. For every 1 ≤ i ≤ n, fi is twice continuously diﬀerentiable and Li-smooth, and also, its gradient is bounded by a nonnegative constant Bi, i.e.,

∇fi(w) ≤ Bi ∀w ∈ Rd, ∇fi(w) − ∇fi(u) ≤ Li w − u ∀w, u ∈ Rd.

(11a) (11b)

It is worth noting that (11b) also implies that fi satisﬁes the following conditions for all w, u ∈ Rd:

− LiId ∇2fi(w) LiId,

| fi(w) − fi(u) − ∇fi(u)

(w − u)| ≤ Li 2

w − u 2.

(12a) (12b)

As we discussed in Section 3, the second-order derivative of all functions appears in the update rule of Per-FedAvg Algorithm. Hence, in the next Assumption, we impose a regularity condition on the Hessian of each fi which is also a customary assumption in the analysis of second-order methods.

Assumption 3. For every 1 ≤ i ≤ n, the Hessian of function fi is ρi-Lipschitz continuous, i.e.,

∇2fi(w) − ∇2fi(u) ≤ ρi w − u ∀w, u ∈ Rd.

(13)

To simplify the analysis, in the rest of the paper, we deﬁne B := maxi Bi, L := maxi Li, and ρ := maxi ρi which can be, respectively, considered as a bound on the norm of gradient of fi, smoothness parameter of fi, and Lipschitz continuity parameter of Hessian ∇2fi, for all 1 ≤ i ≤ n.
Now, we state the next assumption which provides upper bounds on the variances of gradient
and Hessian estimation.

Assumption 4. For any i and any w ∈ Rd, the stochastic gradient ∇li(x, y; w) and Hessian ∇2li(x, y; w), computed with respect to a single data point (x, y) ∈ Xi × Yi, has bounded variance,
i.e.,

E(x,y)∼pi ∇li(x, y; w) − ∇fi(w) 2 ≤ σG2 , E(x,y)∼pi ∇2li(x, y; w) − ∇2fi(w) 2 ≤ σH2 ,

(14) (15)

where σG and σH are non-negative constants.
Finally, we state our last assumption which characterizes the similarity between the tasks of users.

Assumption 5. For any w ∈ Rd, the variance of gradient ∇fi(w) and Hessian ∇2fi(w) are bounded, i.e., for some non-negative γG and γH , we have

1n n

∇fi(w) − ∇f (w) 2 ≤ γG2 ,

i=1

1n n

∇2fi(w) − ∇2f (w) 2 ≤ γH2 ,

i=1

(16a) (16b)

for any w ∈ Rd.

Note that Assumption 2 implies that this assumption holds automatically for γG = 2B and γH = 2L. However, we state this assumption separately to highlight the role of similarity of functions corresponding to diﬀerent users in convergence analysis of Per-FedAvg. In particular, in the following subsection, we highlight the connections between this assumption and the similarity of distributions pi for the case of supervised learning (2) under two diﬀerent distribution distances.

6

4.2 On the Connections of Task Similarity and Distribution Distances

Recall the deﬁnition of fi for the supervised learning problem stated in (2). As mentioned above, Assumption 5 captures the similarity of loss functions of diﬀerent users, and one fundamental ques-

tion here is whether this has any connection with the closeness of distributions pi. We study this connection by considering two diﬀerent distances: Total Variation (TV) distance and 1-Wasserstein

distance. Throughout this subsection, we assume all users have the same loss function l(.; .) over

the same set of inputs and labels, i.e., fi(w) := Ez∼pi [l(z; w)] where z := (x, y) ∈ Z := X × Y.

Also,

let

p

=

1 n

i pi denote the average of all users’ distributions.

• Total Variation (TV) Distance: For distributions q1 and q2 over countable set Z, their TV

distance is given by

1

q1 − q2

TV

= 2

|q1(z) − q2(z)|.

(17)

z∈Z

If we further assume a stronger version of Assumption 2 holds where for any z ∈ Z and w ∈ Rd,

we have

∇wl(z; w) ≤ B, ∇2wl(z; w) ≤ L,

(18)

then, Assumption 5 holds with (check Appendix A for the proof)

γG = 2B

1n n

pi − p

2 TV

,

i=1

γH = 2L

1n n

pi − p

2 TV

.

i=1

(19a) (19b)

This simple derivation shows that γG and γH exactly capture the diﬀerence between the probability distributions of the users in a heterogeneous setting.

• 1-Wasserstein Distance: The 1-Wasserstein distance between two probability distributions measures q1 and q2 over a metric space Z deﬁned as1

W1(q1, q2) :=

inf

d(z1, z2) dq(z1, z2)

q∈Q(q1,q2) Z×Z

(20)

where d(., .) is a distance function over metric space Z and Q(q1, q2) denotes the set of all measures on Z ×Z with marginals q1 and q2 on the ﬁrst and second coordinate, respectively. Here, we assume all pi have bounded support (note that this assumption holds in many cases as either Z itself is bounded or because we normalize the data). Also, we assume that for any w, the gradient ∇wl(z; w) and the Hessian ∇2wl(z; w) are both Lipschitz with respect to parameter z and distance d(., .), i.e,

∇wl(z1; w) − ∇wl(z2; w) ≤ LZ d(z1, z2), ∇2wl(z1; w) − ∇2wl(z2; w) ≤ ρZ d(z1, z2).

(21a) (21b)

Then, Assumption 5 holds with (check Appendix A for the proof)

γG = LZ

1 n

n

W1(pi, p)2,

i=1

γH = ρZ

1 n

n

W1(pi, p)2.

i=1

(22a) (22b)

It is worth noting that this derivation does not use other Assumptions such as Assumption 2 and holds in general when (21a) and (21b) are satisﬁed.
1The integral can be replaces by sum if Z is countable.

7

4.3 Convergence Analysis of Per-FedAvg Algorithm

In this subsection, we derive the overall complexity of Per-FedAvg for achieving an -ﬁrst-order

stationary point. To do so, we ﬁrst prove the following intermediate result which shows that under

Assumptions 2 and 3, the local meta-functions Fi(w) deﬁned in (5) and their average function

F (w) = (1/n)

n i=1

Fi(w)

are

smooth.

Lemma 4.2. Recall the deﬁnition of Fi(w) (5) with α ∈ [0, 1/L]. Suppose that the conditions in

Assumptions 2 and 3 are satisﬁed. Then, Fi is smooth with parameter LF := 4L + αρB. As a

consequence, their average F (w) = (1/n)

n i=1

Fi(w)

is

also

smooth

with

parameter

LF .

Proof. Check Appendix B.

The conditions in Assumption 4 provide upper bounds on the variances of gradient and Hessian estimation for functions fi. To analyze the convergence of Per-FedAvg, however, we need an upper bound on the variance of gradient estimation of the functions Fi. We derive such an upper bound in the following lemma.

Lemma 4.3. Recall (9) that we estimate ∇Fi(w) by

∇˜ Fi(w) = I − α∇˜ 2fi(w, D ) ∇˜ fi w − α∇˜ fi(w, D), D

where D, D , and D are independent batches with size D, D , and D , respectively. Suppose that the conditions in Assumptions 2-4 are satisﬁed. Then, for any α ∈ [0, 1/L] and w ∈ Rd, we have E ∇˜ Fi(w) − ∇Fi(w) 2 ≤ σF2 , where σF2 is given by

σF2 := 3

B2 + σG2

1 (αL)2

+

D

D

4

+

σH2

α2 D

− 12B2

Proof. Check Appendix C.

To measure the tightness of this result, we consider two special cases. First, if the exact
gradients and Hessians are available, i.e., σG = σH = 0, then σF = 0 as well which is expected as we can compute exact ∇Fi. Second, for the classic federated learning problem, i.e., α = 0 and Fi = fi, we have σF = O(1)σG2 /D which is tight up to constants.
Next, we use the similarity conditions for the functions fi in Assumption 5 to study the similarity between gradients of the functions Fi.

Lemma 4.4. Recall the deﬁnition of Fi(w) in (5) and assume that α ∈ [0, 1/L]. Suppose that the conditions in Assumptions 2, 3, and 5 are satisﬁed. Then, for any w ∈ Rd, we have

1n n

∇Fi(w) − ∇F (w) 2 ≤ γF2 ,

i=1

with γF2 := 3B2α2γH2 + 192γG2 .

Proof. Check Appendix D.

It is worth going over the two special cases that we discussed for Lemma 4.3 to see how tight Lemma 4.4 is. First, if ∇fi are all equal, i.e., γG = γH = 0, then γF = 0 as well. This is indeed expected as all ∇Fi are equal to each other in this case. Second, for the classic federated learning problem, i.e., α = 0 and Fi = fi, we have γF = O(1)γG which is optimal up to a constant factor given the conditions in Assumption 5.
Now, we are ready to state the main result of our paper on the convergence of our proposed Per-FedAvg method.

Theorem 4.5. Consider the objective function F deﬁned in (4) for the case that α ∈ (0, 1/L]. Suppose that the conditions in Assumptions 1-4 are satisﬁed, and recall the deﬁnitions of LF , σF , and ηF from Lemmas 4.2-4.4. Consider running Algorithm 1 for K rounds with τ local updates in each round and with β ≤ 1/(10τ LF ). Then, the following ﬁrst-order stationary condition holds

1 K−1 τ −1 E
τK
k=0 t=0

∇F (w¯k+1,t) 2

≤ 4(F (w0) − F ∗) + 60 βτK

σF2 + γF2

8

where w¯k+1,t is the average of iterates of users in Ak at time t, i.e.,

1 w¯k+1,t = rn

wki +1,t,

i∈Ak

and in particular, w¯k+1,0 = wk and w¯k+1,τ = wk+1.

Proof. Check Appendix F.

The result in Theorem 4.5 shows that if each user runs τ local updates at each iteration, after K rounds of communication between users and server the average squared gradient norm in expectation converges at a sublinear rate of O(1/Kτ ) to a neighborhood of 0 with radius O(σF2 + γF2 ). This result shows to ﬁnd an O( + σF + γF )-FOSP, we need to ensure that the parameters K and τ satisfy the condition Kτ = O(1/ 2).
Note that σF is not a constant, and as expressed in Lemma 4.3, we can make it arbitrary small by choosing batch sizes D, D , or D large enough. Also, and as we discussed after Lemma 4.4, σF would be zero if we assume we have access to the exact the gradient and Hessians. Similarly, Lemma 4.4 implies that having small values for γG and γH would imply that γF is also small. As we discussed in Section 4.2, this observation is related to the closeness of data distribution of agents with respect to distribution measures such as Total Variation or 1-Wasserstein metric. In particular, consider the special case when fi admits the ﬁnite sum representation (3) and the data distributions are homogeneous, i.e., all users data distributions are drawn from an underlying distribution pu. Then, having more samples for each user, i.e., larger Si in (3), will lead to smaller γG and γH as the empirical distribution of each user becomes closer to pu (see (Reisizadeh et al., 2019)).

Remark 4.6. The result of Theorem 4.5 provides an upper bound on the average of E ∇F (w¯k+1,t) 2 for all k ∈ {0, 1, ..., K − 1} and t ∈ {0, 1, ..., τ − 1}. However, one concern
here is that due to the structure of Algorithm 1, for any k, we only have access to w¯k+1,t for t = 0.
To address this issue, at any iteration k, the center can choose tk ∈ {0, 1..., τ − 1} uniformly at random, and ask all the users in Ak to send wki +1,tk back to the server, possibly in addition to wki +1,τ . If follow such a scheme then we can ensure that

1 K−1 E
τK

∇F (w¯k+1,tk ) 2

≤ 4(F (w0) − F ∗) + 60 βτK

σF2 + γF2

.

k=0

5 Numerical Experiments

In this section, we design a numerical setting to highlight the role of personalization when the data distributions are heterogeneous. In particular, we consider the problem of classifying handwritten digits from the MNIST dataset (LeCun, 1998) and distribute the training data between n users as follows:

• Half of the users have a images of each of the digits 0-4.

• The rest, each have a/2 images from one of 0-4 digits and 2a images from one of 5-9 digits.

This way, we create an example where the distribution of images over all the users are diﬀerent from each other. Similarly, we divide the test data over the nodes with the same distribution as the one for the training data.
We consider three algorithms in this setting: First, the classic FedAvg method, where the users ﬁnd a shared model which all implement without any update during the test timet. Second, we take the output of the FedAvg method, and update it with one step of gradient descent with respect to the test data, and then evaluate its performance. Third, we consider our proposed algorithm, Per-FedAvg, and update its output, again with one step of gradient descent, during the test time. Similar to MAML, implementation of Per-FedAvg requires access to second-order information which is computationally costly. To address this issue, we replace the gradient estimate at each iteration with its ﬁrst-order approximation which ignores the Hessian term, i.e., ∇˜ Fi(wki +1,t−1) in (9) is approximated by

∇˜ fi wki +1,t−1 − α∇˜ fi(wki +1,t−1, Dti), Dti .

(23)

9

ﬁg1: τ = 5 and r = 0.2.

ﬁg2: τ = 10 and r = 0.2

ﬁg3: τ = 5 and r = 0.4.

ﬁg4: τ = 10 and r = 0.4

Figure 1: Comparison of FedAvg, with and without update at test time, and Per-FedAvg

This is the same idea deployed in First-Order MAML (FO-MAML) in (Finn et al., 2017), and it has been shown that it almost achieves the same level of performance as MAML when the the learning rate α is small (Fallah et al., 2019). Also, in Appendix G, we discuss how our analysis can be extended to ﬁrst-order approximations of Per-FedAvg, such as the one implemented for this experiment.
For this experiment, we use a neural network classiﬁer with two hidden layers with sizes 80 and 60, respectively, and we use Exponential Linear Unit (ELU) activation function. We run all three algorithms for K = 1000 rounds. At each round, we assume a fraction of agents with size rn are chosen to run τ local updates. The batch sizes D = D = 50 and the learning rates are chosen as α = 0.01 and β = 0.001. Further, we consider the case that there are n = 10 users in the network. We would like to mention that part of the code is adopted from (Langelaar, 2019).
The results for diﬀerent values of number of local updates τ and ratio of active users r are illustrated in Figure 1. As expected, in all considered cases, the model trained by running the update of FedAvg to solve the classic FL problem in (1) performs worse than the same model after running one step of local gradient in the test phase. Hence, if extra computation is available at the test time, the model of FedAvg after one step of gradient descent leads to a more personalized solution.
More importantly, the Per-FedAvg method, which is originally designed to ﬁnd a point which performs well once it is updated using one step of local gradient descent has the best performance among the three considered approaches. In other words, its model has a better test accuracy compared to the model that is obtained by running one step of local gradient over the solution of FedAvg. These experiments show that by solving the MAML variant of the FL problem we obtain a solution that performs better in heterogeneous settings.

10

6 Conclusion
In this paper, we studied the Federated Learning (FL) problem in a heterogeneous case that the probability distribution of the users in the network are not identical and could be diﬀerent. To solve this problem, we studied a personalized variant of the classic FL formulation in which our goal is to ﬁnd a proper initialization model for the users in the network that can be quickly adapted to the local data of each user after the training phase. In particular, we introduced a Model-Agnostic Meta-Learning (MAML) variant of FL in which instead of minimizing the average loss over the data of all users, we ﬁnd the best initial model that after one step of local gradient leads to a good model for each individual user. As expected, this approach leads to a more personalized model for each user. We then introduced a personalized variant of the FedAvg algorithm, called Per-FedAvg, to solve the proposed personalized FL problem. We also characterized the overall complexity of the Per-FedAvg method for nonconvex settings. Speciﬁcally, for the case that each user runs τ local updates at each iteration, we showed that after K rounds of communication between users and server Per-FedAvg converges to a neighborhood of a ﬁrst-order stationary point at a rate of O(1/Kτ ), where the radius of this neighborhood depends on the closeness of data distribution of diﬀerent users. Finally, we provided a numerical experiment to illustrate the performance of Per-FedAvg and its comparison with FedAvg method.

Appendix

A Proofs of results in Subsection 4.2
A.1 TV Distance
Note that

∇fi(w) − ∇f (w) =

∇wl(z; w) (pi(z) − p(z))

z∈Z

≤

∇wl(z; w) |pi(z) − p(z)|

z∈Z

≤ B |pi(z) − p(z)| = 2B pi − p T V

(24)

z∈Z

where for the inequality we used the assumption that ∇wl(z; w) ≤ B for any w and z. Plugging

(24) in

1n n

∇fi(w) − ∇f (w) 2

(25)

i=1

gives us the desired result. The other result on Hessians can be proved similarly.

A.2 1-Wasserstein Distance
We claim that for any i and w ∈ Rd, we have

∇fi(w) − ∇f (w) ≤ LZ W1(pi, p)

(26)

which will immediately gives us one of the two results. To show this, ﬁrst, note that

∇fi(w) − ∇f (w) = sup v (∇fi(w) − ∇f (w))
v∈Rd: v ≤1
= sup Ez∼pi v ∇l(z; w) − Ez∼p v ∇l(z; w)
v∈Rd: v ≤1
Thus, we need to show for any v ∈ Rd with v ≤ 1, we have

Ez∼pi v ∇l(z; w) − Ez∼p v ∇l(z; w) ≤ LZ W1(pi, p).

(27)

11

Next, note that since pi and p both have bounded support, by Kantorovich–Rubinstein Duality Theorem Villani (2008), we have

W1(pi, p) = sup {Ez∼pi [g(z)] − Ez∼p [g(z)] | continuous g : Z → R, Lip(g) ≤ 1} . (28)
Using this result, to show (27), it suﬃces to show g(z) = v ∇l(z; w) is LZ -Lipschitz. Note that Cauchy-Schwarz inequality implies

v ∇l(z1; w) − v ∇l(z2; w) ≤ v ∇l(z1; w) − ∇l(z2; w) ≤ LZ d(z1, z2)

(29)

where the last inequality is obtained using v ≤ 1 along with (21). Finally, note that we can similarly show the result for γH by considering the fact that
∇2fi(w) − ∇2f (w) = sup v (∇fi(w) − ∇f (w)) v
v∈Rd: v ≤1
= sup Ez∼pi v ∇l(z; w)v − Ez∼p v ∇l(z; w)v
v∈Rd: v ≤1

and taking the function g(z) = v ∇l(z; w)v and using Kantorovich–Rubinstein Duality Theorem again.

B Proof of Lemma 4.2

Recall that

∇Fi(w) = I − α∇2fi(w) ∇fi(w − α∇fi(w)).

(30)

Given this, note that

∇Fi(w1) − ∇Fi(w2)

= I − α∇2fi(w1) ∇fi(w1 − α∇fi(w1)) − I − α∇2fi(w2) ∇fi(w2 − α∇fi(w2))

= I − α∇2fi(w1) (∇fi(w1 − α∇fi(w1)) − ∇fi(w2 − α∇fi(w2)))

+ I − α∇2fi(w1) − I − α∇2fi(w2) ∇fi(w2 − α∇fi(w2))

(31)

≤ I − α∇2fi(w1) ∇fi(w1 − α∇fi(w1)) − ∇fi(w2 − α∇fi(w2))

+ α ∇2fi(w1) − ∇2fi(w2) ∇fi(w2 − α∇fi(w2))

(32)

where (31) is obtained by adding and subtracting I − α∇2fi(w1) ∇fi(w2 − α∇fi(w2)) and the last inequality follows from the triangle inequality and the deﬁnition of matrix norm. Now, we
bound two terms of (32) separately. First, note that by (12a), I − α∇2fi(w1) ≤ 1 + αL. Using this along with smoothness of fi, we have

I − α∇2fi(w1) ∇fi(w1 − α∇fi(w1)) − ∇fi(w2 − α∇fi(w2))

≤ (1 + αL)L w1 − α∇fi(w1)) − w2 + α∇fi(w2)

≤ (1 + αL)L ( w1 − w2 + α ∇fi(w1) − ∇fi(w2) )

≤ (1 + αL)L(1 + αL) w1 − w2 ≤ 4L w1 − w2

(33)

where we used smoothness of fi along with α ≤ 1/L for the last line. For the second term, Using (11a) in Assumption 2 along with Assumption 3 implies

α ∇2fi(w1) − ∇2fi(w2) ∇fi(w2 − α∇fi(w2)) ≤ αρB w1 − w2 .

(34)

Putting (33) and (34) together, we obtain the desired result.

C Proof of Lemma 4.3

Recall that the expression for the stochastic gradient ∇˜ Fi(w) is given by

∇˜ Fi(w) = I − α∇˜ 2fi(w, D ) ∇˜ fi w − α∇˜ fi(w, D), D

(35)

12

which can be written as

∇˜ Fi(w) = I − α∇2fi(w) + e1 (∇fi (w − α∇fi(w)) + e2) .

(36)

Note that in the above expression e1 and e2 are given by

e1 = α ∇2fi(w) − ∇˜ 2fi(w, D ) ,

and e2 = ∇˜ fi(w − α∇˜ fi(w, D), D ) − ∇fi (w − α∇fi(w)) .

It can be easily shown that

E

e1 2

≤ α2 σH2 . D

(37)

Next, we proceed to bound the second moment of e2. To do so, ﬁrst note that e2 can also be

written as

e2 = ∇˜ fi w − α∇˜ fi(w, D), D − ∇fi w − α∇˜ fi(w, D)

+ ∇fi w − α∇˜ fi(w, D) − ∇fi (w − α∇fi(w)) .

(38)

Note that, conditioning on D, the ﬁrst term is zero mean and the second term is deterministic. Therefore,
E e2 2 = E E e2 2|D = E ∇˜ fi w − α∇˜ fi(w, D), D − ∇fi w − α∇˜ fi(w, D) 2

+E

∇fi

w − α∇˜ fi(w, D)

2
− ∇fi (w − α∇fi(w))

≤

σG2 D

+

L2α2E

∇˜ fi(w, D) − ∇fi(w) 2

(39)

≤ σG2

1 (αL)2 +
DD

(40)

where (39) is obtained using smoothness of fi along with the fact that E ∇˜ fi w − α∇˜ fi(w, D), D − ∇fi w − α∇˜ fi(w, D)

2 ≤ σG2 . D

The last inequality is also obtained using

E

2
∇˜ fi(w, D) − ∇fi(w)

≤ σG2 . D

Next, note that, by comparing (36) and (6), along with the matrix norm deﬁnition, we have

∇˜ Fi(w) − ∇Fi(w) ≤ I − α∇2fi(w) e2 + e1 ∇fi (w − α∇fi(w)) + e1 e2 . (41)

As a result, by the Cauchy-Schwarz inequality (a + b + c)2 ≤ 3(a2 + b2 + c2) for a, b, c ≥ 0, we have

2
∇˜ Fi(w) − ∇Fi(w) ≤ 3 I − α∇2fi(w) 2 e2 2 + 3 e1 2 ∇fi (w − α∇fi(w)) 2 + 3 e1 2 e2 2. (42)

By taking expectation, and using the fact that I − α∇2fi(w) ≤ 1 + αL ≤ 2 and

∇fi (w − α∇fi(w)) ≤ B,

we have

E ∇˜ Fi(w) − ∇Fi(w) 2 ≤ 3B2E e1 2 + 12E e2 2 + 3E e1 2 E e2 2

(43)

where we also used the fact that e1 and e2 are independent as D is independent from D and D . Plugging (37) and (40) in (43), we obtain

E

∇˜ Fi(w) − ∇Fi(w) 2

≤

3B2α2 σH2 D

+ 12σG2

1 (αL)2 +
DD

+ 3α2σG2 σH2

1

(αL)2

+

D D DD

which gives us the desired result.

13

D Proof of Lemma 4.4

Recall that

∇Fi(w) = I − α∇2fi(w) ∇fi(w − α∇fi(w)).

(44)

which can be expressed as

∇Fi(w) = I − α∇2f (w) + Ei (∇f (w − α∇f (w)) + ri)

(45)

where

Ei = α ∇2f (w) − ∇2fi(w) ,

(46)

ri = ∇fi(w − α∇fi(w)) − ∇f (w − α∇f (w)).

(47)

First, note that, by Assumption 5, we have

1n n

Ei 2 = α2γH2 .

i=1

(48)

Second, note that

ri ≤ ∇fi(w − α∇fi(w)) − ∇fi(w − α∇f (w)) + ∇fi(w − α∇f (w)) − ∇f (w − α∇f (w))

≤ αL ∇fi(w) − ∇f (w) + ∇fi(w − α∇f (w)) − ∇f (w − α∇f (w))

(49)

where the last inequality is obtained using (11b) in Assumption 2. Now, by using (a + b)2 ≤ 2(a2 + b2), we have

1n n

ri

2≤

2 n

n

(αL)2 ∇fi(w) − ∇f (w) 2 + ∇fi(w − α∇f (w)) − ∇f (w − α∇f (w)) 2

i=1

i=1

≤ 2 1 + (αL)2 (γG2 + γG2 )

(50)

≤ 8γG2 .

(51)

where the second inequality follows from Assumption 5 and the last inequality is obtained using
αL ≤ 1. Next, recall that the goal is to bound the variance of ∇Fi(w) when i is drawn from a uniform distribution. We know that by subtracting a constant from a random variable, its variance does not change. Thus, variance of ∇Fi(w) is equal to variance of ∇Fi(w)− I − α∇2f (w) ∇f (w− α∇f (w)). Also, the variance of the latter is bounded by its second moment, and hence,

1n n

∇Fi(w) − ∇F (w)

2

≤

1 n

n

Ei∇f (w − α∇f (w)) + I − α∇2f (w) ri + Eiri 2

i=1

i=1

1n ≤
n

Ei∇f (w − α∇f (w)) + I − α∇2f (w) ri + Eiri 2

(52)

i=1

Therefore, using ∇f (w − α∇f (w)) ≤ B along with I − α∇2f (w) ≤ 1 + αL and CauchySchwarz inequality (a + b + c)2 ≤ 3(a2 + b2 + c2) for a, b, c ≥ 0, we obtain

1n n

∇Fi(w) − ∇F (w) 2 ≤ 3

i=1

≤3

B2 1 n n
i=1
B2 1 n n
i=1

Ei

2 + (1 + αL)2 1 n

n

ri

2+ 1 n

n

Eiri 2

i=1

i=1

Ei

2 +41 n

n

ri

2+ 1 n

n

Ei 2 ri 2

i=1

i=1

(53)

where the last inequality is obtained using αL ≤ 1 along with Eiri ≤ Ei ri which comes from the deﬁnition of matrix norm. Finally, to complete the proof, notice that we have

1n n
i=1

Ei 2 ri 2 ≤ max Ei 2
i

1n n

ri 2

i=1

≤ max
i

Ei

2(8γG2 )

≤ 32(αL)2γG2 ≤ 32γG2

(54) (55) (56)

14

where (55) follows from (51) and the last line is obtained using αL ≤ 1 along with the fact that

∇2fi(w) ≤ L, and thus,

Ei α

=

∇2f (w) − ∇2fi(w)

≤ 2L.

(57)

Plugging (55) in (53) along with (48) and (51), we obtain the desired result.

E An Intermediate Result

Proposition E.1. Recall from Section 3 that at any round k ≥ 1, and for any agent i ∈ {1, .., n}, we can deﬁne a sequence of local updates {wki ,t}τt=0 where wki ,0 = wk−1 and, for τ ≥ t ≥ 1,

wki ,t = wki ,t−1 − β∇˜ Fi(wki ,t−1).

(58)

We further deﬁne the average of these local updates at round k and time t as wk,t = 1/n

n i=1

wki ,t.

Suppose that the conditions in Assumptions 2-4 are satisﬁed. Then, for any α ∈ [0, 1/L] and any

t ≥ 1, we have

1n En

wki ,t − wk,t

i=1

1n En

wki ,t − wk,t 2

i=1

≤ (1 + 2βLF )t(σF + γF )/LF ,

≤

1

+

φ

+

16(1

+

1 φ

)β

2

L2F

t 2σF2 + γF2 4L2F

(59a) (59b)

where φ > 0 is an arbitrary positive constant and LF , σF , and γF are given in Lemmas 4.2, 4.3, and 4.4, respectively.

Before stating the proof, note that an immediate consequence of this result is the following corollary:

Corollary E.2. Under the same assumptions as Proposition E.1, and for any β ≤ 1/(10τ LF ), we have

1n En

wki ,t − wk,t

i=1

1n En

wki ,t − wk,t 2

i=1

≤ 2(σF + γF )/LF ,

≤

2σF2 + L2F

γF2

(60a) (60b)

for any 1 ≤ t ≤ τ .

Proof. Let

1n St := n E
i=1

wki ,t − wk,t

(61)

where S0 = 0 since wki ,0 = wk−1 for any i. Note that

1n

St+1 = n

E

i=1

wki ,t+1 − wk,t+1





1 =
n

n

E

wki ,t

−

β∇˜ Fi(wki ,t)

−

1 n

n

wkj,t − β∇˜ Fj (wkj,t)



i=1

j=1









1 ≤
n

n

E

wki ,t −

1 n

n

wkj ,t

1 +βn

n

E

∇˜ Fi(wki ,t) −

1 n

n

∇˜ Fj (wkj,t)  .

i=1

j=1

i=1

j=1

(62)

15

Note that the ﬁrst term in (62) is in fact St and the second one can be upper bounded as follows





1 n

n

E

∇˜ Fi(wki ,t) −

1 n

n

∇˜ Fj (wkj,t) 

i=1

j=1





1 ≤
n

n

E

∇Fi(wki ,t) −

1 n

n

∇Fj (wkj,t)

+

1 n

n

E

i=1

j=1

i=1





1n

1n

+ n

En

∇Fj (wkj,t) − ∇˜ Fj (wkj,t) 

i=1

j=1





1 ≤
n

n

E

∇Fi(wki ,t) −

1 n

n

∇Fj (wkj,t)  + 2βσF

i=1

j=1

∇Fi(wki ,t) − ∇˜ Fi(wki ,t)

where the last inequality is obtained using Lemma 4.3. By substituting this in (62), we obtain





1 St+1 ≤ St + 2βσF + β n

n

E

∇Fi(wki ,t) −

1 n

n

∇Fj (wkj,t)  .

i=1

j=1

(63)

If we deﬁne ηi := ∇Fi(wki ,t) − ∇Fi(wk,t), using (63), we obtain









1n

1n

1n

1n

St+1

≤

St

+

2βσF

+

β n

E  ∇Fi(wk,t) − n

∇Fj(wk,t)  + β n

E  ηi − n

ηj  .

i=1

j=1

i=1

j=1

(64)

Note that, by Lemma 4.2,

ηi ≤ LF wki ,t − wk,t ,

(65)

and thus,

1n

n

ηi ≤ LF St.

(66)

i=1

As a result, and by using (64), we have





1n

1n

St+1

≤

(1

+

2βLF )St

+

2βσF

+

β n

E  ∇Fi(wk,t) − n

∇Fj (wk,t)  .

i=1

j=1

≤ (1 + 2βLF )St + 2β(σF + γF )

(67)

where the last inequality is obtained using Lemma 4.4. Using (67) inductively, we obtain





St+1

≤

t
 (1
j=0

+

2βLF )j 2β(σF

+

γF )

=

(1 + 2βLF )t+1 − (1 + 2βLF ) − 1

1

2β

(σF

+

γF )

≤

(1

+

2βLF

)t+1

σF + LF

γF

(68)

which completes the proof of (59a). To prove (59b), let

1n Σt := n E
i=1

wki ,t − wk,t 2 .

(69)

16

Similarly Σ0 = 0. Note that

1n

Σt+1 = n

E

i=1

wki ,t+1 − wk,t+1 2



2

1 =
n

n

E

 

wki ,t

−

β∇˜ Fi(wki ,t)

−

1 n

n

wkj,t − β∇˜ Fj (wkj,t)

 

i=1

j=1









1+φ ≤
n

n

E

wki ,t −

1 n

n

wkj ,t

2 + β2 1 + 1/φ n

n

E

∇˜ Fi(wki ,t) −

1 n

n

∇˜ Fj (wkj,t)

2


i=1

j=1

i=1

j=1

(70)





≤

(1

+

φ)Σt

+

β2

1

+ 1/φ n

n

E

∇˜ Fi(wki ,t) −

1 n

n

∇˜ Fj (wkj,t)

2


i=1

j=1

(71)

where (70) is obtained using a + b 2 ≤ (1 + φ) a 2 + (1 + 1/φ) b 2 for with φ > 0 an arbitrary positive real number. To bound the second term in (71), note that







E

∇˜ Fi(wki ,t) −

1 n

n

∇˜ Fj (wkj,t)

2


≤

2E



∇Fi(wki ,t) −

1 n

n

∇Fj (wkj,t)

2


j=1

j=1



2

+

2E

 

∇˜ Fi(wki ,t) − ∇Fi(wki ,t)

1 +
n

n

∇Fj (wkj,t) − ∇˜ Fj (wkj,t)

. 

j=1

(72)

Now, we bound the second term in (72). Using Cauchy-Schwarz inequality

n+1

2

n+1

albl ≤

al 2

n+1
bl 2

(73)

l=1

l=1

l=1

with

a1

=

∇˜ Fi(wki ,t) − ∇Fi(wki ,t), b1

=

1

and

al

=

√ 1/ n

(∇˜ Fl−1(wkl−,t1) − ∇Fl−1(wkl−,t1)),

bl

=

√ 1/ n,

for l = 2, ..., n + 1, implies



2

E

 

∇˜ Fi(wki ,t) − ∇Fi(wki ,t)

1 +
n

n

∇Fj (wkj,t) − ∇˜ Fj (wkj,t)

 

j=1





≤ 2E 

∇˜ Fi(wki ,t) − ∇Fi(wki ,t)

21 + n

n

∇Fj (wkj,t) − ∇˜ Fj (wkj,t)

2


j=1

≤ 4σF2

(74)

where the last inequality is obtained using Lemma 4.3. Plugging (74) in (72) and using (71), we obtain





Σt+1

≤

(1

+

φ)Σt

+

8(1

+

1 φ

)β

2σF2

+

2(1

+

1 )β2 1 φn

n

E

∇Fi(wki ,t) −

1 n

n

∇Fj (wkj,t) 2 .

i=1

j=1

(75)

Now, it remains to bound the last term in (75). Recall ηi = ∇Fi(wki ,t) − ∇Fi(wk,t). First, note that, using a + b 2 ≤ 2 a 2 + 2 b 2, we have

∇Fi(wki ,t)

−

1 n

n

∇Fj (wkj,t)

2≤2

1 ∇Fi(wk,t) − n

n

∇Fj (wk,t)

2+2

ηi −

1 n

n

ηj 2.

(76)

j=1

j=1

j=1

17

Substituting this bound in (75) and using Lemma 4.4 yields





Σt+1

≤

(1

+

φ)Σt

+

4(1

+

1 φ

)β

2

(2σF2

+

γF2 )

+

4(1

+

1 )β2 1 φn

n

1 E  ηi − n

n

ηj

2 .

(77)

i=1

j=1

√ No√te that, using Cauchy-Schwarz inequality (73) with a1 = ηi, b1 = 1 and al = 1/ nηl−1, bl = 1/ n for l = 2, ..., n + 1, implies





1 ηi − n

n

ηj

2 ≤ 2

ηi

2+ 1 n

n

ηj

2


j=1

j=1





≤ 2L2F 

wki ,t − wk,t

2+ 1 n

n

wki ,t − wk,t

2


j=1

(78)

where the last inequality is obtained using Lemma 4.2 which states

ηi ≤ LF wki ,t − wk,t .

(79)

Plugging (78) in (77) implies

Σt+1 ≤

1

+

φ

+

16(1

+

1 φ

)β2L2F

Σt

+

4(1

+

1 φ

)β2(2σF2

+

γF2 ).

(80)

As a result, using induction similar to (68), we obtain

Σt+1 ≤

1

+

φ

+

16(1

+

1 φ

)β2L2F

t+1 2σF2 + γF2 4L2F

(81)

which gives us the desired result (59b). Finally, to show (60), ﬁrst note that for any n, we know

(1 + 1 )n ≤ e.

(82)

n

Using this, along with the assumption β ≤ 1/(10LF τ ) and the fact that e0.2 ≤ 2, we immediately obtain (60a). To show the other one (60b), we use (59b) with φ = 1/(2τ ):

φ + 16(1 +

1 φ

)β2L2F

=

1 2τ

+ 16(1 + 2τ )β2L2F

1

1

≤ 2τ + 16(1 + 2τ ) 100τ 2

1

≤

(83)

τ

where the ﬁrst inequality follows from the assumption β ≤ 1/(10LF τ ) and the last inequality is obtained using the trivial bound 1 + 2τ ≤ 3τ . Finally, using (83) along with (82) completes the proof.

F Proof of Theorem 4.5

Although we only ask a fraction of agents to compute their local updates in Algorithm 1, here,

and just for the sake of analysis, we assume all agents perform local updates. This is just for our

analysis and we will not use all agents’ updates in computing wk+1. Also, from Proposition E.1,

recall that wk,t = 1/n

n i=1

wki ,t.

Let Fkt+1 denote the σ-ﬁeld generated by {wki +1,t}ni=1. Note that, by Lemma 4.2, we know F

is smooth with gradient Lipschitz parameter LF , and thus, by (12b), we have

F (w¯k+1,t+1) ≤ F (w¯k+1,t) + ∇F (w¯k+1,t)

(w¯k+1,t+1

−

w¯k+1,t)

+

LF 2

w¯k+1,t+1 − w¯k+1,t 2

≤ F (w¯k+1,t) − β∇F (w¯k+1,t)

1 rn

∇˜ Fi(wki +1,t)

i∈Ak

+ LF β2 1 2 rn

∇˜ Fi(wki +1,t) 2

i∈Ak

(84)

18

where the last inequality is obtained using the fact that

1 w¯k+1,t+1 = rn

wki +1,t+1

=

1 rn

wki +1,t − β∇˜ Fi(wki +1,t)

1 = w¯k+1,t−β rn

∇˜ Fi(wki +1,t).

i∈Ak

i∈Ak

i∈Ak

Taking expectation from both sides of (84) yields

E [F (w¯k+1,t+1)]

(85)

≤ E[F (w¯k+1,t)] − βE ∇F (w¯k+1,t)

1 rn

∇˜ Fi(wki +1,t)

i∈Ak

+

LF 2

β2E

1 rn

∇˜ Fi(wki +1,t) 2

i∈Ak

Next, note that

1 rn

∇˜ Fi(wki +1,t)

=

X

+Y

+Z

+

1 rn

∇Fi(w¯k+1,t)

i∈Ak

i∈Ak

(86)

where

1 X=
rn

∇˜ Fi(wki +1,t) − ∇Fi(wki +1,t) ,

i∈Ak

1 Y=
rn

∇Fi(wki +1,t) − ∇Fi(wk+1,t) ,

i∈Ak

1

Z= rn

(∇Fi(wk+1,t) − ∇Fi(w¯k+1,t)) .

i∈Ak

(87) (88) (89)

We next bound the second moment of X, Y , and Z, condition on Fkt+1. First, recall the Cauchy-

Schwarz inequality

rn

2

rn

rn

aibi ≤

ai 2

bi 2 .

(90)

i=1

i=1

i=1

•

Using this inequality with ai

=

(∇˜ Fi(wki +1,t)

−

∇Fi

(wki +1,t

√ ))/ rn

and

bl

√ = 1/ rn, we obtain

X 2≤ 1 rn

∇˜ Fi(wki +1,t) − ∇Fi(wki +1,t)

2
,

i∈Ak

(91)

and hence, by using Lemma 4.3 along with the tower rule, we have

E[ X 2] = E[E[ X 2 | Fkt+1]] ≤ σF2 .

(92)

• Regarding Y , note that by using Cauchy-Schwarz inequality (similar to what we did above) along with smoothness of Fi, we obtain

Y 2≤ 1 rn

∇Fi(wki +1,t) − ∇Fi(wk+1,t)

2≤

L2F rn

wki +1,t − wk+1,t 2 .

i∈Ak

i∈Ak

(93)

Again, taking expectation and using the fact that Ak is chosen uniformly at random, implies

E[ Y 2] = E[E[ Y 2 | Fkt+1]]

≤ L2F E

E

1 rn

i∈Ak

≤ 2σF2 + γF2

wki +1,t − wk+1,t 2

Fkt+1

= L2F E

1 n

n

wki ,t − wk,t 2

i=1

(94)

where the last step follows from (60b) in Corollary E.2.

•

Regarding Z, ﬁrst variance σ2 = 1/n

recall
n
i=1

that if we |ai − µ|2 ,

have and

n numbers a1, ..., an with we take a subset of them

mean µ {ai}i∈A

= 1/n

n i=1

ai

and

with size |A| = rn

by sampling without replacement, then we have

E

i∈A ai − µ 2

σ2 =

rn − 1 1−

σ2 ≤.

rn

rn

n − 1 rn

(95)

19

Using this, we have

E

w¯k+1,t − wk+1,t 2 | Fkt+1

1/n ≤

n i=1

wki +1,t − wk+1,t

2
,

rn

(96)

and hence, by taking expectation from both sides and using the tower rule along with (60b) in Corollary E.2, we obtain

E

w¯k+1,t − wk+1,t 2

≤

2σF2 + γF2 rnL2F

.

(97)

Next, note that by us√ing Cauchy-Sch√warz inequality (90), with ai = (∇Fi(wk+1,t) − ∇Fi(w¯k+1,t)) / rn and bi = 1/ rn, we have

Z 2≤ 1 rn

∇Fi(wk+1,t) − ∇Fi(w¯k+1,t) 2

i∈Ak

≤ L2F rn

wk+1,t − w¯k+1,t 2 = L2F w¯k+1,t − wk+1,t 2

i∈Ak

(98)

where the last inequality is obtained using smoothness of Fi (Lemma 4.2). Now, taking expectation from both sides and using (97) yields

E[

Z

2] ≤

2σF2 + γF2 . rn

(99)

Now, getting back to (85), we ﬁrst lower bound the term

E ∇F (w¯k+1,t) To do so, note that, by (86), we have

1 rn

∇˜ Fi(wki +1,t) .

i∈Ak

E ∇F (w¯k+1,t)

1 rn

∇˜ Fi(wki +1,t)

i∈Ak

= E ∇F (w¯k+1,t)

1

X+Y +Z+ rn

∇Fi(w¯k+1,t)

i∈Ak

≥ E ∇F (w¯k+1,t)

1

rn

∇Fi(w¯k+1,t)

i∈Ak

1 − 2 E[

∇F (w¯k+1,t)

2]

−

1 2 E[

X +Y

+Z

2]

(100)

where the last inequality is obtained using the fact that

E ∇F (w¯k+1,t)

(X + Y + Z)

1 ≤
2

E[ ∇F (w¯k+1,t) 2] + E[ X + Y + Z 2] .

Now, we bound terms in (100) separately. First, note that by tower rule we have

E ∇F (w¯k+1,t)

1

rn

∇Fi(w¯k+1,t)

i∈Ak

= E E ∇F (w¯k+1,t)

1

rn

∇Fi(w¯k+1,t)

i∈Ak

= E ∇F (w¯k+1,t) E = E ∇F (w¯k+1,t) 2

1

rn

∇Fi(w¯k+1,t)

i∈Ak

Fkt+1 Fkt+1

(101)

20

where the last equality is obtained using the fact that Ak is chosen uniformly at random, and thus,

1 E rn

∇Fi(w¯k+1,t)

Fkt+1

1 =
n

n
∇Fi(w¯k+1,t).

i∈Ak

i=1

Second, note that by Cauchy-Schwarz inequality,

E[ X + Y + Z 2] ≤ 3 E[ X 2] + E[ Y 2] + E[ Z 2] ≤ 3 (3 + 2/rn)σF2 + (1 + 2/rn)γF2 ≤ 15σF2 + 9γF2

(102)

where second inequality is obtained using (92), (94), and (99). Plugging (101) and (102) in (100) implies

E ∇F (w¯k+1,t)

1 rn

∇˜ Fi(wki +1,t)

i∈Ak

≥

1 2 E[

∇F (w¯k+1,t)

2] −

15 2

(σF2

+ γF2 ).

Next, we characterize an upper bound for the other term in (85):

(103)

1 E rn

∇˜ Fi(wki +1,t) 2

i∈Ak

Note that, by (86) we have

1 rn

∇˜ Fi(wki +1,t)

2≤2

X +Y

+Z

2+2

1 rn

∇Fi(w¯k+1,t) 2,

i∈Ak

i∈Ak

and thus, by (102), we have

(104)

1 E rn

∇˜ Fi(wki +1,t) 2 ≤ 2E

1 rn

∇Fi(w¯k+1,t) 2 + 30σF2 + 18γF2 .

i∈Ak

i∈Ak

(105)

Note that, E 1/(rn) i∈Ak ∇Fi(w¯k+1,t) | Fkt+1 = ∇F (w¯k+1,t), since Ak is chosen uniformly at random. Also, by Lemma 4.4, we have

1 nE

∇Fi(w¯k+1,t) − ∇F (w¯k+1,t) 2 Fkt+1 ≤ γF2 ,

and thus, by (95), we have

1 E rn

∇Fi(w¯k+1,t) 2

≤E

∇F (w¯k+1,t) 2

+ γF2 . rn

i∈Ak

Plugging (106) in (105), we obtain

(106)

1 E rn

∇˜ Fi(wki +1,t) 2 ≤ 2E ∇F (w¯k+1,t) 2 + 30(σF2 + γF2 ).

i∈Ak

(107)

Substituting (107) and (103) in (85) implies

E [F (w¯k+1,t+1)] ≤ E[F (w¯k+1,t)] − β(1/2 − βLF )E

∇F (w¯k+1,t)

2

+

1 15(
2

+

βLF )β(σF2

+

γF2 )

β ≤ E[F (w¯k+1,t)] − 4 E

∇F (w¯k+1,t) 2 + 15β(σF2 + γF2 )

(108)

where the last inequality is obtained using β ≤ 1/(10τ LF ). Summing up (108) for all t = 0, ..., τ −1, we obtain

βτ E [F (wk+1)] ≤ E [F (wk)] − 4

1 τ −1 E
τ
t=0

∇F (w¯k+1,t) 2

+ 15βτ (σF2 + γF2 )

(109)

21

where we used the fact that w¯k+1,τ = wk+1. Finally, summing up (109) for k = 0, ..., K − 1 implies

βτK E [F (wK )] ≤ F (w0) − 4 As a result, we have

1 K−1 τ −1 E
τK
k=0 t=0

∇F (w¯k+1,t) 2

+ 15βτ K(σF2 + γF2 ).

(110)

1 K−1 τ −1 E
τK
k=0 t=0

∇F (w¯k+1,t) 2

4 ≤
βτK

F (w0) − E [F (wK )] + 15βτ K(σF2 + γF2 )

≤

4(F (w0) − βτK

F ∗)

+

60(σF2

+

γF2 )

(111)

which gives us the desired result.

G On First-Order Approximations of Per-FedAvg

As we stated previously, the Per-FedAvg method, same as MAML, requires computing Hessianvector product which is computationally costly in some applications. As a result, one may consider using the ﬁrst-order approximation of the update rule for the Per-FedAvg algorithm. The main goal of this section is to show how our analysis can be extended to the case that we either drop the second-order term or approximate the Hessian-vector product using ﬁrst-order techniques.
To do so, we show that it suﬃces to only extend the result in Lemma 4.3 for the ﬁrst-order approximation settings and ﬁnd σ˜F such that E[ ∇˜ Fi(w) − ∇Fi(w) 2] ≤ σ˜F2 . One can easily check that the rest of analysis does not change, and the ﬁnal result (Theorem 4.5) holds if we just replace σF by σ˜F .
We next focus on two diﬀerent approaches, developed for MAML formulation, for approximating the Hessian-vector product, and show how we can characterize σ˜F for both cases:

• Ignoring the second-order term: Finn et al. (2017) suggested to simply ignore the
second-order term in the update of MAML to reduce the computation cost of MAML, i.e., to replace ∇˜ Fi(w) with

∇˜ fi w − α∇˜ fi(w, D), D .

(112)

This approach is known as First-Order MAML (FO-MAML), and it has been shown that it performs relatively well in many cases (Finn et al., 2017). In particular, Fallah et al. (2019) characterized the convergence properties of FO-MAML for the centralized MAML problem. Next, we characterize the variance of this gradient approximation.

Lemma G.1. Assume that we estimate ∇Fi(w) by (112) where D and D are independent batches with size D and D , respectively. Suppose that the conditions in Assumptions 2-4 are satisﬁed. Then, for any α ∈ [0, 1/L] and w ∈ Rd, we have E ∇˜ Fi(w) − ∇Fi(w) 2 ≤ σ˜F2 , where σ˜F2 is

given by

σ˜F2 := 2σG2

1 (αL)2 +
DD

+ 2(αLB)2.

Proof. In fact, in this case, ∇˜ Fi(w) is approximating

Gi(w) := ∇fi (w − α∇fi(w)) .

(113)

To characterize σ˜F note that E ∇˜ Fi(w) − ∇Fi(w) 2 ≤ 2E

∇˜ Fi(w) − Gi(w) 2 + 2E

Gi(w) − ∇Fi(w) 2 .

(114)

We bound these two terms separately. Note that we have already bounded the ﬁrst term in Appendix C (see (40)), and we have

E

2
∇˜ Fi(w) − Gi(w) ≤ σG2

1 (αL)2

+

D

D

.

(115)

22

To bound the second term in (114), note that

Gi(w) − ∇Fi(w) = α ∇2fi(w)∇fi (w − α∇fi(w)) ≤ α ∇2fi(w) · ∇fi (w − α∇fi(w)) ≤ αLB

(116)

where the ﬁrst inequality follows from the matrix norm deﬁnition and the last inequality is obtained using Assumption 2. Plugging (115) and (116) into (114), we obtain the desired result.

Note that while the ﬁrst term in σ˜F can be made arbitrary small by choosing D and D large enough, this is not the case for the second term. However, the second term is also negligible if α is small enough. Yet this bound suggests that this approximation introduces a non-vanishing error term which is directly carried to the ﬁnal result (Theorem 4.5).

• Estimating Hessian-vector product using gradient diﬀerences: In the context of MAML problem, it has been shown that the update of FO-MAML leads to an additive error that does not vanish as time progresses. To resolve this matter, Fallah et al. (2019) introduced another variant of MAML, called HF-MAML, which approximates the Hessian-vector product by gradient diﬀerences. More formally, the idea behind their method is that for any function g, the product of the Hessian ∇2g(w) by any vector v can be approximated by

∇g(w + δv) − ∇g(w − δv) 2δ

(117)

with an error of at most ρδ v 2, where ρ is the parameter for Lipschitz continuity of the Hessian of g. Building on this idea, in Per-FedAvg update rule, we can replace ∇˜ Fi(w) by

∇˜ fi w − α∇˜ fi(w, D), D − αd˜i(w)

(118)

where

d˜i(w) := ∇˜ fi w+δ∇˜ fi(w−α∇˜ fi(w, D), D ), D

−∇˜ fi w−δ∇˜ fi(w−α∇˜ fi(w, D), D ), D .
2δ (119)

For this approximation, we have the following result:

Lemma G.2. Assume that we estimate ∇Fi(w) by (118) where D, D , and D are independent batches with size D, D , and D , respectively. Suppose that the conditions in Assumptions 2-4 are satisﬁed. Then, for any α ∈ [0, 1/L] and w ∈ Rd, we have E ∇˜ Fi(w) − ∇Fi(w) 2 ≤ σ˜F2 , where

σ˜F2 is given by

σ˜F2 := 6σG2

2(αL)2 2

α2

D

+ D

+ 2δ2D

+ 2(αρδ)2B4.

Proof. Note that, this time ∇˜ Fi(w) is approximating

Gi(w) := ∇fi (w − α∇fi(w)) − αdi(w)

(120)

where

di(w)

:=

∇fi (w

+ δ∇fi (w

− α∇fi(w))) − ∇fi 2δ

(w

−

δ∇fi

(w

− α∇fi(w)))

(121)

is the term approximating ∇2fi(w)∇fi (w − α∇fi(w)). To characterize σ˜F , again and similar to (114), we have

E

∇˜ Fi(w) − ∇Fi(w) 2 ≤ 2E

∇˜ Fi(w) − Gi(w) 2 + 2E

2
Gi(w) − ∇Fi(w) .

(122)

We again bound both terms separately. To simplify the notation, let us deﬁne gi(w) := ∇fi (w − α∇fi(w)) , g˜i(w) := ∇˜ fi w − α∇˜ fi(w, D), D .

(123)

23

First, note that, using (a + b + c)2 ≤ 3(a2 + b2 + c2) for a, b, c ≥ 0, we have

∇˜ Fi(w) − Gi(w)

2
≤3

g˜i(w) − gi(w)

2

+

3α2 4δ2

∇˜ fi (w + δg˜i(w), D ) − ∇fi(w + δgi(w)) 2

3α2 + 4δ2

∇˜ fi (w − δg˜i(w), D

) − ∇fi(w − δgi(w))

2
.

(124)

Taking expectation from both sides, along with using (115), we have

E ∇˜ Fi(w) − Gi(w) 2

≤ 3σG2 +E

1 (αL)2 +
DD

3α2 + 4δ2

E

2
∇˜ fi (w + δg˜i(w), D ) − ∇fi(w + δgi(w))

2
∇˜ fi (w − δg˜i(w), D ) − ∇fi(w − δgi(w))

≤ 3σG2

α2

1 (αL)2

2δ2D

++ D

D

3α2 + 4δ2

E

∇fi (w + δg˜i(w)) − ∇fi(w + δgi(w)) 2

+E ∇fi (w − δg˜i(w)) − ∇fi(w − δgi(w)) 2

(125)

where (125) is obtained using the fact that D is independent from D and D which implies

E

∇˜ fi (w ± δg˜i(w), D ) − ∇fi(w ± δgi(w)) 2

≤ σG2 D

+ E ∇fi (w ± δg˜i(w)) − ∇fi(w ± δgi(w)) 2 .

Next, note that Assumption 2 yields

∇fi (w ± δg˜i(w)) − ∇fi(w ± δgi(w)) ≤ δL g˜i(w) − gi(w) .

Plugging this bound into (125) and using (114) implies

E

2
∇˜ Fi(w) − Gi(w) ≤ 3σG2

α2

(αL)2 1 (αL)2

2δ2D + (1 +

)+ 2D

D

≤ 3σG2

2(αL)2 2

α2

D

+ D

+ 2δ2D

where the last inequality is obtained using αL ≤ 1. Bounding the second term in (122) is more straightforward as we have

(126)

Gi(w) − ∇Fi(w) = α di(w) − ∇2fi(w)∇fi (w − α∇fi(w)) ≤ αρδ gi(w) 2 ≤ αρδB2. (127)

Plugging (126) and (127) into (122) gives us the desired result.

24

References
Agarwal, N., Suresh, A. T., Yu, F. X. X., Kumar, S., and McMahan, B. (2018). cpsgd: Communication-eﬃcient and diﬀerentially-private distributed sgd. In Advances in Neural Information Processing Systems, pages 7564–7575.
Antoniou, A., Edwards, H., and Storkey, A. (2019). How to train your MAML. In International Conference on Learning Representations.
Behl, H. S., Baydin, A. G., and Torr, P. H. S. (2019). Alpha MAML: adaptive model-agnostic meta-learning.
Chen, F., Dong, Z., Li, Z., and He, X. (2018). Federated meta-learning for recommendation. arXiv preprint arXiv:1802.07876.
Dai, X., Yan, X., Zhou, K., Ng, K. K., Cheng, J., and Fan, Y. (2019). Hyper-sphere quantization: Communication-eﬃcient sgd for federated learning. arXiv preprint arXiv:1911.04655.
Duchi, J. C., Jordan, M. I., and Wainwright, M. J. (2014). Privacy aware learning. Journal of the ACM (JACM), 61(6):38.
Fallah, A., Mokhtari, A., and Ozdaglar, A. (2019). On the convergence theory of gradient-based model-agnostic meta-learning algorithms. arXiv preprint arXiv:1908.10400.
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia.
Grant, E., Finn, C., Levine, S., Darrell, T., and Griﬃths, T. (2018). Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations.
Guha, N., Talwlkar, A., and Smith, V. (2019). One-shot federated learning. arXiv preprint arXiv:1902.11175.
Haddadpour, F. and Mahdavi, M. (2019). On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425.
Jiang, Y., Konečny`, J., Rush, K., and Kannan, S. (2019). Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488.
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2019). Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977.
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich, S. U., and Suresh, A. T. (2019). Scaﬀold: Stochastic controlled averaging for on-device federated learning. arXiv preprint arXiv:1910.06378.
Khaled, A., Mishchenko, K., and Richtárik, P. (2019). Tighter theory for local sgd on identical and heterogeneous data. arXiv preprint arXiv:1909.04746.
Khodak, M., Balcan, M.-F. F., and Talwalkar, A. S. (2019). Adaptive gradient-based meta-learning methods. In Advances in Neural Information Processing Systems, pages 5915–5926.
Konečny`, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication eﬃciency. arXiv preprint arXiv:1610.05492.
Langelaar, J. (2019). Mnist neural network training and testing. MATLAB Central File Exchange.
LeCun, Y. (1998). The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. (2019). On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189.
25

Li, Z., Zhou, F., Chen, F., and Li, H. (2017). Meta-SGD: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835.
Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. (2018). Don’t use large mini-batches, use local sgd. arXiv preprint arXiv:1808.07217.
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017a). CommunicationEﬃcient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 1273–1282, Fort Lauderdale, FL, USA. PMLR.
McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L. (2017b). Learning diﬀerentially private recurrent language models. arXiv preprint arXiv:1710.06963.
Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999.
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R. (2019). Fedpaq: A communication-eﬃcient federated learning method with periodic averaging and quantization. arXiv preprint arXiv:1909.13014.
Sahu, A. K., Li, T., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V. (2018). On the convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127.
Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S. (2017). Federated multi-task learning. In Advances in Neural Information Processing Systems, pages 4424–4434.
Stich, S. U. (2018). Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767.
Villani, C. (2008). Optimal transport: old and new, volume 338. Springer Science & Business Media.
Wang, J. and Joshi, G. (2018). Cooperative sgd: A uniﬁed framework for the design and analysis of communication-eﬃcient sgd algorithms. arXiv preprint arXiv:1808.07576.
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. (2018). Federated learning with non-iid data. arXiv preprint arXiv:1806.00582.
Zhu, W., Kairouz, P., Sun, H., McMahan, B., and Li, W. (2019). Federated heavy hitters discovery with diﬀerential privacy. arXiv preprint arXiv:1902.08534.
Zintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In Proceedings of the 36th International Conference on Machine Learning, pages 7693–7702.
26

