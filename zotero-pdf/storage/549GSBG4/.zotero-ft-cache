IEEE INTERNET OF THINGS JOURNAL

1

Privacy-preserving TrafÔ¨Åc Flow Prediction: A Federated Learning Approach
Yi Liu, Student Member, IEEE, James J.Q. Yu, Member, IEEE, Jiawen Kang, Dusit Niyato, Fellow, IEEE, Shuyu Zhang

arXiv:2003.08725v1 [cs.LG] 19 Mar 2020

Abstract‚ÄîExisting trafÔ¨Åc Ô¨Çow forecasting approaches by deep learning models achieve excellent success based on a large volume of datasets gathered by governments and organizations. However, these datasets may contain lots of user‚Äôs private data, which is challenging the current prediction approaches as user privacy is calling for the public concern in recent years. Therefore, how to develop accurate trafÔ¨Åc prediction while preserving privacy is a signiÔ¨Åcant problem to be solved, and there is a trade-off between these two objectives. To address this challenge, we introduce a privacy-preserving machine learning technique named federated learning and propose a Federated Learning-based Gated Recurrent Unit neural network algorithm (FedGRU) for trafÔ¨Åc Ô¨Çow prediction. FedGRU differs from current centralized learning methods and updates universal learning models through a secure parameter aggregation mechanism rather than directly sharing raw data among organizations. In the secure parameter aggregation mechanism, we adopt a Federated Averaging algorithm to reduce the communication overhead during the model parameter transmission process. Furthermore, we design a Joint Announcement Protocol to improve the scalability of FedGRU. We also propose an ensemble clustering-based scheme for trafÔ¨Åc Ô¨Çow prediction by grouping the organizations into clusters before applying FedGRU algorithm. Through extensive case studies on a real-world dataset, it is shown that FedGRU‚Äôs prediction accuracy is 90.96% higher than the advanced deep learning models, which conÔ¨Årm that FedGRU can achieve accurate and timely trafÔ¨Åc prediction without compromising the privacy and security of raw data.
Index Terms‚ÄîTrafÔ¨Åc Flow Prediction, Federated Learning, GRU, Privacy Protection, Deep Learning
I. INTRODUCTION
C ONTEMPORARY urban residents, taxi drivers, business sectors, and government agencies have a strong need of accurate and timely trafÔ¨Åc Ô¨Çow information [1] as these road users can utilize such information to alleviate trafÔ¨Åc congestion, control trafÔ¨Åc light appropriately, and improve the efÔ¨Åciency of trafÔ¨Åc operations [2]‚Äì[4]. TrafÔ¨Åc Ô¨Çow information can also be used by people to develop better travelling plans. TrafÔ¨Åc Flow Prediction (TFP) is to provide such trafÔ¨Åc Ô¨Çow information by using historical trafÔ¨Åc Ô¨Çow data to predict future trafÔ¨Åc Ô¨Çow. TFP is regarded as a critical element for
Yi Liu, James J.Q. Yu, and Shuyu Zhang are with the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China. Yi Liu is also with the School of Data Science and Technology, Heilongjiang University, Harbin, China (email: 97liuyi@ieee.org; yujq3@sustech.edu.cn; 11712122@mail.sustech.edu.cn).
Jiawen Kang and Dusit Niyato are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore (e-mail: kavinkang@ntu.edu.sg; dniyato@ntu.edu.sg).
This work is supported by the General Program of Guangdong Basic and Applied Basic Research Foundation under grant No. 2019A1515011032.

the successful deployment of Intelligent Transportation System (ITS) subsystems, particularly the advanced traveler information, online car-hailing, and trafÔ¨Åc management systems.
In TFP, centralized machine learning methods are typically utilized to predict trafÔ¨Åc Ô¨Çow by training with sufÔ¨Åcient sensor data, e.g., from mobile phones, cameras, radars, etc. For example, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and their variants have achieved gratifying results in predicting trafÔ¨Åc Ô¨Çow in the literature. Such learning methods typically collaboratively require sharing data among public agencies and private companies. Indeed, in recent years, the general public witnessed partnerships among public agencies and mobile service providers such as DiDi Chuxing, Uber, and Hellobike. These partnerships extend the capability and services of companies that provide real-time trafÔ¨Åc Ô¨Çow forecasting, trafÔ¨Åc management, car sharing, and personal travel applications [5].
Nonetheless, it is often overlooked that the data may contain sensitive private information, which leads to potential privacy leakage. As shown in Fig. 1, there are some privacy issues in the trafÔ¨Åc Ô¨Çow prediction context. For example, road surveillance cameras capture vehicle license plate information when monitoring trafÔ¨Åc Ô¨Çow, which may leak user private information [6] When different organizations use data collected by sensors to predict trafÔ¨Åc Ô¨Çow, the collected data is stored in different clouds and should not be exchanged for privacy preservation. These make it challenging to train an effective model with this valuable data. While the assumption is widely made in the literature, the acquisition of massive user data is not possible in real applications respecting privacy. Furthermore, Tesla Motors leaked the vehicle‚Äôs location information when using the vehicle‚Äôs GPS data to achieve trafÔ¨Åc prediction, which would cause many security risks to the owner of the vehicle1. In EU, Cooperative ITS (CITS)2 service providers must provide clear terms to end-users in a concise and accessible form so that users can agree to the processing of their personal data [7]. Therefore, it is important to protect privacy while predicting trafÔ¨Åc Ô¨Çow. To predict trafÔ¨Åc Ô¨Çow in ITS without compromising privacy, reference [8] introduced a privacy control mechanism based on ‚Äúk-anonymous diffusion,‚Äù which can complete taxi order scheduling without leaking user privacy. Le Ny et al. proposed a differentially private real-time trafÔ¨Åc state estimator system to predict trafÔ¨Åc Ô¨Çow in [9]. However, these privacy-preserving
1https://www.anquanke.com/post/id/197750 2https://new.qq.com/omn/20180411/20180411A1W9FI.html

IEEE INTERNET OF THINGS JOURNAL

2

Cloud A Cannot train a powerful model Cloud B Cannot train a powerful model Cloud C

Data cannot be exchanged
Government for privacy concerns

Company

Data cannot be exchanged for privacy concerns

Private data i.e., license plate number

Private data i.e., GPS

Camera

Sensors

Radar

Collect traffic flow information

Sensors

Station Private data i.e., Location
Bus Stop

Fig. 1. Privacy and security problems in trafÔ¨Åc Ô¨Çow prediction.

of trafÔ¨Åc Ô¨Çow data, thereby further improving the prediction accuracy. ‚Ä¢ We conduct extensive experiments on a real-world dataset to demonstrate the performance of the proposed schemes for trafÔ¨Åc Ô¨Çow prediction compared to non-federated learning methods.
The remainder of this paper is organized as follows. Section II reviews the literature on short-term TFP and privacy research in ITS. Section III deÔ¨Ånes the Centralized TFP Learning problem and Federated TFP Learning problem, and proposes a security parameter aggregation mechanism. Section IV presents FedGRU algorithm and ensemble clusteringbased FedGRU algorithm. In FedGRU, we introduce FedAVG algorithm, Joint-Announcement Protocol in detail. Section V and Section V-F discusse the experimental results. Concluding remarks are described in Section VI.

methods cannot achieve the trade-off between accuracy and privacy, rendering subpar performance. Therefore, we need to seek an effective method to accurately predict trafÔ¨Åc Ô¨Çow under the constraint of privacy protection.
To address the data privacy leakage issue, we incorporate a privacy-preserving machine learning technique named Federated Learning (FL) [10] for TFP in this work. In FL, distributed organizations cooperatively train a globally shared model through their local data without exchanging the raw data. To accurately predict trafÔ¨Åc Ô¨Çow, we propose an enhanced federated learning algorithm with a Gated Recurrent Unit neural network (FedGRU) in this paper, where GRU is an advanced time series prediction model that can be used to predict trafÔ¨Åc Ô¨Çow. Through FL and its aggregation mechanism [11], FedGRU aggregates model parameters from different geographically located organizations to build a global deep learning model under privacy well-preserved conditions. Furthermore, contributed by the outstanding data regression capability of GRU neural networks, FedGRU can achieve accurate and timely trafÔ¨Åc Ô¨Çow prediction for different organizations. The major contributions of this paper are summarized as follows:
‚Ä¢ Unlike existing algorithms, we propose a novel privacypreserving algorithm that integrates emerging federated learning with a practical GRU neural network for trafÔ¨Åc Ô¨Çow prediction. Such an algorithm provides reliable data privacy preservation through a locally training model without raw data exchange.
‚Ä¢ To improve the scalability and scalabiligy of federated learning in trafÔ¨Åc Ô¨Çow prediction, we design an improved Federated Averaging (FedAVG) algorithm with a JointAnnouncement protocol in the aggregation mechanism. This protocol uses random sub-sampling for participating organizations to reduce the communication overhead of the algorithm, which is particularly suitable for largescale and distribution prediction.
‚Ä¢ Based on FedGRU algorithm, we develop an ensemble clustering-based FedGRU scheme to integrate the optimal global model and capture the spatio-temporal correlation

II. RELATED WORK
A. TrafÔ¨Åc Flow Prediction
TrafÔ¨Åc Ô¨Çow forecasting has always been a hot issue in ITS, which serves as functions of real-time trafÔ¨Åc control and urban planning. Although researchers have proposed many new methods, they can generally be divided into two categories: parametric models and non-parametric models.
1) Parametric models: Parametric models predict future data by capturing existing data feature within its parameters. M. S. Ahmed et al. in [12] proposed the Autoregressive Integrated Moving Average (ARIMA) model in the 1970s to predict short-term freeway trafÔ¨Åc. Since then, many researchers have proposed variants of ARIMA such as Kohonen-ARIMA (KARIMA) [13], subset ARIMA [14], seasonal ARIMA [15], etc. These models further improve the accuracy of TFP by focusing on the statistical correlation of the data. Parametric models have several advantages. First of all, such parametric models are highly transparent and interpreted for easy human understanding. Second, these solutions usually take less time than non-parametric models. However, these solutions suffer from low model express ability, rarely solutions to achieve accurate and timely TFP.
2) Non-parametric models: With the improvement of data storage and computing, non-parametric models have achieved great success in TFP [16]. Davis and Nihan et al. in [17] proposed k-NN model for short-term trafÔ¨Åc Ô¨Çow prediction. Lv et al. in [1] Ô¨Årst applied the stacked autoencoder (SAE) model to TFP. Furthermore, SAE adopts a hierarchical greedy network structure to learn non-linear features and has better performance than Support Vector Machines (SVM) [18] and Feed-forward Neural Network (FFNN) [19]. Considering the timing of the data, Ma et al. in [20] and Tian et al. in [21] applied Long Short-Term Memory (LSTM) to achieve accurate and timely TFP. Fu et al.in [22] Ô¨Årst proposed GRU neural network methods for TFP. In recent years, due to the success of convolutional networks and graph networks, Yu et al. in [23], [24] proposed graph convolutional generative autoencoder to address the real-time trafÔ¨Åc speed estimation problem.

IEEE INTERNET OF THINGS JOURNAL

3

B. Privacy Issues for Intelligent Transportation Systems
In ITS, many models and methods rely on training data from users or organizations. However, with the increasing privacy awareness, direct data exchange among users and organizations is not permitted by law. Matchi et al. in [25] developed privacy-preserving service to compute meeting points in ridesharing based on secure multi-party computing, so that each user remains in control of his location data. Brian et al. [26] designed a data sharing algorithm based on informationtheoretic k-anonymity. This data sharing algorithm implements secure data sharing by k-anonymity encryption of the data. The authors in [27] proposed a privacy-preserving transportation trafÔ¨Åc measurement scheme for cyber-physical road systems by using maximum-likelihood estimation (MLE) to obtain the prediction result. Reference [28] presents a system based on virtual trip lines and an associated cloaking technique to achieve privacy-protected trafÔ¨Åc Ô¨Çow monitoring. To avoid leaking the location of vehicles while monitoring trafÔ¨Åc Ô¨Çow, [29] proposed a privacy-preserving data gathering scheme by using encrypt methods. Nevertheless, these approaches have two problems: 1) they respects privacy at the expense of accuracy; 2) they cannot properly handle a large amount of data within limit time [30]. Besides, the EU has promulgated General Data Protection Regulation (GDPR), which means that as long as the organization has the possibility of revealing privacy in the data sharing process, the data transaction violates the law [31]. Therefore, we need to develop new methods to adapt to the general public with a growing sense of privacy.
In recent years, Federated Learning (FL) models have been used to analyze private data because of its privacy-preserving features. FL is to build machine-learning models based on datasets that are distributed across multiple devices while preventing data leakage [32]. Bonawitz et al. in [33] Ô¨Årst applied FL to decentralized learning of mobile phone devices without compromising privacy. To ensure the conÔ¨Ådentiality of the user‚Äôs local gradient during the federated learning process, the author in [34] proposed VerifyNet, which is the Ô¨Årst framework to protect privacy and veriÔ¨Åable federated learning. Reference [35] used a multi-weight subjective logic model to design a reputation-based device selection scheme for reliable federated learning. The authors in [36] applied the federated learning framework to edge computing to achieve privacyprotected data analysis. Nishio et al. in [37] applied FL to mobile edge computing and proposed the FedCS protocol to reduce the time of the training process. Chen et al. in [38] combined transfer learning [39] with FL to propose FedHealth to be applied to healthcare. Yang et al. in [32] introduced Federated Machine Learning, which can be applied to multiple applications in smart cities such as energy demand forecasting.
Although researchers have proposed some privacypreserving methods to predict trafÔ¨Åc Ô¨Çow, they do not comply with the requirements of GDPR. In this paper, we explore a privacy-preserving method FL with GRU for trafÔ¨Åc Ô¨Çow prediction.
III. PROBLEM DEFINITION
We use the term ‚Äúorganization‚Äù throughout the paper to describe entities in TFP, such as urban agencies, private

Organization Device
Secure Parameters Aggregation
Parameters

‚àë

Cloud

aggregates

organizations' updates

into a new global

model.

Local Training Data
Homomorphic Encryption

Fig. 2. Secure parameter aggregation mechanism.
companies, and detector stations. We use the term ‚Äúclient‚Äù to describe computing nodes that correspond to one or multiple sensors in FL and use the term ‚Äúdevice‚Äù to describe the sensor in the organizations. Let C = {C1, C2, ¬∑ ¬∑ ¬∑ , Cn} and O = {O1, O2, ¬∑ ¬∑ ¬∑ , Om} denote the client set and organization set in ITS, respectively. In the context of trafÔ¨Åc Ô¨Çow prediction, we treat organizations as clients in the deÔ¨Ånition of federated learning. This equivalency does not undermine the privacy preservation constraint of the problem and the federated learning framework. Each organization has ki devices and their respective database Di. We aim to predict the number of vehicles with historical trafÔ¨Åc Ô¨Çow information from different organizations without sharing raw data and privacy leakage. We design a secure parameter aggregation mechanism as follows:
Secure Parameter Aggregation Mechanism: Detector station Oi has N devices, and the trafÔ¨Åc Ô¨Çow data collected by the N devices constitute a database Di. The deep learning model constructed in Oi calculates updated model parameters pi using the local training data from Di. When all detector stations Ô¨Ånish the same operation, they upload their respective pi to the cloud and aggregate a new global model.
According to Secure Parameters Aggregation, no trafÔ¨Åc Ô¨Çow data is exchanged among different detector stations. The cloud aggregates organizations‚Äô submitted parameters into a new global model without exchanging data. (As shown in Fig. 2)
In this paper, t and vt represent the t-th timestamp in the time-series and trafÔ¨Åc Ô¨Çow at the t-th timestamp, respectively. Let f (¬∑) be the trafÔ¨Åc Ô¨Çow prediction function, the deÔ¨Ånitions of privacy, centralized, and federated TFP learning problems as follows:
Information-based Privacy: Information-based privacy deÔ¨Ånes privacy as preventing direct access to private data. This data is associated with the user‚Äôs personal information or location. For example, a mobile device that records location data allows weather applications to directly access the location of the current smartphone, which actually violates the information-based privacy deÔ¨Ånition [11]. In this work, every device trains its local model by using local dataset instead of sharing the dataset and upload the updated gradients (i.e., parameters) to the cloud.
Centralized TFP Learning: Given organizations O, each organization‚Äôs devices ki, and an aggregated database D =

IEEE INTERNET OF THINGS JOURNAL

4

D1 ‚à™ D2 ‚à™ D3 ‚à™ ¬∑ ¬∑ ¬∑ ‚à™ DN , the centralized TFP problem is to calculate vt+s = f (t + s, D), where s is the prediction window after t.
Federated TFP Learning: Given organizations O and each organization‚Äôs devices ki, and their respective database Di, the federated TFP problem is to calculate vt+s = fi(t+s, Di) where fi(¬∑, ¬∑) is the local version of f (¬∑, ¬∑) and s is the prediction window after t. Subsequently, the produced results
are aggregated by a secure parameter aggregation mechanism.

for the input sample xi is yi ‚àà R. If we input the training

sample vector xi (e.g., the trafÔ¨Åc Ô¨Çow data), we need to Ô¨Ånd the model parameter vector œâ ‚àà Rd that characterrizes the

output yi (e.g., the value output of the trafÔ¨Åc Ô¨Çow data) with

loss

function

fi(œâ)

(e.g.,

fi(œâ)

=

1 2

(xTi

œâ

‚àí yi)).

Our

goal

is

to learn this model under the constraints of local data storage

and processing by devices in the organization with a secure

parameter aggregation mechanism. The loss function on the

data set of device k is deÔ¨Åned as:

IV. METHODOLOGY
Traditional centralized learning methods consist of three steps: data processing, data fusion, and model building. In the traditional centralized learning context, data processing means that data feature and data label need to be extracted from the original data (e.g. text, images, and application data) before performing the data fusion operation. SpeciÔ¨Åcally, data processing includes sample sampling, outlier removal, feature normalization processing, and feature combination. For the data fusion step, traditional learning models directly share data among all parties to obtain a global database for training. Such a centralized learning approach faces the challenge of new data privacy laws and regulations as organizations may disclose privacy and violate laws such as GDPR when sharing data. FL is introduced into this context to address the above challenges. However, existing FL frameworks typically employ simple machine learning models such as XGBoost and decision trees rather than complicated deep learning models [10], [40]. Because such models need to upload a large number of parameters to the cloud in FL framework, it leads to expensive communication overhead which can cause training failures for a single model or a global model [41], [42]. Therefore, FL framework needs to develop a new parameter aggregation mechanism for deep learning models to reduce communication overhead.
In this section, we present two approaches to predict trafÔ¨Åc Ô¨Çow, including FedGRU and clustering-based FedGRU algorithms. SpeciÔ¨Åcally, we describe an improved Federated Averaging (FedAVG) algorithm with a Joint-Announcement protocol in the aggregation mechanism to reduce the communication overhead. This approach is useful to implement in the following particular scenarios.

A. Federated Learning and Gated Recurrent Unit

Federated Learning (FL) [10] is a distributed machine learn-

ing (ML) paradigm that has been designed to train ML models

without compromising privacy. With this scheme, different

organizations can contribute to the overall model training

while keeping the training data locally.

Particularly, FL problem involves learning a single and

globally predicted model from the database separately stored

in dozens of or even hundreds of organizations [43], [44]. We

assume that a set K of K device stores its local dataset Dk

of size Dk. So we can deÔ¨Åne the local training dataset size

D=

K k=1

Dk

.

In

a

typical

deep

learning

setting,

given

a

set of input-output pairs {xi, yi}Di=k1, where the input sample

vector with d features is xi ‚àà Rd, and the labeled output value

1

Jk(œâ) := Dk

fi(œâ)Œªh(œâ),
i‚ààDk

(1)

where œâ ‚àà Rd is the local model parameter, ‚àÄŒª ‚àà [0, 1] , and h(¬∑) is a regularizer function. Dk is used in Eq. (1) to illustrate that the local model in device k needs to learn every sample in the local data set.
At the cloud, the global predicted model problem can be represented as follows:

arg min J(œâ), J(œâ) =
œâ‚ààRd

K k=1

Hk D

Fk (œâ ),

(2)

we recast the global predicted model problem in (6) as follows:

arg min J(œâ) :=
œâ‚ààRd

K

i‚ààDk fi(œâ) + Œªh(œâ) . (3)

k=1

D

The Eq. (2)-(3) illustrates the global model aggregation of model update aggregations uploaded by each device to obtain updates.
For the TFP problem, we regard GRU neural network model as the local model in Eq. (1). Cho et al. in [45] proposed the GRU neural network in 2014, which is a variant of RNN that handles time-series data. GRU is different from RNN is that it adds a ‚ÄúProcessor‚Äù to the algorithm to judge whether the information is useful or not. The structure of the processor is called ‚ÄúCell.‚Äù A typical structure of GRU cell uses two data ‚Äúgates‚Äù to control the data from processor: reset gate r and update gate z.
Let X = {x1, x2, ¬∑ ¬∑ ¬∑ , xn}, Y = {y1, y2, ¬∑ ¬∑ ¬∑ , yn}, and H = {h1, h2, ¬∑ ¬∑ ¬∑ , hn} be the input time series, output time series and the hidden state of the cells, respectively. At time step t, the value of update gate zt is expressed as:

zt = œÉ(W (z)xt + U (z)ht‚àí1),

(4)

where xt is the input vector of the t-th time step, W (z) is the weight matrix, and ht‚àí1 holds the cell state of the previous time step t ‚àí 1. The update gate aggregates W (z)xt and U (z)ht‚àí1, then maps the results in (0, 1) through a Sigmoid activation function. Sigmoid activation can transform data into
gate signals and transfer them to the hidden layer. The reset
gate rt is computed similarly to the update gate:

rt = œÉ(W (z)xt + U (r)ht‚àí1).

(5)

The candidate activation ht is denoted as:

ht = tanh(W xt + rt U ht‚àí1),

(6)

where rt U ht‚àí1 represents the Hadamard product of rt and U ht‚àí1. The tanh activation function can map the data to the

IEEE INTERNET OF THINGS JOURNAL

5

+

+

++

++

FedAVG algorithm

‚àë
Joint-Announcement Protocol

Updated Gradient

Send the new global model to each organizations.

Local Model

Local data Government

Company
Small-scale organization

Station

Organization
Large-scale organization

Fig. 3. Federated learning-based trafÔ¨Åc Ô¨Çow prediction architecture. Note that when the organization in the architecture is small-scale, we will use the FedAVG algorithm to calculate the update, and when the organization is largescale, we will use the Joint-Announcement Protocol to calculate the update by subsampling the organizations. Details will be described in detail in following sub-subsection IV-B3.

range of (-1,1), which can reduce the amount of calculations

and prevent gradient explosions.

The Ô¨Ånal memory of the current time step t is calculated as

follows:

ht = zt ht‚àí1 + (1 ‚àí zt) ht .

(7)

B. Privacy-preserving TrafÔ¨Åc Flow Prediction Algorithm
Since centralized learning methods use central database D to merge data from organizations and upload the data to the cloud, it may lead to expensive communication overhead and data privacy concerns. To address these issues, we propose a privacy-preserving trafÔ¨Åc Ô¨Çow prediction algorithm FedGRU as shown in Fig. 3. Firstly, we introduce FedAVG algorithm as the core of the secure parameter aggregation mechanism to collect gradient information from different organizations. Secondly, we design an improved FedAVG algorithm with a Joint-Announcement protocol in the aggregation mechanism. This protocol uses random sub-sampling for participating organizations to reduce the communication overhead of the algorithm, which is particularly suitable for larger-scale and distribution prediction. Finally, we give the details of FedGRU, which is a prediction algorithm combining FedAVG and JointAnnouncement protocol.
1) FedAVG algorithm: A recognized problem in federated learning is the limited network bandwidth that bottlenecks cloud-aggregated local updates from the organizations. To reduce the communication overhead, each client uses its local data to perform gradient descent optimization on the current model. Then the central cloud performs a weighted average aggregation of the model updates uploaded by the clients. As shown in Algorithm 2, FedAVG consists of three steps:
(i) The cloud selects volunteers from organizations O to participate in this round of training and broadcasts global model œâo to the selected organizations;
(ii) Each organization o trains data locally and updates œâto for E epochs of SGD with mini-batch size B to obtain œâto+1, i.e., œâto+1 ‚Üê LocalUpdate(o, œâto);

Algorithm 1: Federated Averaging (FedAVG) Algorithm.

Input: Organizations O = {O1, O2, ¬∑ ¬∑ ¬∑ , ON }. B is the local mini-batch size, E is the number of local

epochs, Œ± is the learning rate, ‚àáL(¬∑; ¬∑) is the

gradient optimization function.

Output: Parameter œâ. 1 Initialize œâ0 (Pre-trained by a public dataset);

2 foreach round t = 1, 2, ¬∑ ¬∑ ¬∑ do

3 {Ov} ‚Üê select volunteer from organizations O

participate in this round of training;

4 Broadcast global model œâo to organization in {Ov};

5 foreach organization o ‚àà {Ov} in parallel do

6

Initialize œâto = œâo;

7

œâto+1 ( œâto+1 ‚Üê LocalUpdate(o, œâto);

8

œât+1

‚Üê

1 |{Ov }|

o‚ààOv œâto+1;

9 LocalUpdate(o, œâto): // Run on organization o ; 10 B ‚Üê (split So into batches of size B);
11 if each local epoch i from 1 to E then

12 if batch b ‚àà B then

13

œâ ‚Üê œâ ‚àí Œ± ¬∑ ‚àáL(œâ; b);

14 return œâ to cloud

(iii) The cloud aggregates each organization‚Äôs œât+1 through a secure parameter aggregation mechanism.
FedAVG algorithm is a critical mechanism in FedGRU to reduce the communication overhead in the process of transmitting parameters. This algorithm is an iterative process. For the i-th round of training, the models of the organizations participating in the training will be updated to the new global one.
2) Federated Learning-based Gated Recurrent Unit neural network algorithm: FedGRU aims to achieve accurate and timely TFP through FL and GRU without compromising privacy. The overview of FedGRU is shown in Fig. 3. It consists of four steps:
i) The cloud model is initialized through pre-training that utilizes domain-speciÔ¨Åc public datasets without privacy concerns;
ii) The cloud distributes the copy of the global model to all organizations, and each organization trains its copy on local data;
iii) Each organization uploads model updates to the cloud. The entire process does not share any private data, but instead sharing the encrypted parameters;
iv) The cloud aggregates the updated parameters uploaded by all organizations by the secure parameter aggregation mechanism to build a new global model, and then distributes the new global model to each organization.
Given voluntary organization {Ov} ‚äÜ O and ov ‚àà {Ov}, referring to the GRU neural network in Section IV-A, we have:

zvt = œÉ(W (zv) + U (zv)htv‚àí1),

(8)

rvt = œÉ(W (rv) + U (rv)hvt‚àí1),

(9)

htv = tanh(W xtv + rvt U htv‚àí1),

(10)

IEEE INTERNET OF THINGS JOURNAL

6

Algorithm 2: Federated Learning-based Gated Recurrent Unit neural network (FedGRU) algorithm.

Input: {Ov} ‚äÜ O, X, Y and H. The mini-batch size m,

the number of iterations n and the learning rate

Œ±. The optimizer SGD.

Output: J (œâ), œâ and Wvr, Wvz, Wvh. 1 According to X, Y , H and Eq. (8)‚Äì(12), initialize the

cloud model J (œâ0), œâ0, Wvr0 , Wvz0 , Wvh0 , and Hv0; 2 foreach round i = 1, 2, 3, ¬∑ ¬∑ ¬∑ do

3 {Ov} ‚Üê select volunteer from organizations to

participate in this round of training;

4 while gœâ has not convergence do

5

foreach organization o ‚àà Ov in parallel do

6

Conduct a mini-batch input time step

{xv (i) }m i=1 ;

7

Conduct a mini-batch true trafÔ¨Åc Ô¨Çow

{yv (i) }m i=1 ;

8

Initalize œâto+1 = œâto;

9

gœâ

‚Üê

‚àáœâ

1 m

m i=1

fœâ(x(vi)) ‚àí yv(i)

2
;

10

œâto+1 ‚Üê œâto + Œ± ¬∑ SGD(œâto, gœâ);

11

Update the parameters Wvr0 , Wvz0 , Wvh0 , and

Hv0;

12

Update reset gate r and update gate z;

13 Collect the all parameters from {Ov} to update œât+1. (Referring to the Algorithm 1.);
14 return J (œâ), œâ and Wvr, Wvz, Wvh

htv = zvt htv‚àí1 + (1 ‚àí zvt ) htv .

(11)

where X = {x1v, x2v, ¬∑ ¬∑ ¬∑ , xnv }, Y = {yv1, yv2, ¬∑ ¬∑ ¬∑ , yvn}, H = {h1v, h2v, ¬∑ ¬∑ ¬∑ , hnv } denote ov‚Äôs input time series, ov‚Äôs output time series and the hidden state of the cells, respectively. According to Eq. (3), the objective function of FedGRU is as follows:

K
arg min J(œâ) :=

i‚ààDv

fi(œâ)

+

Œªh(œâ) .

(12)

œâ‚ààRd

k=1

D

The pseudocode of FedGRU is presented in Algorithm 3: 3) Joint-Announcement Protocol: Generally, the number
of participants in FedGRU is small. For example, WeBank worked with 7 auto insurance companies to build a trafÔ¨Åc violation insurance pricing model using FL3. In this case, since there are only 8 participants, we can deÔ¨Åne it as a small-scale federated learning model. However, a large number of participants may join FedGRU for trafÔ¨Åc Ô¨Çow forecasting. When FedGRU is expanded to a large-scale scenario with many participants, FedAVG algorithm is hard to converge because of expensive communication overhead, thereby the accuracy of FedGRU will decrease. To address this issue, we design an improved FedAVG algorithm with a Joint-Announcement protocol in the aggregation mechanism to randomly select a certain proportion of organizations from a large number of participants in the i-th round training.

3https://www.fedai.org/cases/a-case-of-trafÔ¨Åc-violations-insurance-usingfederated-learning/

Preparation

Training

Aggregation

Training

‚ë£ Training

‚ë†

‚ë¢

Training

‚ë°
Organization

‚ë§
Round i

Aggregation
‚ë•

Round i+1

Cloud

Persistent storage

Rejection

Network failure

Fig. 4. Federated learning joint-announcement protocol.

The participants in the Joint-Announcement protocol are organizations and the cloud, which is a cloud-based distributed service [46]. For i-th round of training, the protocol consists of three phases: preparation, training, and aggregation. The speciÔ¨Åc implementation phases of the protocol are given as follows:
i) Phase 1, Preparation: Given a FL task (i.e., trafÔ¨Åc Ô¨Çow prediction task in this paper), the organizations that voluntarily participate will check-in with the Cloud (as shown in Fig. 4‚Äì 1 ). Who rejects ones represent if unwillingness to participate in this task or have other failures.
ii) Phase 2, Training: First, the cloud loads the pre-trained model (as shown in 2 ). Then the cloud sends the model checkpoint (i.e., gradient information) to the organizations (as shown in Fig. 4‚Äì 3 ). The cloud randomly selects a Ô¨Åxed proportion (e.g., 10%, 20%, ¬∑ ¬∑ ¬∑) of organizations to participate in this round of training (as shown in Fig. 4‚Äì 4 ). Each organization will train the data locally and send the parameters to the cloud.
iii) Phase 3, Aggregation: Subsequently, the cloud aggregates the parameters uploaded by organizations to update the global model through the security parameter aggregation mechanism (as shown in Fig. 4‚Äì 5 ). In this mechanism, the cloud executes FedAVG algorithm (presented in Section IV-B1) to reduce the uplink communication costs. The cloud updates the global model by sending checkpoints to persistent storage (as shown in Fig. 4‚Äì 6 ). Finally, the global model sends update parameters to each organization.

C. Ensemble Clustering Federated Learning-Based TrafÔ¨Åc Flow Prediction Algorithm
Since the organizations select FL tasks based on its location information in the federated trafÔ¨Åc Ô¨Çow prediction learning problem, for the same FL task, better spatio-temporal correlation of the data, leads to better performance. Based on the above hypothesis, we propose the ensemble clusteringbased FedGRU algorithm to obtain better prediction accuracy and handle scenarios in which a large number of clients

IEEE INTERNET OF THINGS JOURNAL

7

jointly work on training a trafÔ¨Åc Ô¨Çow prediction problem by

grouping organizations into K clusters before implementing

FedGRU. Then it integrates the global model of each cluster

center by using an ensemble learning scheme, thereby obtains

the best accuracy. In this scheme, the clustering decision is

determined by using the latitude and longitude information of

the organizations. We use the constrained K-Means algorithm

proposed in [47]. According to the deÔ¨Ånition of the constrained

K-Means clustering algorithm, our goal is to determine the

cluster center that minimizes the Sum of Squared Error

(SSE).

More

formally,

given

a

set

P

of

m

points

in

n
R

(i.e

organizations‚Äô location information) and the minimum cluster

membership values Œ∫h ‚â• 0, h = 1, ..., k, cluster centers C1t, C2t, ..., Ckt at iteration t, compute C1t+1, C2t+1, ..., Ckt+1 at
iteration t + 1 in the following three steps as follows:

i) Sum of the Euclidean Metric Distance Squared:

d(x, y)2 = n (xi ‚àí yi)2 = ||x ‚àí y||22,
i=1

(13)

SSE =

m i=1

k h=1

œÑi,h(

1 2

||xi

‚àí

Cht ||22).

where œÑi,h is a solution to the following linear program with Cht Ô¨Åxed. ii) Cluster Assignment: To minimize SSE, we have

min

SSE

œÑ

m i=1

œÑi,h

‚â•

Œ∫h,

h

=

1,

2,

¬∑

¬∑

¬∑,

k

subject to

k h=1

œÑi,h

=

1,

i

=

1,

2,

¬∑

¬∑

¬∑

,

m

œÑi,h ‚â• 0, i = 1, 2, ¬∑ ¬∑ ¬∑ , m, h = 1, ¬∑ ¬∑ ¬∑ , k.

(14)

iii) Cluster Update: Update Ch(t+1) as follows:

Cth+1 =

m i=1

œÑit,h xi

m i=1

œÑit,h

Cht

if

m i=1

œÑit,h

>

0,

(15)

otherwise.

If and only if SSE is minimum and Cht+1 = Cht (h = 1, ¬∑ ¬∑ ¬∑ , k), we can obtain the optimal clustering center Ck and the optimal set Pk = {P i}m i=1. Let ‚Ñ¶k = {œâ1, œâ2, ¬∑ ¬∑ ¬∑ , œâk} denote the global model of the optimal set Pk. As shown in Fig. 5, we utilize the ensemble learning scheme to Ô¨Ånd the optimal
ensemble model by integrating the global model from ‚Ñ¶k with the best accuracy after executing the constrained K-Means
algorithm. More formally, such a scheme needs to Ô¨Ånd an
optimal global model subset of the following equation:

1 j‚â§k

max
‚Ñ¶

|‚Ñ¶j |

j=1

‚Ñ¶j, where

‚Ñ¶j

‚äÜ

‚Ñ¶k .

(16)

The ensemble clustering-based FedGRU is thus presented
in Algorithm 3. It consists of three steps:
i) Given organization set O, we random initialize cluster centers Cht , and execute the constrained K-Means algorithm;
ii) With the optimal clustering center Ck and the optimal set Ok = {Oi}m i=1, the cloud executes the ensemble scheme to Ô¨Ånd the optimal global model set ‚Ñ¶j (i.e., subset of ‚Ñ¶k );
iii) The cloud sends the new global model to each organiza-
tion.

Algorithm 3: Ensemble clustering federated learningbased FedGRU algorithm.
Input: Organizations set O = {Oi}m i=1. Output: J (œâ), œâ and Wvr, Wvz, Wvh. 1 Initialize random cluster center Cht ; 2 while Cht+1 = Cht (h = 1, ¬∑ ¬∑ ¬∑ , k) do 3 Execute the constrained K-Means algorithm‚Äôs step 1
and step 2 (Referring to Eq. (13)‚Äì(14)); 4 Update Ch(t+1) according to Eq. (15) in step 3 of the
constrained K-Means algorithm;
5 foreach clustering center Ck and the optimal set Ok = {Oi}ki=1 do
6 Execute FedGRU algorithm;
7 Obtain the global model set ‚Ñ¶k; 8 Execute the ensemble learning scheme to Ô¨Ånd the
optimal global model subset ‚Ñ¶j (Referring to Eq. (16); 9 The cloud sends the new global model to each
organization; 10 return J (œâ), œâ and Wvr, Wvz, Wvh.

Œ©1

Œ©2

Œ©3

Œ©4

Œ©1 Œ©2 Œ©4
Optimal Global Model Subset

Clustering Algorithm

Ensemble Model Ensemble Learning Scheme

Fig. 5. Ensemble clustering federated learning-based trafÔ¨Åc Ô¨Çow prediction scheme.

V. EXPERIMENTS
In this experiment, the proposed FedGRU and clusteringbased FedGRU algorithms are applied to the real-world data collected from the Caltrans Performance Measurement System (PeMS) [48] database for performance demonstration. The trafÔ¨Åc Ô¨Çow data in PeMS database was collected from over 39,000 individual detectors in real time. These sensors span the freeway system across all major metropolitan areas of the State of California [1]. In this paper, trafÔ¨Åc Ô¨Çow data collected during the Ô¨Årst three months of 2013 is used for experiments. We select the trafÔ¨Åc Ô¨Çow data in the Ô¨Årst two months as the training dataset and the third month as the testing dataset. Furthermore, since the trafÔ¨Åc Ô¨Çow data is timeseries data, we need to use them at the previous time interval, i.e., xt‚àí1, xt‚àí2, ¬∑ ¬∑ ¬∑ , xt‚àír, to predict the trafÔ¨Åc Ô¨Çow at time interval t, where r is the length of the history data window.
We adopt Mean Absolute Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE) to indicate the prediction accuracy as follows:

1n

MAE = n

|yi‚àíyÀÜp|,

(17)

i=1

IEEE INTERNET OF THINGS JOURNAL

8

1 MSE =
n

n

(yi ‚àí yÀÜp)2,

(18)

i=1

1 RMSE = [
n

n

(|yi

‚àí

yÀÜp|)2]

1 2

,

(19)

i=1

100% MAPE =

n

| yÀÜp ‚àí yi |.

(20)

n
i=1

yi

Where yi is the observed trafÔ¨Åc Ô¨Çow, and yÀÜp is the predicted trafÔ¨Åc Ô¨Çow.
Without loss of generality, we assume that the detector stations are distributed and independent and the data cannot be exchanged arbitrarily among them. In the secure parameter aggregation mechanism, PySyft [49] framework is adopted to encrypt the parameters4. The FedGRU code is available at https://github.com/niklausliu/TF FedGRU demo.
For the cloud and each organization, we use mini-batch SGD for model optimization. PeMS dataset is split equally and assigned to 20 organizations. During the simulation, learning rate Œ± = 0.001, mini-batch size m = 128, and |Ov|= 20. Note that the client C = 2 of the FedGRU model is the default setting in FL [10]. All experiments are conducted using TensorFlow and PyTorch [50] with Ubuntu 18.04.

A. FedGRU Model Architecture
In the context of deep learning, proper hyperparameter selection, e.g. the size of the input layer, the number of hidden layers, and hidden units in each hidden layer, is a notable factor that determines the model performance. In this section, we investigate the performance of FedGRU with different hyperparameter conÔ¨Ågurations and try to determine the bestperforming neural network architecture for it. Additionally, we also obtain the optimal length of history data window r = 12. In particular, we employ r = 12, the number of hidden layers in [1, 3], and the number of hidden units in {50, 100, 150} [1] to adjust the structure of FedGRU. Furthermore, we utilize the grid search approach to Ô¨Ånd the best architecture for FedGRU.
We Ô¨Årst evaluate the performance of FedGRU on a 5-min trafÔ¨Åc Ô¨Çow prediction task through MAE, MSE, RMSE, and MAPE. After performing the grid search, we obtain the best architecture of FedGRU as shown in Table I. The optimal architecture consists of two hidden layers, each with a hidden layer number of {50, 50}. The results show that the optimal number of hidden layers in our experiment is two. From a model perspective, the number of hidden layers of FedGRU model should not be too small or too large. Our results conÔ¨Årm these facts.

B. TrafÔ¨Åc Flow Prediction Accuracy
We compared the performance of the proposed FedGRU model with that of GRU, SAE, LSTM, and support vector machine (SVM) with an identical simulation conÔ¨Åguration. Among these Ô¨Åve competing methods, FedGRU is a federated machine learning model, and the rest are centralized ones.
4https://github.com/OpenMined/PySyft

(a)

0.035

GRU FedGRU

0.030

0.025

0.020

0.015

0.010

0.005

Loss

0

100 200 300 400 500 600

epoch

(b)

Fig. 6. (a) TrafÔ¨Åc Ô¨Çow prediction of GRU model and FedGRU model. (b) Loss of GRU model and FedGRU model.

Among them, GRU is a widely-adopted baseline model that has good performance for trafÔ¨Åc Ô¨Çow forecast tasks, as aforementioned in Section IV, and SVM is a popular machine learning model for general prediction applications [1]. In all investigations, we use the same PeMS dataset. The prediction results are given in Table II for 5-min ahead trafÔ¨Åc Ô¨Çow prediction. From the simulation results, it can be observed that MAE of FedGRU is lower than those of SAEs, LSTM, and SVM but higher than that of GRU. SpeciÔ¨Åcally, MAE of FedGRU is 9.04% lower than that of the worst case (i.e., SVM) in this experiment. This result is contributed by the fact that FedGRU inherits the advantages of GRU‚Äôs outstanding performance in prediction tasks.
Fig. 6(a) shows a comparison between GRU and FedGRU for a 5-min trafÔ¨Åc Ô¨Çow prediction task. We can Ô¨Ånd that the predict results of FedGRU model are very close to that of GRU. This is because the core technique of FedGRU to prediction is GRU structure, so the performance of FedGRU is comparable to GRU model. Furthermore, FedGRU can protect data privacy by keeping the training dataset locally. Fig. 6(b) illustrates the loss of GRU model and FedGRU model. From the results, the loss of FedGRU model is not signiÔ¨Åcantly different from GRU model. This proves that FedGRU model has good convergence and stability. In a word, FedGRU can achieve accurate and timely trafÔ¨Åc Ô¨Çow prediction without compromising privacy.

IEEE INTERNET OF THINGS JOURNAL

9

TABLE I STRUCTURE OF FEDGRU FOR TRAFFIC FLOW PREDICTION

Metrics FedGRU (default setting)

Time steps 5

Hidden layers 1 2 3

Hidden units
50 100 150 50, 50 100, 100 150, 150 50, 50, 50 100, 100, 100 150, 150, 150

MAE
9.03 8.96 8.46 7.96 8.64 8.75 8.29 8.41 8.75

MSE
103.24 102.36 101.05 101.49 102.21 102.91 102.17 103.01 103.98

RMSE
13.26 14.32 14.98 11.04 15.06 14.93 12.05 13.45 13.74

MAPE
19.12 18.94 18.46 17.82 19.22 19.35 18.62 18.96 19.24

TABLE II PERFORMANCE COMPARSION OF MAE, MSE, RMSE, AND MAPE FOR FEDGRU, LSTM, SAE, AND SVM

Metrics
FedGRU (default setting) GRU [22] SAE [1] LSTM [20] SVM [51]

MAE
7.96 7.20 8.26 8.28 8.68

MSE
101.49 99.32 99.82 107.16 115.52

RMSE
11.04 9.97 11.60 11.45 13.24

MAPE
17.82% 17.78% 19.80% 20.32% 22.73%

3UHGLFWLRQ(UURU
&RPPXQLFDWLRQ2YHUKHDG0%


1RQ)HG$9*



)HG$9* 0$(

506(

 0$3(







 C = 2 C = 4 C = 8 C = 10C > 10
&OLHQW1XPEHU



)HG*58

/DUJHVFDOH)HG*58











 &   &   &   &   0HWULFV

Fig. 8. Communication overhead between large-scale FedGRU and FedGRU.

Fig. 7. The prediction error of FedGRU model with different client numbers.
C. Performance Comparison of FedGRU Model Under Different Client Numbers
In Section V-B, the default client number is set C = 2. However, it is highly plausible that trafÔ¨Åc data can be gathered by more than two entities, e.g., organizations and companies. In this experiment, we explore the impact of different client numbers (i.e., C = 2, 4, 8, 10) on the performance of FedGRU. The simulation results are presented in Fig. 7, where we observe that the number of clients has an adverse inÔ¨Çuence on the performance of FedGRU. The reason is that more clients introduce increasing communication overhead to the underlying communication infrastructure, which makes it more difÔ¨Åcult for the cloud to simultaneously perform aggregation of gradient information. Furthermore, such overhead may cause communication failures in some clients, causing clients to fail to upload gradient information, thereby reducing the accuracy of the global model.
In this paper, we initially use FedAVG algorithm to alleviate the expensive communication overhead issue. FedAVG

reduces communication overhead by i) computing the average gradient of a batch size samples on the client and ii) computing the average aggregation gradient from all clients. Fig. 7 shows that FedAVG performs well when the number of clients is less than 8, but when the number of clients exceeds 8, the performance of FedAVG starts to decline. The reason is that, when the number of clients exceeds a certain threshold (e.g., C = 8), the probability of client failure will increase, which causes FedAVG to calculate wrong gradient information. Nevertheless, FedAVG is signiÔ¨Åcant for reducing communication overhead because the number of entities involved in predicting trafÔ¨Åc Ô¨Çow tasks in real life is usually small. Therefore, we need to propose a new communication protocol for largescale organizations to solve the problem of communication overhead.
D. TrafÔ¨Åc Flow Prediction With Large-scale FedGRU Model
In Section V-C, the experimental results show that FedAVG is no longer suitable for large-scale organizations when C ‚â• 8. However, in real life, we sometimes cannot avoid large-scale organization participation in FedGRU model. To solve this problem, we design the joint-announcement protocol, which

IEEE INTERNET OF THINGS JOURNAL

10

Value

C=2

120

=10% C=4

100

=20% C=8

80

=40% C=10

60

=50%

40

20

MAE

RMSE

MAPE(%)

MSE

Error

Fig. 9. The prediction results of models with different participation ratios (Œ≤ = 10%, 20%, 40%, 50%).

can randomly select a certain proportion of organizations from a large number of participating organizations to participate in the i-th round training. In this experiment, we set the participation ratio Œ≤ ‚àà {10%, 20%, 40%, 50%} and set C = 20. Then we compare these four cases with the ones of section V-C.
In this experiment, we Ô¨Årst focus on the communication overhead of a large-scale FedGRU model. In Fig. 8, we show that FedGRU with joint-announcement protocol can signiÔ¨Åcantly reduce the communication overhead. SpeciÔ¨Åcally, when C = 10 (Œ≤ = 50%), the communication overhead of FedGRU with the joint-announcement protocol is reduced by 64.10% compared to FedGRU with FedAVG algorithm. The Joint-announcement protocol Ô¨Årst performs sub-sampling on participating organizations, which can reduce the number of participants. Then it uses FedAVG algorithm to calculate the average gradient information, which guarantees the reliability of model training. Furthermore, experimental results show that such a protocol is robust to the number of participants, that is, the performance of the protocol is not affected by the number of participants.
Fig. 9 shows the prediction results of models with different participation ratios. It shows that when C = 10 (Œ≤ = 50%), the prediction results of large-scale FedGRU is the most different from FedGRU. When C = 10 (Œ≤ = 50%), MAE of large-scale FedGRU is 29.08% upper than MAE of FedGRU. This is because the performance of FedAVG starts to decrease when C ‚â• 8 (as shown in Fig. 9), and FedGRU with joint-announcement protocol can control the number of participants through sub-sampling to maintain the performance of FedAVG. In Fig. 10, we can Ô¨Ånd the loss of models with different Œ≤. It shows that the larger Œ≤ of the model, the greater the loss in the early training period. But Œ≤ does not affect the convergence of the models. Therefore, FedGRU using jointannouncement protocol can maintain good stability, robustness and efÔ¨Åciency.
E. TrafÔ¨Åc Flow Prediction With Ensemble Clustering-Based FedGRU Model
In this subsection, we evaluate an ensemble clusteringbased FedGRU in scenarios where a large number of clients jointly work on training a trafÔ¨Åc Ô¨Çow prediction problem. In particular, we examine the effect of K on the proposed

TABLE III PERFORMANCE COMPARISON OF FEDGRU ALGORITHM AND
CLUSTERING-BASED FEDGRU ALGORITHM

Metrics
FedGRU FedGRU (K = 2) FedGRU (K = 4) FedGRU (K = 8) FedGRU (K = 10)

MAE
7.96 7.89 7.42 7.17 6.85

MSE
101.49 100.98 99.98 99.16 97.77

RMSE
11.04 10.82 10.01 9.86 9.49

MAPE
17.82% 17.16% 16.85% 16.22% 14.69%

ensemble clustering-based mechanism. In Table III, we show the accuracy of Clustering-Based FedGRU model when the cluster centers K = 0, 2, 4, 8, 10. The results indicate that the proposed ensemble clustering-Based FedGRU model achieves the best prediction accuracy and can further improve the performance of the original FedGRU model. Compared with GRU model, the Clustering-Based FedGRU model can even outperform the centralized GRU model when K = 8, 10, which still compromises the data privacy. The reason is that the ensemble clustering-based scheme can improve the prediction accuracy by classifying similar spatio-temporal features into the same cluster and integrating the advantages of the optimal global model set. Furthermore, in such a scheme, it is easy for FedGRU to Ô¨Ånd the optimal global model subset because K is relatively small. Therefore, our proposed ensemble clusteringbased federated learning scheme can further improve the accuracy of prediction, thereby achieving accurate and timely trafÔ¨Åc Ô¨Çow prediction.
F. Discussion
In this subsection, we further discuss the advantages and limitations of FedGRU in different scenarios for predicting trafÔ¨Åc Ô¨Çow. In the previous subsections, we carry out a series of comprehensive case studies to show the effectiveness of our proposed method. Based on the above empirical results, the following observations can be derived:
i) Communication overhead is the bottleneck of FedGRU model. For a large-scale FedGRU model, the jointannouncement protocol helps solve the communication overhead problem. It mitigates the communication overhead of FedGRU model by reducing the number of participants participating in each round of communication.
ii) Global model updates for the client in FedGRU model are not synchronized. For example, FedGRU may fail to synchronize global model updates due to the failure of some clients. This may potentially make the local model of arbitrary clients deviate from the current global one, which affects the next global model update. To address this problem, we use random sub-sampling to select organizations that participate in the i-th round of training, as reducing the number of participants participating in the i-th training can reduce the probability of client failure, thereby alleviating the out-of-sync issue.
iii) Statistical heterogeneity issue is a problem that needs to be solved. Due to a large number of organizations involved in training, the large-scale FedGRU model faces

IEEE INTERNET OF THINGS JOURNAL

11

Loss

0.02 0.01
0 0.10 0.05 0.00 0

=10% 0.075

Loss

0.050

0.025

100 200 300 400 500 600 0.000 0

epoch

=40%

0.3

0.2

Loss

0.1

100 200 300 400 500 600 epoch

0.0 0

=20%

100 200 300 400 500 600

epoch

=50%

100 200 300 400 500 600 epoch

Loss

Fig. 10. Loss of FedGRU model with different participation ratios. (Œ≤ = 10%, 20%, 40%, 50%)

a challenge: the local data are not i.i.d. [52], [53]. Organizations often generate and collect data across the network in a completely different way [54]. This data generation paradigm violates the i.i.d. assumption commonly used in distributed optimization, increasing the likelihood of sprawl and possibly increasing the complexity of modeling, analysis, and evaluation [55].
G. Privacy Analysis
According to the deÔ¨Ånition of information-based privacy, we discuss the privacy protection capabilities of the proposed FedGRU model from the following aspects:
i) Data Access: The proposed model is developed based on the federated learning framework, and its core idea is a distributed privacy protection framework. SpeciÔ¨Åcally, the proposed model achieves accurate trafÔ¨Åc Ô¨Çow prediction by aggregating encrypted parameters rather than accessing the original data, which guarantees the model‚Äôs privacy protection for the data.
ii) Model Performance: Experimental results show that the performance of the proposed model is comparable to GRU model. GRU model is a centralized machine learning model, which needs to aggregate a large amount of raw data to achieve high-precision trafÔ¨Åc Ô¨Çow prediction. Furthermore, there is a trade-off between trafÔ¨Åc Ô¨Çow prediction and privacy. The proposed model achieves comparable results to a centralized machine learning approach under the constraint of privacy preservation, therefore demonstrates its superiority.
VI. CONCLUSION
In this paper, we propose a FedGRU algorithm for trafÔ¨Åc Ô¨Çow prediction with federated learning for privacy preservation. FedGRU does not directly access distributed organizational data but instead employs secure parameter aggregation mechanism to train a global model in a distributed manner. It aggregates the gradient information uploaded by all locally trained models in the cloud to construct the global one for trafÔ¨Åc Ô¨Çow forecast. We evaluate the performance

of FedGRU on a PeMS dataset and compared it with GRU, LSTM, SAE, and SVM, which all potentially compromise user privacy during the forecast. The results show that the proposed method performs comparably to the competing methods with minuscule accuracy degradation with privacy well-preserved. Furthermore, we apply an ensemble clustering-based FedGRU for TFP to further improve the model performance. We also demonstrate by empirical studies that the proposed jointannouncement protocol is efÔ¨Åcient in reducing the communication overhead for FedGRU by 64.10% compared with centralized models.
To the best of our knowledge, this is among the pioneering work for trafÔ¨Åc Ô¨Çow forecasts with federated deep learning. In the future, we plan to apply Graph Convolutional Network (GCN) [56] to the federated learning framework to better capture the spatial-temporal dependency among trafÔ¨Åc Ô¨Çow data to further improve the prediction accuracy.
REFERENCES
[1] Y. Lv, Y. Duan, W. Kang, Z. Li, and F.-Y. Wang, ‚ÄúTrafÔ¨Åc Ô¨Çow prediction with big data: a deep learning approach,‚Äù IEEE Transactions on Intelligent Transportation Systems, vol. 16, no. 2, pp. 865‚Äì873, 2014.
[2] N. Zhang, F.-Y. Wang, F. Zhu, D. Zhao, S. Tang et al., ‚ÄúDynacas: Computational experiments and decision support for ITS,‚Äù 2008.
[3] K. Ren, Q. Wang, C. Wang, Z. Qin, and X. Lin, ‚ÄúThe security of autonomous driving: Threats, defenses, and future directions,‚Äù Proceedings of the IEEE, vol. 108, no. 2, pp. 357‚Äì372, Feb 2020.
[4] X. Lin, X. Sun, P. Ho, and X. Shen, ‚ÄúGsis: A secure and privacypreserving protocol for vehicular communications,‚Äù IEEE Transactions on Vehicular Technology, vol. 56, no. 6, pp. 3442‚Äì3456, Nov 2007.
[5] Y. Zhang, C. Xu, H. Li, K. Yang, N. Cheng, and X. S. Shen, ‚ÄúProtect: EfÔ¨Åcient password-based threshold single-sign-on authentication for mobile users against perpetual leakage,‚Äù IEEE Transactions on Mobile Computing, pp. 1‚Äì1, 2020.
[6] R. N. Fries, M. R. Gahrooei, M. Chowdhury, and A. J. Conway, ‚ÄúMeeting privacy challenges while advancing intelligent transportation systems,‚Äù Transportation Research Part C: Emerging Technologies, vol. 25, pp. 34‚Äì45, 2012.
[7] C. Zhang, J. J. Q. Yu, and Y. Liu, ‚ÄúSpatial-temporal graph attention networks: A deep learning approach for trafÔ¨Åc forecasting,‚Äù IEEE Access, vol. 7, pp. 166 246‚Äì166 256, 2019.
[8] S. Madan and P. Goswami, ‚ÄúA novel technique for privacy preservation using k-anonymization and nature inspired optimization algorithms,‚Äù Available at SSRN 3357276, 2019.
[9] J. L. Ny, A. Touati, and G. J. Pappas, ‚ÄúReal-time privacy-preserving model-based estimation of trafÔ¨Åc Ô¨Çows,‚Äù in 2014 ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS), April 2014, pp. 92‚Äì102.

IEEE INTERNET OF THINGS JOURNAL

12

[10] J. KonecÀány`, H. B. McMahan, F. X. Yu, P. Richta¬¥rik, A. T. Suresh, and D. Bacon, ‚ÄúFederated learning: Strategies for improving communication efÔ¨Åciency,‚Äù arXiv preprint arXiv:1610.05492, 2016.
[11] X. Yuan, X. Wang, C. Wang, J. Weng, and K. Ren, ‚ÄúEnabling secure and fast indexing for privacy-assured healthcare monitoring via compressive sensing,‚Äù IEEE Transactions on Multimedia (TMM), vol. 18, no. 10, pp. 1‚Äì13, 2016.
[12] M. S. Ahmed, ‚ÄúAnalysis of freeway trafÔ¨Åc time series data and their application to incident detection,‚Äù Equine Veterinary Education, vol. 6, no. 1, pp. 32‚Äì35, 1979.
[13] M. Van Der Voort, M. Dougherty, and S. Watson, ‚ÄúCombining kohonen maps with arima time series models to forecast trafÔ¨Åc Ô¨Çow,‚Äù Transportation Research Part C: Emerging Technologies, vol. 4, no. 5, pp. 307‚Äì318, 1996.
[14] S. Lee and D. B. Fambro, ‚ÄúApplication of subset autoregressive integrated moving average model for short-term freeway trafÔ¨Åc volume forecasting,‚Äù Transportation Research Record, vol. 1678, no. 1, pp. 179‚Äì 188, 1999.
[15] B. M. Williams and L. A. Hoel, ‚ÄúModeling and forecasting vehicular trafÔ¨Åc Ô¨Çow as a seasonal arima process: Theoretical basis and empirical results,‚Äù Journal of transportation engineering, vol. 129, no. 6, pp. 664‚Äì 672, 2003.
[16] J. J. Q. Yu, A. Y. S. Lam, D. J. Hill, Y. Hou, and V. O. K. Li, ‚ÄúDelay aware power system synchrophasor recovery and prediction framework,‚Äù IEEE Transactions on Smart Grid, vol. 10, no. 4, pp. 3732‚Äì3742, July 2019.
[17] G. A. Davis and N. L. Nihan, ‚ÄúNonparametric regression and short-term freeway trafÔ¨Åc forecasting,‚Äù Journal of Transportation Engineering, vol. 117, no. 2, pp. 178‚Äì188, 1991.
[18] C.-C. Chang and C.-J. Lin, ‚ÄúLibsvm: A library for support vector machines,‚Äù ACM transactions on intelligent systems and technology (TIST), vol. 2, no. 3, p. 27, 2011.
[19] D. Svozil, V. Kvasnicka, and J. Pospichal, ‚ÄúIntroduction to multi-layer feed-forward neural networks,‚Äù Chemometrics and intelligent laboratory systems, vol. 39, no. 1, pp. 43‚Äì62, 1997.
[20] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, ‚ÄúLong short-term memory neural network for trafÔ¨Åc speed prediction using remote microwave sensor data,‚Äù Transportation Research Part C: Emerging Technologies, vol. 54, pp. 187‚Äì197, 2015.
[21] Y. Tian and L. Pan, ‚ÄúPredicting short-term trafÔ¨Åc Ô¨Çow by long shortterm memory recurrent neural network,‚Äù in 2015 IEEE international conference on smart city/SocialCom/SustainCom (SmartCity). IEEE, 2015, pp. 153‚Äì158.
[22] R. Fu, Z. Zhang, and L. Li, ‚ÄúUsing lstm and gru neural network methods for trafÔ¨Åc Ô¨Çow prediction,‚Äù in 2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC), Nov 2016, pp. 324‚Äì328.
[23] J. J. Q. Yu, W. Yu, and J. Gu, ‚ÄúOnline vehicle routing with neural combinatorial optimization and deep reinforcement learning,‚Äù IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, pp. 3806‚Äì3817, Oct 2019.
[24] J. J. Q. Yu and J. Gu, ‚ÄúReal-time trafÔ¨Åc speed estimation with graph convolutional generative autoencoder,‚Äù IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, pp. 3940‚Äì3951, Oct 2019.
[25] U. M. A¬®ƒ±vodji, S. Gambs, M.-J. Huguet, and M.-O. Killijian, ‚ÄúMeeting points in ridesharing: A privacy-preserving approach,‚Äù Transportation Research Part C: Emerging Technologies, vol. 72, pp. 239‚Äì253, 2016.
[26] B. Y. He and J. Y. Chow, ‚ÄúOptimal privacy control for transport network data sharing,‚Äù Transportation Research Part C: Emerging Technologies, 2019.
[27] Y. Zhou, Z. Mo, Q. Xiao, S. Chen, and Y. Yin, ‚ÄúPrivacy-preserving transportation trafÔ¨Åc measurement in intelligent cyber-physical road systems,‚Äù IEEE Transactions on Vehicular Technology, vol. 65, no. 5, pp. 3749‚Äì3759, 2015.
[28] B. Hoh, M. Gruteser, R. Herring, J. Ban, D. Work, J.-C. Herrera, A. M. Bayen, M. Annavaram, and Q. Jacobson, ‚ÄúVirtual trip lines for distributed privacy-preserving trafÔ¨Åc monitoring,‚Äù in Proceedings of the 6th international conference on Mobile systems, applications, and services, 2008, pp. 15‚Äì28.
[29] K. Xie, X. Ning, X. Wang, S. He, Z. Ning, X. Liu, J. Wen, and Z. Qin, ‚ÄúAn efÔ¨Åcient privacy-preserving compressive data gathering scheme in wsns,‚Äù Information Sciences, vol. 390, pp. 82‚Äì94, 2017.
[30] C. Dwork, G. N. Rothblum, and S. Vadhan, ‚ÄúBoosting and differential privacy,‚Äù in 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, Oct 2010, pp. 51‚Äì60.
[31] F. Lyu, N. Cheng, H. Zhu, H. Zhou, W. Xu, M. Li, and X. Shen, ‚ÄúTowards rear-end collision avoidance: Adaptive beaconing for connected

vehicles,‚Äù IEEE Transactions on Intelligent Transportation Systems, pp. 1‚Äì16, 2020. [32] Q. Yang, Y. Liu, T. Chen, and Y. Tong, ‚ÄúFederated machine learning: Concept and applications,‚Äù ACM Transactions on Intelligent Systems and Technology (TIST), vol. 10, no. 2, p. 12, 2019. [33] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth, ‚ÄúPractical secure aggregation for federated learning on user-held data,‚Äù arXiv preprint arXiv:1611.04482, 2016. [34] G. Xu, H. Li, S. Liu, K. Yang, and X. Lin, ‚ÄúVerifynet: Secure and veriÔ¨Åable federated learning,‚Äù IEEE Transactions on Information Forensics and Security, vol. 15, pp. 911‚Äì926, 2019. [35] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, ‚ÄúIncentive mechanism for reliable federated learning: A joint optimization approach to combining reputation and contract theory,‚Äù IEEE Internet of Things Journal, vol. 6, no. 6, pp. 10 700‚Äì10 714, 2019. [36] J. Ni, X. Lin, and X. S. Shen, ‚ÄúToward edge-assisted internet of things: from security and efÔ¨Åciency perspectives,‚Äù IEEE Network, vol. 33, no. 2, pp. 50‚Äì57, 2019. [37] T. Nishio and R. Yonetani, ‚ÄúClient selection for federated learning with heterogeneous resources in mobile edge,‚Äù in ICC 2019-2019 IEEE International Conference on Communications (ICC). IEEE, 2019, pp. 1‚Äì7. [38] Y. Chen, J. Wang, C. Yu, W. Gao, and X. Qin, ‚ÄúFedhealth: A federated transfer learning framework for wearable healthcare,‚Äù arXiv preprint arXiv:1907.09173, 2019. [39] S. J. Pan and Q. Yang, ‚ÄúA survey on transfer learning,‚Äù IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345‚Äì 1359, 2009. [40] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, ‚ÄúFederated learning: Challenges, methods, and future directions,‚Äù arXiv preprint arXiv:1908.07873, 2019. [41] T. Li, M. Sanjabi, and V. Smith, ‚ÄúFair resource allocation in federated learning,‚Äù 2019. [42] Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato, ‚ÄúMobile edge computing, blockchain and reputation-based crowdsourcing iot federated learning: A secure, decentralized and privacy-preserving system,‚Äù arXiv preprint arXiv:1906.10893, 2019. [43] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, ‚ÄúFederated learning with non-iid data,‚Äù arXiv preprint arXiv:1806.00582, 2018. [44] J. Kang, Z. Xiong, D. Niyato, D. Ye, D. I. Kim, and J. Zhao, ‚ÄúToward secure blockchain-enabled internet of vehicles: Optimizing consensus management using reputation and contract theory,‚Äù IEEE Transactions on Vehicular Technology, vol. 68, no. 3, pp. 2906‚Äì2920, 2019. [45] K. Cho, B. Van Merrie¬®nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, ‚ÄúLearning phrase representations using rnn encoder-decoder for statistical machine translation,‚Äù arXiv preprint arXiv:1406.1078, 2014. [46] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konen, S. Mazzocchi, H. B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, ‚ÄúTowards federated learning at scale: System design,‚Äù 2019. [47] Wagstaff, Cardie, Claire, Rogers, Seth, and Stefan, ‚ÄúConstrained kmeans clustering with background knowledge,‚Äù ICML-2001, 2001. [48] C. Chao, Freeway performance measurement system (pems), 2003. [49] T. Ryffel, A. Trask, M. Dahl, B. Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach, ‚ÄúA generic framework for privacy preserving deep learning,‚Äù arXiv preprint arXiv:1811.04017, 2018. [50] A. Paszke, S. Gross, S. Chintala, and G. Chanan, ‚ÄúPytorch,‚Äù Computer software. Vers. 0.3, vol. 1, 2017. [51] M. A. Mohandes, T. O. Halawani, S. Rehman, and A. A. Hussain, ‚ÄúSupport vector machines for wind speed prediction,‚Äù Renewable Energy, vol. 29, no. 6, pp. 939‚Äì947, 2004. [52] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, ‚ÄúFederated learning with Non-IID data,‚Äù 2018. [53] J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani, ‚ÄúReliable federated learning for mobile networks,‚Äù arXiv preprint arXiv:1910.06837, 2019. [54] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, ‚ÄúFederated learning: Challenges, methods, and future directions,‚Äù 2019. [55] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, ‚ÄúFederated optimization in heterogeneous networks,‚Äù 2018. [56] T. N. Kipf and M. Welling, ‚ÄúSemi-supervised classiÔ¨Åcation with graph convolutional networks,‚Äù arXiv preprint arXiv:1609.02907, 2016.

