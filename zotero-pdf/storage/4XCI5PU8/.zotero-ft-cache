arXiv:1907.09769v1 [cs.IT] 23 Jul 2019

1
Federated Learning over Wireless
Fading Channels
Mohammad Mohammadi Amiri and Deniz Gündüz
Abstract
We study federated machine learning at the wireless network edge, where limited power wireless devices, each with its own dataset, build a joint model with the help of a remote parameter server (PS). We consider a bandwidth-limited fading multiple access channel (MAC) from the wireless devices to the PS, and implement distributed stochastic gradient descent (DSGD) over-the-air. We ﬁrst propose a digital DSGD (D-DSGD) scheme, in which one device is selected opportunistically for transmission at each iteration based on the channel conditions; the scheduled device quantizes its gradient estimate to a ﬁnite number of bits imposed by the channel condition, and transmits these bits to the PS in a reliable manner. Next, motivated by the additive nature of the wireless MAC, we propose a novel analog communication scheme, referred to as the compressed analog DSGD (CA-DSGD), where the devices ﬁrst sparsify their gradient estimates while accumulating error from previous iterations, and project the resultant sparse vector into a low-dimensional vector. We also design a power allocation scheme to align the received gradient vectors at the PS in an efﬁcient manner. Numerical results show that the proposed CA-DSGD algorithm converges much faster than the D-DSGD scheme and other schemes in the literature, while providing a signiﬁcantly higher accuracy.
I. INTRODUCTION As the dataset sizes and model complexity grow, distributed machine learning (ML) is becoming the only viable alternative to centralized ML, where all the dataset is gathered at a centralized server, and a joint model is trained. With the increasing amount of information collected through wireless edge devices, such centralized solutions are becoming increasingly costly, due to the limited power and bandwidth available, and less desirable due to privacy concerns. Federated
The authors are with the Information Processing and Communications Laboratory (IPC-Lab), Department of Electrical and Electronic Engineering, Imperial College London. (e-mail: m.mohammadi-amiri15@imperial.ac.uk; d.gunduz@imperial.ac.uk).
This work was supported in part by the European Research Council (ERC) Starting Grant BEACON (grant agreement no. 725731).

2

learning (FL) has been proposed as an alternative privacy-preserving distributed ML scheme, where each device participates in training using only locally available data with the help of a parameter server (PS) [1]. Devices exchange model parameters and their local updates with the PS, but the data never leaves the devices.
ML problems often involve the minimization of the empirical loss function

F

(θ)

=

1 |B|

f (θ, u)

(1)

u∈B

where θ ∈ Rd denotes the model parameters to be optimized, B is the training dataset with

size |B| consisting of data samples and their labels, and f (·) is the loss function deﬁned by

the learning task. The minimization of F (θ) is typically carried out through iterative stochastic

gradient descent (SGD) algorithm, in which the model parameter vector at iteration t, θt, is updated with a stochastic gradient

θt+1 = θt − ηtg (θt) ,

(2)

which satisﬁes E [g (θt)] = ∇F (θt), where ηt is the learning rate. SGD can easily be implemented across multiple devices, each of which has access to only a small fraction of the dataset. In distributed SGD (DSGD), at each iteration, device m computes a gradient vector based on the global parameter vector with respect to its local dataset, denoted by Bm, and sends the result to the PS, which updates the global parameter vector according to

θt+1

=

θt

−

ηt

1 M

M
m=1 gm (θt) ,

(3)

where M denotes the number of wireless devices, and gm (θt)

1 |Bm |

u∈Bm ∇f (θt, u), m ∈

[M]. In FL, each device participating in the training can also carry out model updates as in (3)

locally, and share the overall difference with respect to the previous model parameters with the

PS [1].

What distinguishes FL from conventional ML is the large number of devices that participate

in the training, and the low-capacity and unreliable links that connect these devices to the PS.

Therefore, there have been signiﬁcant research efforts to reduce the communication requirements

in FL [1]–[24]. However, these and follow-up studies consider orthogonal channels from the

participating devices to the PS, and ignore the physical layer aspects of wireless connections,

even though FL has been mainly motivated for mobile devices.

3
In this paper, we consider DSGD over-the-air; that is, we consider a wireless shared medium from the devices to the PS as it has been considered independently in the parallel works, [25]– [27]. To emphasize the limitations of wireless channels, we note that the dimension of some of the recent ML models, which also determines the size of the gradient estimates that must be transmitted to the PS at each iteration, can be extremely large, e.g., the 50-layer ResNet network has ∼ 26 million weight parameters, while the VGGNet architecture has approximately 138 million parameters. On the other hand, available channel bandwidth is typically small due to the bandwidth and latency limitations; for example 1 LTE frame of 5MHz bandwidth and duration 10ms can carry only 6000 complex symbols. In principle, we can treat each iteration of the DSGD algorithm as a distributed over-the-air computation problem. FL over a statis Gaussian MAC is studied in [25], where both a digital scheme, which separates computation and communication, and an analog over-the-air computation scheme are introduced. While the digital scheme exploits gradient quantization followed by independent channel coding at the participating wireless devices, the analog scheme exploits the additive nature of the wireless channel and gradient sparsiﬁcation and random linear projection for dimensionality reduction. In [26] the authors consider a fading MAC, and also apply analog transmission, where each entry of a gradient vector at each of the devices is scheduled for transmission depending on the corresponding channel condition. A multi-antenna PS is considered in [27], where receive beamforming is used to maximize the number of devices scheduled for transmission at each iteration.
Here, we extend our previous work in [25], and study DSGD over a wireless fading MAC. We ﬁrst consider the separate computation and communication approach, and propose a digital DSGD (D-DSGD) scheme, in which only a single device is opportunistically scheduled for transmission at each iteration of DSGD. The scheduled device quatizes its gradient estimate to a ﬁnite number of bits while accumulating the error from previous iterations (this will be clariﬁed later), and employs a capacity achieving channel code to transmit the bits over the available bandwidth-limited channel to the PS. We then study analog transmission from the devices to the PS motivated by the signal-superposition property of the wireless MAC. At ﬁrst, we extend the scheme in [26] by introducing error accumulation, which is shown to improve the performance numerically. We then propose a novel scheme, inspired by the random projection used in [25] for dimensionality reduction, which we will refer to as the compressed analog DSGD (CA-DSGD). With CA-DSGD, each device projects its gradient estimate to a low-dimensional

4
vector and transmits only the important gradient entries while accumulating the error. CA-DSGD scheme provides the ﬂexibility of adjusting the dimension of the gradient estimate sent by each device, which is particularly important for bandwidth-limited wireless channels. A power allocation scheme is also designed, which aligns the vectors sent by different devices at the PS while satisfying the average power constraint. We implement these schemes for the MNIST classiﬁcation task, where the training dataset is distributed randomly across wireless devices. Numerical results show that the proposed CA-DSGD scheme substantially improves both the training and test accuracies compared to the D-DSGD as well as the analog DSGD schemes under consideration with the same average power constraint and bandwidth resources. In addition to the signiﬁcant beneﬁts of the proposed CA-DSGD scheme we make the following observations:
1) The improvement of analog over-the-air computation compared to the D-DSGD scheme is particularly striking in the low power regime. This is mainly due to the “beamforming” effect of simultaneously transmitting highly correlated gradient estimates.
2) While the accuracy of the D-DSGD scheme increases drastically with the available average power, the performance of the analog schemes improve marginally. This highlights the energy efﬁciency of over-the-air computation, and makes it particularly attractive for FL across low-power IoT sensors.
3) Increasing the number of devices improves the accuracy for all the schemes even if the total dataset size and total power consumption remain the same. This “diversity gain” is much more limited for the analog schemes, and diminishes further as the training duration increases.
4) We observe that the performance of the CA-DSGD scheme improves if we reduce the bandwidth used at each iteration, and increase the number of DSGD iterations instead.
Notations: R and C represent the sets of real and complex values, respectively. For two vectors x and y with the same dimension, x·y returns their inner product. For a vector z ∈ Ci, Re{z} ∈ Ri and Im{z} ∈ Ri return the entry-wise real and imaginary components of z, respectively. Also, [v, w] represents the concatenation of two row vectors v and w. We denote a zero-mean normal distribution with variance σ2 by N (0, σ2), and CN (0, σ2) represents a complex normal distribution with independent real and imaginary terms each distributed according to N (0, σ2/2). For positive integer i, we let [i] {1, . . . , i}. We denote the cardinality of set A by |A|, and l2
√ norm of vector x by x 2. The imaginary unit is represented by −1.

5

Fig. 1: Illustration of wireless FL architecture. The PS sends the updated parameter vector to all the wireless devices over an error-free ideal multi-cast channel, while the gradient estimates, computed by each device using only the available local dataset, are transmitted to the PS over the fading uplink channel.

II. SYSTEM MODEL
We consider FL across M wireless devices, each with its own local dataset, which employ DSGD with the help of a remote PS. We model the channel from the devices to the PS as a wireless fading MAC, and OFDM is employed for transmission. The system model is illustrated in Fig. 1. The parameter vector at iteration t is denoted by θt, and we assume that it is delivered from the PS to the devices over an error-free shared link. We denote the set of data samples available at device m by Bm, and the stochastic gradient computed by device m with respect to local data samples by gm (θt) ∈ Rd, m ∈ [M]. At the t-th iteration of the DSGD algorithm in (3), the local gradient estimates of the devices are sent to the PS over a wireless fading MAC using s subchannels for a total of N time slots, where s ≤ d (in practice, we typically have s ≪ d). We denote the length-s channel input vector transmitted by device m at the n-th time slot of the t-th iteration of the DSGD by xnm(t) = [xnm,1(t) · · · xnm,s(t)]T ∈ Cs. The i-th entry of channel output yn(t) ∈ Cs received by the PS at the n-th time slot of the t-th iteration, n ∈ [N ], is given by

yin(t) =

m∈Mni (t) hnm,i(t)xnm,i(t) + zin(t), i ∈ [s],

(4)

where Mni (t) ⊂ [M ], hnm,i(t) ∈ C is the i-th entry of the vector of channel gains hnm(t) from device m to the PS, and is assumed to be independent and identically distributed (i.i.d.) according

6

to CN (0, σm2 ), e.g., Rayleigh fading, and zin(t) ∈ C is the i-th entry of complex Gaussian noise vector zn(t) and is i.i.d. according to CN (0, 1). The channel input vector of device m at the n-th time slot of iteration t, n ∈ [N ], is a function of the channel gains hnm(t), current parameter vector θt, the local dataset Bm, and the current gradient estimate at device m, gm (θt), m ∈ [M]. We assume that, at each time slot, the channel state information (CSI) is known by the devices and the PS. For a total of T iterations of the DSGD algorithm, the following total average transmit power constraint is imposed at device m:

1 NT

T t=1

N
E
n=1

||xnm(t)||22

≤ P¯,

∀m ∈ [M],

(5)

where the expectation is taken over the randomness of the channel gains.

The

goal

is

to

recover

1 M

M m=1

gm

(θt)

at

the

PS,

which

then

updates

the

model

parameter

as in (3) after N time slots. However, due to the pre-processing performed at each device and

the distortion caused by the wireless channel, the PS uses a noisy estimate to update the model parameter. Having deﬁned y(t) [y1(t)T · · · yN (t)T ]T , we have θt+1 = φ(θt, y(t)) for some update function φ : Rd×CNs → Rd. The updated model parameter is then multicast to the devices

by the PS through an error-free shared link, so the devices receive a consistent parameter vector

for their computations in the next iteration.

We note that the goal of the devices is to transmit their local gradient estimates to the PS

over the wireless channel, which is a joint source-channel coding problem. Moreover, the PS is

not interested in the individual estimates of the devices, but in their average; hence, we have

a distributed lossy computation problem. We will consider both a digital approach separating

computations and communication and an analog transmission approach, where the gradients are

transmitted simultaneously over the wireless MAC in an uncoded fashion.

III. DIGITAL DSGD
We ﬁrst consider DSGD with digital transmission of the gradient estimates by the devices over the wireless fading MAC, referred to as the digital DSGD (D-DSGS) scheme. For D-DSGD, we consider N = 1, i.e., the parameter vector is updated after each time slot, and drop the dependency on time slot parameter n.
The goal here is to schedule devices and employ power allocation across time slots such that devices can transmit to the PS their local gradient estimates as accurately as possible. A possible approach is to schedule all the devices at all the iterations; however, due to the interference among

7

the devices this will result in each device sending a very coarse description of its local gradient

estimate, and it is observed in [25] that the performance degrades with the increasing number of

devices. Instead here we will schedule the devices opportunistically according to their channel

states.

In particular, with the knowledge of channel state information (CSI), at each iteration t, we

select the device which has the largest value of

s i=1

|hm,i|2,

m

∈

[M ].

Accordingly,

the

index

of the transmitting device at iteration t is given by:

m∗(t) = arg max

s |hm,i|2 .

(6)

m∈[M ]

i=1

At each iteration, we denote the probability of selecting device m by πm, which is time invariant

due to the statistics of the channel gains, and

M m=1

πm

=

1.

The

power

allocated

to

device

m

at the t-th iteration is given by Pm(t), which, for an average transmit power P¯, should satisfy

πm T

T t=1

Pm(t)

≤

P¯,

for m ∈ [M].

(7)

Next, we derive the capacity of the communication channel from device m∗(t) to the PS at the t-th iteration of D-DSGD, which provides an upper bound on the number of bits device m∗(t) can transmit. The i-th entry of the channel output at the t-th iteration, which is the result of transmission from device m∗(t), is given by

yi(t) = hm∗(t),i (t) xm∗(t),i(t) + zi(t), i ∈ [s],

(8)

which is equivalent to a wireless fast fading channel with a limited number of channel uses s where the CSI is known at the transmitter and the receiver. In the following, we provide an upper bound on the capacity of the communication channel given in (8) with ﬁnite channel uses s. We consider a scenario, referred to as the inﬁnite ﬁxed channel scenario, where the channel realization hm∗(t),i, ∀i ∈ [s], is repeated over inﬁnitely many time iterations. The capacity per iteration of this communication channel provides an upper bound on that of given in (8). For a transmit power Pm∗(t)(t), the capacity per iteration of the wireless channel with the inﬁnite ﬁxed channel scenario is the result of the following optimization problem [28, Section 5.4.6]:

max
P1,...,Ps

s
i=1 log2

1 + Pi hm∗(t),i (t) 2

,

subject to

s i=1

Pi

=

Pm∗(t)(t).

(9)

8

Optimization problem (9) is solved through waterﬁlling, and the optimal power allocation is given by

Pi∗ = max

1 ζ

−

1 hm∗(t),i (t) 2 , 0

,

(10)

where ζ is determined such that

s i=1

Pi∗

=

Pm∗(t)(t).

Having

calculated

P1∗,

.

.

.

,

Ps∗,

the

capacity

of the wireless channel in (8) is given by

R(t) =

s
i=1 log2

1 + Pi∗ hm∗(t),i (t) 2

,

(11)

which provides an upper bound on the capacity of the communication channel between device m∗(t) and the PS.

Remark 1. An alternative device selection criteria rather than the one given in (6) is to select the device resulting in the highest channel capacity. Here we do not employ this selection criteria due to the overhead introduced by the computational complexity at the PS.

We adopt the D-DSGD scheme proposed in [25, Section III], which is an extension of the one

proposed in [18], for digital transmission. With the D-DSGD scheme, gradient estimate gm (θt), computed at device m, is added to the error accumulated from previous iterations, denoted

by ∆m(t − 1), where we set ∆m(0) = 0, m ∈ [M]. The error compensated gradient vector gm (θt) + ∆m(t − 1) is sparsiﬁed by setting d − q(t) entries to zero (refer to [25, Section III] for the details of the sparﬁcation method). Let gˆm (θt) denote the sparsiﬁed vector at device m, m ∈ [M]. After sparsifying the error compensated gradient estimate at device m and computing

gˆm (θt), m ∈ [M], the error accumulation vector is updated as follows:



∆m(t) = gm (θt) + ∆m(t − 1) − gˆm (θt) , if m = m∗(t),

(12)

gm (θt) ,

otherwise.

For a sparsity level q(t), the D-DSGD scheme requires transmission of a total number of bits [25, Equation (10)]

r(t) = log2

d q(t)

+ 33.

(13)

We assume that device m∗(t) employs a capacity achieving channel code, and we set the sparsity level q(t) as the highest value satisfying r(t) ≤ R(t).

9

We will evaluate the performance of the D-DSGD scheme in Section V, and study in detail the impact of various system parameters, such as the average power constraint, number of devices, etc., on the performance. We will also compare the D-DSGD scheme with the analog transmission of local gradients, which we present next.
IV. ANALOG DSGD Analog DSGD is motivated by the fact that the PS is only interested in the average of the gradient vectors, and the underlying wireless MAC can provide the sum of the gradients if they are sent without any coding. We ﬁrst present a generalization of the over-the-air computation approach introduced in [26], referred to as the entry-wise scheduled analog DSGD (ESA-DSGD) scheme, and then extend the ESA-DSGD scheme by introducing error accumulation, and refer to the scheme as error compensated ESA-DSGD (ECESA-DSGD). Finally, we propose our scheme, built upon our previous work [25], referred to as the compressed analog DSGD (CA-DSGD) scheme.

A. ESA-DSGD
With the ESA-DSGD scheme studied in [26], each device sends its gradient estimate entirely after applying power allocation, which is to satisfy the average power constraint. At the t-th iteration of the DSGD, device m, m ∈ [M], transmits its local gradient estimate gm (θt) ∈ Rd over N = ⌈d/2s⌉ time slots by utilizing both the real and imaginary components of the available s subchannels. We deﬁne, for n ∈ [N], m ∈ [M],

gnm,re (θt) gnm,im (θt)
gnm (θt)

[gm,2(n−1)s+1 (θt) , · · · , gm,(2n−1)s (θt)]T ,

[gm,(2n−1)s+1 (θt) , · · · , gm,2ns (θt)]T ,

√

g

n m,re

(θt)

+

−1g

n m,im

(θt)

,

(14a) (14b) (14c)

where gm,i (θt) is the i-th entry of gm (θt), and we zero-pad gm (θt) to have dimension 2sN . We note that, according to (14),

gm

(θt)

=

[g

1 m,re

(θt)

,

g

1 m,im

(θt)

,

·

·

·

,

gNm,re

(θt)

,

gNm,im

(θt)]T ,

(15)

where N = ⌈d/2s⌉. At the n-th time slot of the t-th iteration, device m, m ∈ [M], sends

xnm (t)

=

αem,n(t) ·

g

n m

(θt

),

where

αem,n(t)

∈

Cs

is

the

power

allocation

vector,

which

is

set

to

10

satisfy the average transmit power constraint. Thus, after N time slots, each device sends its

gradient estimate of dimension d entirely. The i-th entry of the power allocation vector αem,n(t) is set as follows:



αme,n,i(t)

=

 

γe(t) hnm,i (t)

,

if hnm,i(t) 2 ≥ λem,n(t),

(16)

0,

otherwise,

for some γe(t), λem,n(t) ∈ R, where λem,n(t) is chosen to satisfy the average power constraint. According to (16), each entry of a gradient vector is transmitted if its corresponding channel gain is over a threshold. The set of devices selected to transmit the i-th entry of the channel input vector at the n-th time slot is given by, i ∈ [s], n ∈ [N],

Mni (t) = m ∈ [M ] : hnm,i(t) 2 ≥ λem,n(t) .

(17)

By substituting xnm (θt) and αem,n(t) into (4), it follows that, for i ∈ [s], n ∈ [N ],

yin(t) = γe(t)

√ m∈Mni (t) gm,2(n−1)s+i(θt) + −1gm,(2n−1)s+i(θt) + zin(t).

(18)

The

PS

has

perfect

CSI,

and

hence,

knows

set

Mni (t).

Its

goal

is

to

recover

1
|Mni (t)|

Mni (t) m=1

gm,2(n−1)s+i

(θt)

and

1
|Mni (t)|

Mni (t) m=1

gm,(2n−1)s+i

(θt),

which

provide

estimates

for

1 M

M m=1

gm,2(n−1)s+i

(θt)

and

1 M

M m=1

gm,(2n−1)s+i

(θt),

respectively.

The

PS

estimates

1
|Mni (t)|

Mni (t) m=1

gm,2(n−1)s+i

(θt),

for

i ∈ [s], n ∈ [N ], using its noisy observation yin(t), given in (18), as



gˆ2e(n−1)s+i

(θt)

=

 

Re{yin (t)}
γe | (t) Mni (t)|

,

if |Mni (t)| = 0,

(19)

0,

otherwise,

and

estimates

1
|Mni (t)|

Mni (t) m=1

gm,(2n−1)s+i

(θt)

through



gˆ(e2n−1)s+i

(θt)

=

 

Im{yin (t)}
γe | (t) Mni (t)|

,

0,

if |Mni (t)| = 0, otherwise.

(20)

Estimated vector gˆe (θt) θt+1 = θt − ηtgˆe (θt).

[gˆ1e (θt) · · · gˆde (θt)]T is then used to update the parameter vector as

Remark 2. We remark here that the scheme in [26] imposes a stricter average power constraint

11

P¯ per iteration of the DSGD, i.e., at device m we should have

1 N

N
E
n=1

||xnm(t)||22

≤ P¯,

∀m ∈ [M], ∀t.

(21)

For fairness in our comparisons we relax this power constraint, and impose the one in (5), which constrains the average power over all the iterations.

B. ECESA-DSGD

With the ESA-DSGD scheme, entries of the gradient vectors that are not sent due to the

limited average transmit power are completely forgotten. The proposed ECESA-DSGD scheme

modiﬁes ESA-DSGD by incorporating error accumulation technique to retain the accuracy of

local gradients.

We denote the error accumulation vector calculated by device m at the n-th time slot of the

t-iteration by ∆vm,n(t) ∈ Cs, and set ∆vm,n(t) = 0, ∀n, t, m. Similarly to the ESA-DSGD scheme, with ECESA-DSGD, each device sends its entire gradient estimate of dimension d through

N = ⌈d/2s⌉ time slots, where the gradient estimates at the devices are zero-padded to dimension

2sN . After computing gm (θt) and obtaining gnm (θt) according to (14), device m, m ∈ [M],

updates

its

gradient

estimate

with

the

accumulated

error

as

g

v,n m

(θt)

gnm (θt) + ∆vm,n(t − 1),

for n ∈ [N ], and transmits vector xnm (t) = αvm,n(t) · gvm,n (θt), where αvm,n(t) ∈ Cs is the power

allocation vector, whose i-th entry is determined as follows:



αmv,n,i(t)

=

 

γ v (t) hnm,i(t)

,

if hnm,i(t) 2 ≥ λvm,n(t),

(22)

0,

otherwise,

for some γv(t), λvm,n(t) ∈ R. device m, m ∈ [M ], then updates the i-th entry of vector ∆vm,n(t) as follows:

∆vm,n,i(t) = 1 − 1 αmv,n,i(t) = 0 = 1 − 1 αmv,n,i(t) = 0

gmn ,i (θt) √
gm,2(n−1)s+i (θt) + −1gm,(2n−1)s+i (θt) ,

(23)

where 1(·) is the indicator function, and gmn ,i (θt) denotes the i-th entry of gnm (θt), for i ∈ [s],

12

n

∈

[N ].

Thus,

the

i-th

entry

of

vector

g

v,n m

(θt)

is

given

by,

for

i

∈

[s],

n

∈

[N ],

gmv,,ni

(θt)

=gm,2(n−1)s+i

(θt)

+

√ −1gm,(2n−1)s+i

(θt)

+ 1 − 1 αmv,n,i(t − 1) = 0

√ gm,2(n−1)s+i (θt−1) + −1gm,(2n−1)s+i (θt−1) . (24)

According to (23), each entry of the gradient vector gvm,n (θt) that is not transmitted due to the power allocation given in (22), is retained in the error accumulation vector ∆vm,n(t) for possible transmission in the next iteration. From the power allocation in (22), it follows that, for i ∈ [s], n ∈ [N],

yin(t) = γv(t)

m∈Mni (t) gmv,,ni(θt) + zin(t),

(25)

where we have

Mni (t) = m ∈ [M ] : hnm,i(t) 2 ≥ λvm,n(t) .

(26)

Having

perfect

CSI,

the

PS’s

goal

is

to

recover

1
|Mni (t)|

Mni (t) m=1

gmv,n,i

(θt),

the

real

and

imaginary

terms

of

which

provide

estimates

for

1 M

M m=1

gmn ,2(n−1)s+i

(θt)

and

1 M

M m=1

gmn ,(2n−1)s+i

(θt),

respectively,

for

i

∈

[s],

n

∈

[N ].

The

PS

estimates

1 M

M m=1

gmn ,2(n−1)s+i

(θt)

as



gˆ2v(n−1)s+i

(θt)

=

 

γ

Re{yin (t)}
v(t)|Mni (t)|

,

gˆ2v(n−1)s+i (θt−1) ,

if |Mni (t)| = 0, otherwise,

(27)

and

estimates

1 M

M m=1

gmn ,(2n−1)s+i

(θt)

through



gˆ(v2n−1)s+i

(θt)

=

 

γ

Im{yin (t)}
v(t)|Mni (t)|

,

gˆ(v2n−1)s+i (θt−1) ,

if |Mni (t)| = 0, otherwise,

(28)

for i ∈ [s], n ∈ [N ]. After N = ⌈d/2s⌉ time slots, estimated vector gˆv (θt) [gˆ1v, · · · , gˆdv]T is then used to update the parameter vector as θt+1 = θt − ηtgˆv (θt).

C. CA-DSGD
As opposed to ESA-DSGD and ECESA-DSGD, which aim to transmit all the gradient entries to the PS at each DSGD iteration, i.e., N = ⌈d/2s⌉, the CA-DSGD scheme proposed here applies gradient sparsiﬁcation with error accumulation followed by a linear projection of the gradients

13

Algorithm 1 CA-DSGD

1: Initialize θ1 = 0 and ∆c1(0) = · · · = ∆cM (0) = 0 2: for t = 1, . . . , T do

• devices do:

3: for m = 1, . . . , M in parallel do

4:

Compute gm (θt) with respect to local dataset Bm

5:

g

ec m

(θt)

=

gm

(θt)

+

∆cm(t

−

1)

6:

g

sp m

(θt)

=

sparsek

(gemc

(θt))

7:

∆cm(t) = gemc (θt) − gsmp (θt)

8:

g˜m (θt) = Agsmp (θt)

9:

for n = 1, . . . , N do

10:

xnm (t) = αcm,n(t) · g˜nm (θt)

11:

end for

12: end for

• PS does:

13: if yˆ(t) = 0 then

14:

gˆc (θt) = AMPA (yˆ(t))

15:

θt+1 = θt − ηtgˆc (θt)

16: else

17:

θt+1 = θt

18: end if

19: end for

at each device. With the CA-DSGD scheme, each device reduces the dimension of its gradient

estimate to length s˜ = 2sN, which can then be transmitted through N time slots, for an arbitrary

N ∈ [⌈d/2s⌉]. Thus, the CA-DSGD scheme can have any N value, where N ∈ [⌈d/2s⌉], and it

generalizes the ESA-DSGD and ECESA-DSGD schemes. Algorithm 1 presents the CA-DSGD

scheme, which we explain below.

We describe the CA-DSGD scheme for an arbitrary number of time slots N ∈ [⌈d/2s⌉]

per iteration of DSGD, which is determined later. At each iteration of the DSGD the devices

sparsify their gradient estimates as described below. In order to retain the accuracy of their

local gradient estimates, devices employ error accumulation [7], where the accumulated error

vector at device m until iteration t is denoted by ∆cm(t − 1) ∈ Rd, where we set ∆cm(0) = 0,

∀m ∈ [M]. After computing gm (θt), device m updates its estimate with the accumulated error as

g

ec m

(θt)

gm (θt) + ∆cm(t − 1), m ∈ [M], where k ≤ s˜ is a design parameter. Next, the devices

apply gradient sparsiﬁcation: device m sets all but k elements with the highest magnitudes of

vector gemc (θt) to zero, and obtains a sparse vector gsmp (θt), m ∈ [M ]. This k-level sparsiﬁcation

is

represented

by

function

sparsek

in

Algorithm

1,

i.e.,

g

sp m

(θt)

=

sparsek (gemc (θt)).

device

m,

14
m ∈ [M ], then updates ∆cm(t) as ∆cm(t) = gemc (θt) − gsmp (θt). To transmit the sparse vectors over the limited-bandwidth channel, devices employ a random projection matrix, similarly to compressive sensing.
Assuming local data points with identical distribution across the devices, the local gradient estimates also follow an identical distribution (in particular, for sufﬁciently large |Bm|, where the local gradient estimates are expected to be close to the true gradient); and hence, they are expected to have similar sparsity patterns. A pseudo-random matrix A ∈ Rs˜×d, with each entry i.i.d. according to N (0, 1/s˜), is generated and shared between the PS and the devices, where s˜ = 2sN, for an arbitrary N ∈ [⌈d/2s⌉]. At each iteration t, device m computes g˜m (θt) Agsmp (θt) ∈ Rs˜, and aims to transmit it to the PS through N = s˜/2s time slots. We deﬁne, for n ∈ [N], m ∈ [M],

g˜nm,re (θt) [g˜m,2(n−1)s+1 (θt) , · · · , g˜m,(2n−1)s (θt)]T ,
g˜nm,im (θt) [g˜m,(2n−1)s+1 (θt) , · · · , g˜m,2ns (θt)]T , √
g˜nm (θt) g˜nm,re (θt) + −1g˜nm,im (θt) ,

(29a) (29b) (29c)

where g˜m,i (θt) is the i-th entry of g˜m (θt), i ∈ [s˜]. At the n-th time slot of the t-th iteration of the DSGD, device m, m ∈ [M ], sends xnm (t) = αcm,n(t) · g˜nm (θt), where αcm,n(t) ∈ Cs is the power allocation vector. The i-th entry of the power allocation vector αcm,n(t) is set as follows:



αmc,n,i(t)

=

 

γc(t) hnm,i (t)

,

if hnm,i(t) 2 ≥ λcm,n(t),

(30)

0,

otherwise,

for some γc(t), λcm,n(t) ∈ R. The set of devices scheduled to transmit the i-th entry of the channel input vector at the n-th time slot is given by, i ∈ [s], n ∈ [N],

Mni (t) = m ∈ [M ] : hnm,i(t) 2 ≥ λcm,n(t) .

(31)

By substituting xnm (θt) and αcm,n(t) into (4), it follows that, for i ∈ [s], n ∈ [N ]

yin(t) = γc(t) = γc(t)

√ m∈Mni (t) g˜m,2(n−1)s+i(θt) + −1g˜m,(2n−1)s+i(θt) + zin(t)

aT2(n−1)s+i

√ m∈Mni (t) gsmp(θt) + −1aT(2n−1)s+i

m∈Mni (t) gsmp(θt)

+ zin(t), (32)

15

where aTj denotes the j-th row of measurement matrix A, and we note that g˜m,j(θt) = aTj gsmp(θt),

j

∈

[s˜].

The

PS

wants

to

recover

1 M

M m=1

gsmp

(θt)

from

its

noisy

observations

in

(32).

For

this,

using its knowledge of matrix A and the CSI, PS employs the approximate message passing

(AMP) algorithm [29]. The AMP algorithm is represented by AMPA in Algorithm 1. The PS

ﬁrst obtains, for i ∈ [s], n ∈ [N],



yˆ2(n−1)s+i(t)

=

 

Re{yin (t)}
γc | (t) Mni (t)|

,

0,

if |Mni (t)| = 0, otherwise,



yˆ(2n−1)s+i(t)

=

 

Im{yin (t)}
γc | (t) Mni (t)|

,

0,

if |Mni (t)| = 0, otherwise,

(33a) (33b)

and then estimates

gˆc (θt) = AMPA (yˆ(t)) ,

(34)

where we deﬁne yˆ(t) [yˆ1(t), · · · , yˆs˜(t)]T . If yˆ(t) = 0, gˆc (θt) is used to update the parameter vector as θt+1 = θt − ηtgˆc (θt). On the other hand, if yˆ(t) = 0, the previous parameter vector is simply used as the new one, i.e., θt+1 = θt.

Remark 3. For N = ⌈d/2s⌉, in which the entire gradient vectors are transmitted to the PS at each iteration, the CA-DSGD scheme reduces to the ECESA-DSGD scheme; that is, sparsiﬁcation and projection to a vector of smaller dimension, given in lines 6 and 8 of Algorithm 1, respectively, are not performed at the devices.

Remark 4. We remark that k is a design parameter which can take different values limited to

k

<

s˜.

For

relatively

small

k

values,

1 M

M m=1

gsmp

(θt)

can

be

recovered

from

1 M

M m=1

g˜m

(θt)

in a more reliable manner as we have more noisy measurements of the sparsiﬁed gradient

estimate

than

the

number

of

its

non-zero

elements;

however,

1 M

M m=1

gsmp

(θt)

provides

a

less

accurate

estimate

of

the

actual

average

gradient

1 M

M m=1

gm

(θt)

as

we

have

removed

more

componenets from the gradient estimates. On the other hand, for a relatively high k value,

1 M

M m=1

gsmp

(θt)

provides

a

better

estimate

for

1 M

M m=1

g

m

(θt);

however,

it

becomes

harder

to

recover

1 M

M m=1

gsmp

(θt)

from

1 M

M m=1

g˜m

(θt)

in

a

reliable

manner.

Remark 5. The proposed CA-DSGD scheme does not require data to be independent across

16

the devices. We assume that the gradient estimates computed by different devices have similar sparsity patterns. The rationale behind this assumption is that the gradient estimates across the devices follow an identical distribution, which is the result of assuming identical distribution for the local datasets across the devices. It is worth noting that this assumption helps to speed up the computation, but this is neither a requirement nor a limitation for the CA-DSGD scheme.
Remark 6. With ESA-DSGD, each device transmits only the entries of its estimated gradient whose corresponding channel conditions are sufﬁciently good. Thus, the gradient vector is inherently sparsiﬁed, but only based on the channel gains, regardless of the importance of the gradient entries. Then the entire gradient vector is sent over the bandwidth-limited wireless MAC over orthogonal time periods. On the other hand, with CA-DSGD, each device sends only k ≤ s˜ important gradient entries, where the magnitude of each entry is regarded as the importance metric, by projecting the sparse gradient vector to a low-dimensional vector of length s˜ ≤ d. We further highlight the error accumulation technique incorporated into ECESA-DSGD and CA-DSGD, whereas with ESA-DSGD, entries of the gradient vectors that are not sent are forgotten.

D. Average Transmit Power Analysis

Here we analyze the performance of the analog DSGD schemes under consideration in terms of the average transmit power. The average transmit power at device m, m ∈ [M], of the ESA-DSGD scheme in time slot n, n ∈ [⌈d/2s⌉], of iteration t based on (16), is given by

P¯me,n(t) E ||xnm(t)||22 =

s
E
i=1

αme,n,i(t) 2 gm,(n−1)s+i(θt) 2 .

(35)

We highlight that the entries of the gradient vector gnm(θt) are independent of the channel gains hnm,i(t), ∀i, n, m. Since the power allocation vector αem,n(t) is a function of gnm(θt), it follows that, for n ∈ [⌈d/2s⌉], m ∈ [M],

P¯me,n(t) =

s i=1

gm,(n−1)s+i(θt) 2 E

αme,n,i(t) 2 .

(36)

Note that hnm,i(t) 2 follows an exponential distribution with mean σm2 , ∀i, n, m. Thus, we have

E

αme,n,i(t) 2 =

γe(t) σm

2
E1(λem,n(t)),

(37)

17

where E1(x)

´∞
x

e−τ τ

dτ .

It

follows

that,

m

∈

[M ],

n

∈

[⌈d/2s⌉],

P¯me,n(t) =

γe(t) σm

2
E1(λem,n(t))Pme,n(t),

(38)

where we deﬁne Pme,n(t) gnm (θt) 22. To satisfy the average power constraint, we set the values P¯me,n(t), ∀m, n, t , such that

max
m∈[M ]

1 NT

T t=1

N n=1

P¯me,n(t)

≤ P¯,

(39)

where N = ⌈d/2s⌉. Since the power allocation vectors of ECESA-DSGD and CA-DSGD schemes, given in (22)
and (30), respectively, are similar to that of the ESA-DSGD scheme, we follow a similar procedure to obtain the average transmit power of each device for both the ECESA-DSGD and CA-DSGD schemes. For ECESA-DSGD, we have, m ∈ [M], n ∈ [⌈d/2s⌉],

where we deﬁne Pmv,n(t)

P¯mv,n(t) =

γv(t) σm

2
E1(λvm,n(t))Pmv,n(t),

(40)

gvm,n (θt) 22. Then, the values of P¯mv,n(t), ∀m, n, t are set to satisfy

max
m∈[M ]

1 NT

T t=1

N n=1

P¯mv,n(t)

≤ P¯,

(41)

where N = ⌈d/2s⌉. Similarly, for CA-DSGD, we have, m ∈ [M], n ∈ [N],

where we deﬁne Pmc,n(t)

P¯mc,n(t) =

γc(t) σm

2
E1(λcm,n(t))Pmc,n(t),

(42)

g˜nm (θt)

2 2

.

Then,

the

values of

P¯mc,n(t), ∀m, n, t

are set to satisfy

max
m∈[M ]

1 NT

T t=1

N n=1

P¯mc,n(t)

≤ P¯,

(43)

where N ∈ ⌈d/2s⌉.

V. NUMERICAL EXPERIMENTS
Here we compare the performances of the presented wireless edge learning schemes for the task of image classiﬁcation. We run experiments on the MNIST dataset [30] with 60000 training and 10000 test samples, and train a single layer neural network with d = 7850 parameters utilizing ADAM optimizer [31]. To model the data distribution across the devices, we assume

18

that B training data samples, selected at random, are assigned to each device at the beginning of training, in which case, with high probability, the datasets of any two devices will have non-empty intersection; that is, the local datasets are not independent across the devices. We highlight the fact that, rather than distributing the training dataset to the devices by assigning each of them a mutually exclusive distinct subset, we assign a random subset to each device allowing intersections. We believe the latter represents our scenario of edge learning across wireless nodes, e.g., mobile devices, vehicles, drones, or IoT sensors, which may share some of their data for various sensors, such as physical proximity, whereas the former would be more appropriate for a distributed learning scenario, where a master node with all the dataset employs a number of devices to speed up the learning process. We consider σm2 = 1, ∀m ∈ [M], which results in πm = 1/M, each device is selected for transmission with equal probability in the D-DSGD scheme. Thus, according to (7), for D-DSGD the following average power is imposed at device m:

1 T

T Pm(t) ≤ M P¯, ∀m ∈ [M ].
t=1

(44)

Throughout the experiments, we consider s = ⌈d/20⌉ parallel subchannels, which results in N = 10 for ESA-DSGD and ECESA-DSGD; and for any s˜ of the CA-DSGD scheme, we set the sparsity level to k = ⌊s˜/2.5⌋.
For a fair comparison between the analog DSGD schemes, we consider γx(t) = γ, ∀x ∈ {e, v, c}. We also set the threshold values λxm,n(t), ∀x ∈ {e, v, c}, such that device m consumes the same amount of power with any of the analog DSGD schemes at each time slot. To clarify, for two different DSGD schemes with N1 and N2 number of time slots per iteration, respectively, the n1-th time slot of the t1-th iteration of the ﬁrst scheme is equivalent to ((t1 − 1)N1 + n1)-th total time slot, and therefore, n2-th time slot of the t2-th DSGD iteration of the second scheme, where

t2 =

(t1 − 1)N1 + n1 N2

,

n2 = (t1 − 1)N1 + n1 − N2t2.

(45a) (45b)

Accordingly, for a design parameter s˜ with the CA-DSGD scheme resulting in N = s˜/2s ∈

19

[⌈d/2s⌉], we consider, for m ∈ [m], n ∈ [N],

P¯mc,n(t) = P¯me,n′(t′) = P¯mv,n′(t′), ∀t,

(46)

where

t′ =

(t − 1)N + n ⌈d/2s⌉

,

n′ = (t − 1)N + n − t′ ⌈d/2s⌉ .

(47a) (47b)

We remind here that P¯me,n(t), P¯mv,n(t) and P¯mc,n(t) are given in (38), (40) and (42), respectively, and from (46), it follows that

E1(λcm,n(t))Pmc,n(t) = E1(λme,n′(t′))Pme,n′(t′) = E1(λvm,n′(t′))Pmv,n′(t′), ∀m, n, t.

(48)

We remark that (48) guarantees the same average transmit power for all the analog DSGD schemes under consideration. Furthermore, to have a fair comparison between the analog and digital DSGD schemes, we set, for n ∈ [N],

Pm((t − 1)N + n) =

M m=1

P¯mc,n(t),

∀t,

(49)

i.e., the power allocated to the selected device for digital transmission at each time slot is equal to the total power consumed by all the devices for analog transmission at the same time slot. We note that, since

1 NT

T t=1

N n=1

P¯mc,n(t)

≤

P¯,

∀m ∈ [M],

(50)

(49) satisﬁes the average power constraint for the D-DSGD scheme for a total of T ′ = NT

iterations, given as follows:

1 T′

T ′ Pm(t) ≤ M P¯, ∀m ∈ [M ].
t=1

(51)

The performance is measured as the accuracy with respect to the training dataset versus the normalized time Nt, and the ﬁnal accuracy for the test samples, i.e., test accuracy, based on the parameter vector obtained after a total of NT = 100 time slots, which corresponds to 10 iterations for the ESA-DSGD and ECESA-DSGD schemes, and 100 iterations for the D-DSGD scheme, while the number of iterations for the CA-DSGD scheme depends on s˜.

20

T aining accu acy Training accuracy

0.8 0.7 0.6 0.5 0.4 0.3
0

0.8

0.7

CA-DSGD, P̄ = 3.72

0.6

ECESA-DSGD, P̄ = 3.72

ESA-DSGD, P̄ = 3.72

D-DSGD, P̄ = 3.72

0.5

20

40

60

80

Normalized time, Nt

(a) P¯ = 3.72 (γ = 2)

0.4

0.3

100

0

CA-DSGD, P̄ = 22.9 ECESA-DSGD, P̄ = 22.9 ESA-DSGD, P̄ = 22.9 D-DSGD, P̄ = 22.9

20

40

60

80

100

Normalized time, Nt

(b) P¯ = 22.9 (γ = 5)

Fig. 2: Training accuracy for two different P¯ values, P¯ ∈ {3.72, 22.9}.

TABLE I: Final test accuracy comparison following a training period of NT = 100 time slots.

CA-DSGD ECESA-DSGD ESA-DSGD D-DSGD

P¯ = 3.72 (γ = 2) 0.806

0.704

0.689

0.42

P¯ = 22.9 (γ = 5) 0.806

0.704

0.689

0.65

In Figures 2 and 3, we compare the performance of the analog DSGD schemes with that of the D-DSGD scheme. We consider a relatively high average transmit power in order to guarantee that, at each iteration, the selected device with the D-DSGD scheme can transmit at least one information bit, i.e., q(t) ≥ 1, ∀t.
In Fig. 2, we compare the performances of the CA-DSGD, ESA-DSGD and ECESA-DSGD schemes with D-DSGD for two different average transmit power values P¯ = 3.72 and P¯ = 22.9, averaged over NT = 100 time slots. These values are obtained by setting γ = 2 and γ = 5, respectively. We consider s˜ = 2s = d/10, i.e., N = 1 for CA-DSGD, M = 50 and B = 1200. We set λem,n(t) = 5 × 10−5, ∀m, n, t, and we ﬁnd the corresponding thresholds for CA-DSGD and ECESA-DSGD according to (48), and the transmit power at the selected device for the D-DSGD scheme from (49). The ﬁnal test accuracies of different schemes under consideration are presented in Table I. As it can be seen, CA-DSGD performs signiﬁcantly better than both ESA-DSGD and ECESA-DSGD. The main reasons for the degradation of the ESA-DSGD over CA-DSGD are i) scheduling gradient entries for transmission only based on the channel gains; ii) transmitting the entire gradient vectors of relatively huge dimensions (compared to the channel

21

0.8

0.8

Training accuracy Training accuracy

0.7 0.6 0.5 0.4 0.3 0.2
0

CA-DSGD, M = 50, B = 1200 ECESA-DSGD, M = 50, B = 1200 ESA-DSGD, M = 50, B = 1200 D-DSGD, M = 50, B = 1200

20

40

60

80

Normalized time, Nt

(a) (M, B) = (50, 1200)

0.7

0.6

0.5

0.4

0.3

0.2

100

0

CA-DSGD, M = 100, B = 600 ECESA-DSGD, M = 100, B = 600 ESA-DSGD, M = 100, B = 600 D-DSGD, M = 100, B = 600

20

40

60

80

100

Normalized time, Nt

(b) (M, B) = (100, 600)

Fig. 3: Training accuracy for different M and B values with the same total training dataset size.

TABLE II: Final test accuracy comparison following a training period of NT = 100 time slots.

CA-DSGD ECESA-DSGD ESA-DSGD D-DSGD

(M, B) = (50, 1200) 0.802

0.68

0.66

0.43

(M, B) = (100, 600) 0.812

0.685

0.67

0.556

bandwidth); iii) ignoring the gradient entries which have not been transmitted due to the bad conditions of their corresponding channels. We note that ECESA-DSGD resolves the last issue by utilizing error accumulation technique, which provides some gains with respect to ESA-DSGD, but we observe that better scheduling of the gradient transmissions and more efﬁcient utilization of the bandwidth through linear projection provide signiﬁcant gains, increasing the accuracy from 0.7 to 0.8. We also observe that the analog schemes are superior to the digital D-DSGD scheme, particularly in the low-power regime. By comparing Figures 2a and 2b, it can be seen that the analog schemes are much less sensitive to a reduction in the average transmit power than the D-DSGD scheme. This is because the signal-superposition property of the wireless MAC aligns the transmission power of all the transmitting devices in the case of analog transmission, which provides a much stronger protection against noise collectively, as the system continues to operate in a relatively high effective signal-to-noise ratio (SNR) regime despite some reduction in the transmission power of individual devices.
In Fig. 3, we compare the performances of analog and digital schemes for different (M, B) pairs, (M, B) ∈ {(50, 1200), (100, 600)}, both having the same size of training data in total.

22
We set s˜ = 2s = d/10, which results in N = 1 for CA-DSGD. We consider γ = 2, and λcm,n(t) = 0.1, ∀m, n, t, and ﬁnd the thresholds for other analog schemes from (48), and the transmit power of the scheduled device at the corresponding time slot for D-DSGD from (49). For a total number of time slots NT = 100, the average transmit power is P¯ = 3.72. We present the ﬁnal test accuracies for the settings under consideration in Table II. It is evident that the analog schemes again perform signiﬁcantly better than the D-DSGD scheme, and CA-DSGD improves upon ESA-DSGD and ECESA-DSGD noticeably in terms of the convergence speed and accuracy. As it can be seen, the performances of the analog schemes improve with M, since increasing M provides additional power introduced by each device and increases the robustness of the estimation against noise. D-DSGD also gains from increasing M, which is due to the additional power allocated to the selected device, as the devices are less frequently scheduled for transmission. We note that the superiority of the ECESA-DSGD over ESA-DSGD reduces with M, which shows that error accumulation is less effective for higher M values when MB is ﬁxed. The reason is that, for higher M, the chance of receiving more estimates for each entry of the actual gradient vector is higher (each gradient entry is estimated more accurately at the PS), and it is less likely that no estimate of any gradient entry is received by the PS. Accordingly, the beneﬁt of error accumulation is less signiﬁcant for higher number of devices.
In the following, we only consider analog DSGD schemes, where we evaluate the performance in the low-power regime, which is the regime of interest for many applications requiring edge learning across low-power wireless devices, and we set γ = 1. Speciﬁcally, we consider the impact of changing P¯, M, and s˜ on the performance of the analog DSGD schemes for low-power regime. We compare the performances of the three analog schemes for two different average power values of 0.33 and 0.11 in Fig. 4. We consider M = 100 and B = 600. For the CA-DSGD scheme, we set s˜ = 2s = d/10, in which case N = 1. We set λcm,n(t) = 0.5 and λcm,n(t) = 1, ∀m, n, t, which, respectively, result in average transmit powers 0.33 and 0.11 over NT = 100 time slots. The corresponding thresholds for CA-DSGD and ECESA-DSGD at each time slot are obtained according to (48). The ﬁnal test accuracies are listed in Table III. It is obvious that the gain of gradient compression proposed by the CA-DSGD scheme is huge compared to ESA-DSGD and ECESA-DSGD. Similarly to Fig. 3, we observe that the performance of are not very sensitive to the variation in the average transmit power. Observe that the superiority of ECESA-DSGD over ESA-DSGD decreases with the average power, since, for higher power (equivalently, smaller threshold value) a more accurate gradient vector is transmitted by each

23

0.8

T aining accu acy

0.7

0.6

CA-DSGD, P̄ = 0.33

0.5

CA-DSGD, P̄ = 0.̄̄

ECESA-DSGD, P̄ = 0.33

ECESA-DSGD, P̄ = 0.̄̄

0.4

ESA-DSGD, P̄ = 0.33

ESA-DSGD, P̄ = 0.̄̄

0

20

40

60

80

100

Normalized time, Nt

Fig. 4: Training accuracies of different analog schemes for two different P¯ values.

TABLE III: Final test accuracy comparison following a training period of NT = 100 time slots.

P¯ = 0.33 (λcm,n(t) = 0.5) P¯ = 0.11 (λcm,n(t) = 1)

CA-DSGD 0.828 0.824

ECESA-DSGD 0.707 0.703

ESA-DSGD 0.706 0.698

device (less number of gradient entries are set to zero). Thus, error compensation provides less performance gain.
In Fig. 5, we compare the CA-DSGD, ESA-DSGD and ECESA-DSGD schemes for different (M, B) pairs, (M, B) ∈ {(50, 1200), (100, 600)}, where MB is ﬁxed. For CA-DSGD, we set s˜ = 2s = d/10 resulting in N = 1 time slot per iteration. We consider λem,n(t) = 0.025, ∀m, n, t, and ﬁnd the thresholds for other analog schemes from (48). The resultant average power for a total of NT = 100 time slots is P¯ = 0.33. Table IV provides the ﬁnal accuracies of different schemes based on the parameter vector obtained after NT = 100 time slots. In addition to the signiﬁcant improvement of the CA-DSGD scheme over the other two analog schemes under consideration, it also gains more than other schemes from increasing M, which is due to the more efﬁcient utilization of the gradient estimates computed by the devices. On the other hand, the gap between ECESA-DSGD and ESA-DSGD reduces with M, since by introducing more devices while MB is ﬁxed, it is more likely to have a more accurate gradient entry at each gradient dimension even some gradient entries are ignored.

24

0.8

0.7

Training accuracy

0.6

0.5

CA-DSGD, M = 100, B = 600

CA-DSGD, M = 50, B = 1200

0.4

ECESA-DSGD, M = 100, B = 600

ECESA-DSGD, M = 50, B = 1200

0.3

ESA-DSGD, M = 100, B = 600 ESA-DSGD, M = 50, B = 1200

0

20

40

60

80

100

Normalized time, Nt

Fig. 5: Training accuracies of different analog schemes for different M and B values with the same total training dataset size.

TABLE IV: Final test accuracy comparison following a training period of NT = 100 time slots.

CA-DSGD ECESA-DSGD ESA-DSGD

(M, B) = (50, 1200) 0.82

0.698

0.686

(M, B) = (100, 600) 0.835

0.708

0.698

In Fig. 6 we investigate the impact of s˜ on the performance of CA-DSGD. We consider s˜ ∈ {2s, 4s} = {d/10, d/5} for CA-DSGD, in which s˜ = 2s and s˜ = 4s are equivalent to N = 1 and N = 2, respectively. We have M = 100 and B = 600, and set λem,n(t) = 0.05, ∀m, n, t, and ﬁnd the thresholds for other analog schemes from (48). The resultant average transmit power over NT = 100 time slots is P¯ = 0.26. In Table V, we summarize the ﬁnal test accuracy of different schemes. We highlight the superiority of the CA-DSGD scheme for both s˜ values under consideration over ESA-DSGD. The ECESA-DSGD scheme, which is equivalent to CA-DSGD for s˜ = d, also outperforms ESA-DSGD slightly. However, as it can be seen, the performance of CA-DSGD degrades as s˜ increases, which indicates that transmitting more sparse versions of the gradient estimates while using the available channel bandwidth for further iterations results in a higher accuracy. The ﬂexibility in choosing the dimension of the transmitted gradient estimates makes the proposed CA-DSGD scheme particularly compelling for learning at the wireless edge under strict bandwidth limit.

25

0.8

0.7

T aining accu acy

0.6

0.5

CA-DSGD, s̃ = 2s = d̃10

0.4

CA-DSGD, s̃ = 4s = d̃5 ECESA-DSGD

ESA-DSGD

0

20

40

60

80

100

Normalized time, Nt

Fig. 6: Training accuracies of different analog schemes, and different s˜ values for the CA-DSGD scheme.

TABLE V: Final test accuracy comparison following a training period of NT = 100 time slots.

CA-DSGD s˜ = 2s

CA-DSGD s˜ = 4s

ECESA-DSGD

ESA-DSGD

0.83

0.8

0.675

0.67

VI. CONCLUSIONS
We have studied distributed ML at the wireless edge, where M devices with limited transmit power and datasets communicate with the PS over a bandwidth-limited fading MAC to minimize a loss function by performing DSGD. The PS updates the parameter vector, and shares it with the devices over an error-free shared link. We ﬁrst presented a digital approach that treats computation and communication separately. At each iteration of the proposed D-DSGD scheme, one device is selected depending on the channel states, and the selected device ﬁrst quantizes its gradient estimate, and transmits the quantized bits to the PS using a capacity-achieving channel code. Then we studied the alternative analog transmission approach, cosidered recently in [25]–[27], which does not employ quantization or channel coding, and exploits the superposition property of the wireless MAC, rather than orthogonalizing the transmissions from different devices. First, we have extended the scheme in [26] by employing error accumulation. We have then proposed the CA-DSGD scheme, where each device employs gradient sparsiﬁcation with error accumulation

26
followed by linear projection to reduce the typically very large parameter vector dimension to the limited channel bandwidth. We have also designed a power allocation scheme to align the received vectors at the PS while satisfying the average power constraints at the devices. The CADSGD scheme allows a much more efﬁcient use of the limited channel bandwidth, and beneﬁts from the “beamforming effect” thanks to the similarity in the patterns of the gradient estimates across the devices. Numerical results have shown signiﬁcant improvements in the performance of CA-DSGD compared to D-DSGD and the state-of-the-art analog schemes.
REFERENCES
[1] J. Konecny, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon, “Federated learning: Strategies for improving communication efﬁciency,” arXiv:1610.05492v2 [cs.LG], Oct. 2017.
[2] B. McMahan and D. Ramage, “Federated learning: Collaborative machine learning without centralized training data,” [online]. Available. https://ai.googleblog.com/2017/04/federated-learning-collaborative.html, Apr. 2017.
[3] J. Konecny and P. Richtarik, “Randomized distributed mean estimation: Accuracy vs communication,” arXiv:1611.07555 [cs.DC], Nov. 2016.
[4] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized data,” in AISTATS, 2017.
[5] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task learning,” in Proc. Neural Information Processing Systems (NIPS), Long Beach, CA, USA, 2017.
[6] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning with limited numerical precision,” in ICML, Jul. 2015.
[7] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu, “1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs,” in INTERSPEECH, Singapore, Sep. 2014, pp. 1058–1062.
[8] D. Alistarh, D. Grubic, J. Z. Li, R. Tomioka, and M. Vojnovic, “QSGD: Communication-efﬁcient SGD via randomized quantization and encoding,” in NIPS, Long Beach, CA, Dec. 2017, pp. 1709–1720.
[9] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li, “TernGrad: Ternary gradients to reduce communication in distributed deep learning,” arXiv:1705.07878v6 [cs.LG], Dec. 2017.
[10] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients,” arXiv:1606.06160v3 [cs.NE], Feb. 2018.
[11] H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright, “ATOMO: Communication-efﬁcient learning via atomic sparsiﬁcation,” arXiv:1806.04090v2 [stat.ML], Jun. 2018.
[12] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, “signSGD: Compressed optimisation for non-convex problems,” arXiv:1802.04434v3 [cs.LG], Aug. 2018.
[13] B. Li, W. Wen, J. Mao, S. Li, Y. Chen, and H. Li, “Running sparse and low-precision neural network: When algorithm meets hardware,” in Proc. Asia and South Paciﬁc Design Automation Conference (ASP-DAC), Jeju, South Korea, Jan. 2018.
[14] N. Strom, “Scalable distributed DNN training using commodity gpu cloud computing,” in INTERSPEECH, 2015. [15] A. F. Aji and K. Heaﬁeld, “Sparse communication for distributed gradient descent,” arXiv:1704.05021v2 [cs.CL], Jul.
2017.

27
[16] Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, “Deep gradient compression: Reducing the communication bandwidth for distributed training,” arXiv:1712.01887v2 [cs.CV], Feb. 2018.
[17] X. Sun, X. Ren, S. Ma, and H. Wang, “meProp: Sparsiﬁed back propagation for accelerated deep learning with reduced overﬁtting,” arXiv:1706.06197v4 [cs.LG], Oct. 2017.
[18] F. Sattler, S. Wiedemann, K. Muller, and W. Samek, “Sparse binary compression: Towards distributed deep learning with minimal communication,” arXiv:1805.08768v1 [cs.LG], May 2018.
[19] C. Renggli, D. Alistarh, T. Hoeﬂer, and M. Aghagolzadeh, “SparCML: High-performance sparse communication for machine learning,” arXiv:1802.08021v2 [cs.DC], Oct. 2018.
[20] D. Alistarh, T. Hoeﬂer, M. Johansson, S. Khirirat, N. Konstantinov, and C. Renggli, “The convergence of sparsiﬁed gradient methods,” arXiv:1809.10505v1 [cs.LG], Sep. 2018.
[21] Y. Tsuzuku, H. Imachi, and T. Akiba, “Variance-based gradient compression for efﬁcient distributed deep learning,” arXiv:1802.06058v2 [cs.LG], Feb. 2018.
[22] S. U. Stich, “Local SGD converges fast and communicates little,” arXiv:1805.09767v2 [math.OC], Jun. 2018. [23] T. Lin, S. U. Stich, and M. Jaggi, “Don’t use large mini-batches, use local SGD,” arXiv:1808.07217v3 [cs.LG], Oct. 2018. [24] T. Chen, G. B. Giannakis, T. Sun, and W. Yin, “LAG: Lazily aggregated gradient for communication-efﬁcient distributed
learning,” arXiv:1805.09965 [stat.ML], May 2018. [25] M. M. Amiri and D. Gündüz, “Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air,”
arXiv:1901.00844 [cs.DC], Jan. 2019. [26] G. Zhu, Y. Wang, and K. Huang, “Low-latency broadband analog aggregation for federated edge learning,”
arXiv:1812.11494 [cs.IT], Jan. 2019. [27] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air computation,” arXiv:1812.11750 [cs.LG], Jan.
2019. [28] D. Tse and P. Viswanath, Fundamentals of wireless communication. Cambridge, UK: Cambridge University Press, 2005. [29] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms for compressed sensing,” Proc. Nat. Acad. Sci.
USA, vol. 106, no. 45, pp. 18 914–18 919, Nov. 2009. [30] Y. LeCun, C. Cortes, and C. Burges, “The MNIST database of handwritten digits,” http://yann.lecun.com/exdb/mnist/, 1998. [31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv:1412.6980v9 [cs.LG], Jan. 2017.

