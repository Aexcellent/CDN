PEM: A Practical Diﬀerentially Private System for Large-Scale Cross-Institutional Data Mining
Yi Li1, Yitao Duan2, and Wei Xu1
1 Tsinghua University, Beijing, 100084, China 2 Netease Youdao, Beijing, 100193, China
Abstract. Privacy has become a serious concern in data mining. Achieving adequate privacy is especially challenging when the scale of the problem is large. Fundamentally, designing a practical privacy-preserving data mining system involves tradeoﬀs among several factors such as the privacy guarantee, the accuracy or utility of the mining result, the computation eﬃciency and the generality of the approach. In this paper, we present PEM, a practical system that tries to strike the right balance among these factors. We use a combination of noise-based and noise-free techniques to achieve provable diﬀerential privacy at a low computational overhead while obtaining more accurate result than previous approaches. PEM provides an eﬃcient private gradient descent that can be the basis for many practical data mining and machine learning algorithms, like logistic regression, k-means, and Apriori. We evaluate these algorithms on three real-world open datasets in a cloud computing environment. The results show that PEM achieves good accuracy, high scalability, low computation cost while maintaining diﬀerential privacy.
1 Introduction
With the increasing digitization of society, more and more data are being collected and analyzed in many industries, including e-commerce, ﬁnance, health care and education. At the same time, however, privacy concerns are becoming more and more acute as our ever-increasing ability to extract valuable information from the data may also work against people’s fundamental rights to privacy. How to make good use of the data while providing an adequate level of privacy is an urgent problem facing both data mining and security communities.
Fundamentally, the problem boils down to striking a balance among three factors that are often at odds with one another: privacy, utility, and eﬃciency. Previous works largely focus on one or two of these aspects and, as a result, fail to provide practical solutions that can be used in real-world systems (to be elaborated later). Also, due to the diversity of applications regarding goals, algorithms, and data partition situations, a general solution that supports all the situations appears to be extremely hard, if not entirely impossible.
In this paper we target the cross-institutional mining problem where each institution (referred to as a client here) collects data independently and together they would like to mine collective data to extract insights. This setup can be

found in many real-world situations and typically it is more beneﬁcial to include more data in the analysis. For example, to study the side eﬀects of a drug, it is more accurate and timely if one could pool all the data from multiple relevant institutions (medical research institutes, hospitals, pharmaceutical companies and so on). The same applies to quick public health threats detection, educational data mining etc. Due to privacy concerns, however, institutions are prohibited by law from sharing their data.
This problem is called the horizontally partitioned database model in the privacy-preserving data mining community and has been studied extensively. Many solutions are algorithm-speciﬁc, such as clustering [25] and recommendation [3], and they are not scalable to handle large-scale problems. Some solutions are data-speciﬁc, e.g. [21]. Other solutions either make impratical assumptions (e.g., [27] assumes all the participants are semi-honest and the exact results do not reveal privacy), or involve heavy computation and communication cost that makes it unacceptable (e.g., [2] is based on secure multi-party computation and is very slow even for vector addition). In addition, early works tend to have a less rigorous notion of privacy.
The privacy we provide is diﬀerential privacy [12], a rigorous and strong notion that limits the probability that an adversary can distinguish between two datasets that diﬀer only by one record. A holistic privacy solution needs to protect any information that is released from its owner. In a distributed setting where data is partitioned among multiple institutions, general diﬀerential privacy doctrine would add noise in every of these occasions [3,21,22,24]. This approach does poorly for iterative data mining algorithms, as these algorithms would require noise for all intermediate results at each iteration, resulting in a ﬁnal result too noisy to be useful.
The key idea of our approach is that we can use eﬃcient and noise-free cryptographic primitives to reduce or even eliminate the noise for the intermediate results, preserving ﬁnal accuracy while achieve the same level of diﬀerential privacy. We adopt a secret sharing over small ﬁeld approach, similar to that of P4P [9] and Sharemind [5]. These frameworks are noise-free and eﬃcient, but lacking explicit mechanisms to protect the intermediate results, and supports only limited operations (e.g. addition). Our noise adding mechanisms compensate the limitation.
Concretely, we make the following contributions:
1) We combine a natural noise addition mechanism with an eﬃcient noise-free cryptographic primitive, making it diﬀerentially private. We show that utilizing our framework signiﬁcantly reduces the amount of noise necessary for maintaining diﬀerential privacy. We can use orders of magnitude lower noise and thus improve resulting model accuracy dramatically.
2) We provide a complete solution, together with easy to use programming APIs and ﬂexible options that enable diﬀerential privacy and performance optimizations. PEM automatically determines the noise level. This enables users to easily implement tradeoﬀ between privacy, scalability and accuracy.

3) We design mechanisms to deal with common distributed system issues such as fault tolerance and load balancing.
4) The system provides a diﬀerentially private gradient method that can be the basis of many machine learning algorithms. We also implement a number of commonly used machine learning algorithms such as k-means, Apriori etc.
Our goal is to provide a practical solution with provable privacy for many real-world analysis. We evaluate the tradeoﬀ between accuracy and privacy using three real-word datasets. Our results show that PEM not only guarantees diﬀerential privacy, achieving similar results as the original non-private algorithms, but also introduce very low computation overhead. We are adding more machine learning algorithms into PEM, and we will release PEM open source.
The paper is organized as follows. Section 2 reviews recent research on privacy-preserving computation. Section 3 formulates the problem and introduces import deﬁnitions. Section 4 introduces the goals of our system and describes some details about the system design. Section 5 gives some proofs why our system works. Section 6 illustrates three kinds of algorithms implemented in our system. Section 7 shows the experiment results. We conclude in Section 8.
2 Related Work
People have proposed many deﬁnitions of privacy over the years. Earlier versions, such as k-anonymity and l-diversity [16], are vulnerable to simple attacks [4,12]. Diﬀerential privacy is a popular deﬁnition that has strong privacy guarantee while still achievable in practice [12]. There are extended versions of diﬀerential privacy deﬁnitions such as pan privacy [11] and noiseless privacy [4]. These extensions often come with restriction to the datasets or use cases. Thus we adopt the general diﬀerential privacy deﬁnition.
The general approach to achieving diﬀerential privacy is to add noise to the released results [10,22]. Adding noise inevitably aﬀects the accuracy and people have explored several noise-free methods [4,8]. These works make use of the adversary’s uncertainty about the data thus eliminating the need for external noise. However, both [8] and [4] make strong assumptions about the data distribution to maintain diﬀerential privacy.
Many systems use cryptography to achieve diﬀerential or other types of privacy. They are based on either homomorphic encryption (HE) or secure multiparty computation (MPC) [28]. The problem with HE or MPC is that both make expensive use of large integer operations and are orders of magnitude slower than the non-private version, making them impractical. For example, Rastogi et al. proposes an algorithm PASTE, which uses HE for noise generation to mine distributed time-series data without any trusted server [21]. SEIPA performs privacy-preserving aggregation of network statistics using Shamir’s secret sharing scheme [23], which involves heavy polynomial calculation, making it slow in large datasets [6]. DstrDP [26] and SDQ [29] use HE and secure equality check to garble the noise from participants. Other approaches, including [2,5,14,19], also use expensive cryptograph techniques.

The other trend is to take advantage of the property of application algorithm or statistics of the dataset to avoid the expensive cryptographic operations. Chaudhuri et al. shows that sampling is helpful for increasing the accuracy of principal component analysis while preserving diﬀerential privacy [7]. Shokri et al. implements a system for privacy-preserving deep learning [24], using distributed selective SGD algorithm. However, the convergence of the algorithm strongly depends on the dataset. PINQ assumes a trusted third party to implement privacy-preserving queries [18] by adding Laplace noise. However, a single trusted third party is not only a strong assumption but also a performance and security bottleneck. P4P relaxes the trust assumption by allowing non-colluding semi-honest servers [9]. However, the noise-free approach in P4P still assumes too much about the dataset.
We combine these methods into a coherent system: we take the relaxed trust model, adding (reduced amount of) Laplace noise to achieve provable diﬀerential privacy, and leverage the properties of algorithms (such as sampling) to further reduce the amount of noise.

3 Preliminaries

3.1 Problem Formulation
In PEM, there are n clients. Each client Ci(i = 1, 2, . . . , n) has a subset of records Di to contribute to the computation. The goal of our computation is to use the union of all Di’s, which we denote as D, to compute some statistics, f (D). During the computation, each client wants to ensure that no information about the individual records in the dataset is leaked to other clients or any third pary. Speciﬁcally, we want to support any iterative computation where each iteration can be expressed in the form of

f (D) = g( h(v)).

(1)

v∈D

where both h and g can be nonlinear. This simple form can support a surprisingly large number of widely-used data mining algorithms, including k-means, expectation maximization (EM), singular value decomposition (SVD), etc.
Assume we have m independent servers, S1, . . . , Sm, and an aggregator server. We make the following key assumptions about the servers, which usually hold for real-world applications:

1. All servers are semi-honest, which means each server follows the protocol but may attempt to learn about all private data when it has a chance.
2. There are at least two servers that do not conspire with other servers to break the protocol. We can achieve this goal by using servers from diﬀerent administrate domains.
3. All communications are secure so that adversaries cannot see / change any useful information in the communication channels. We can ensure this assumption using encryption and message authentication methods.

In a realistic scenario, the clients can be collaborating institutions (e.g., hospitals, schools) that do not wish their data exposed to any party. The servers and aggregator can be cloud computing facilities that are owned by diﬀerent service providers. The non-colluding assumption upholds as the service providers are prohibited by law to leak their customer data. The number of servers, m, does not have to be very large. Using more servers increases security but also the cost. m = 2 or 3 is enough for many situations.

3.2 Deﬁnitions

We summarize important deﬁnitions for readers not familiar with the ﬁeld.
Diﬀerential Privacy [12]. An algorithm K gives -diﬀerential privacy if for all adjacent datasets D, D and all S ⊆ Range(K),

P r[K(D) ∈ S] ≤ e · P r[K(D ) ∈ S)

(2)

where adjacent datasets are two data sets that diﬀer in at most a single record. Intuitively, with diﬀerential privacy, we make the distributions of K(D) and K(D ) nearly undistinguishable by perturbing the outputs.
Laplace Distribution [12]. A random variable X follows Laplace distribution Lap(λ) if the probability density function (PDF) is

P r{X = x} = 1 e−|x|/λ

(3)

2λ

where λ is a parameter that determines the variance. The mean and variance of Lap(λ) are 0 and 2λ2, respectively. We denote Lapd(λ) as a d-dimensional vector consisting of d independent Lap(λ) random variables.
Sensitivity . For a function f : D → Rn, the L1-sensitivity S(f ) is deﬁned as:

S(f ) = max f (D1) − f (D2) 1

(4)

D1 ,D2

where D1 and D2 are two neighboring datasets diﬀering in at most one row. It can be shown that f (D) + r is -diﬀerentially private if r ∼ Lapd(S(f )/ ).

4 System Design
Diﬀerential privacy provides a rigorous deﬁnition and tuneable parameters for us to specify the level of protection that we desire. However, prior works such as [3,13,24] indicate that direct application of the noisy response approach may not allow us to ﬁnd a spot where acceptable level of privacy and utility coexist. In all the cases a large must be used, meaning that there is essentially little privacy, if we want the results to have any sensible usage.
To address this problem, it is clear that we must reduce noise as much as possible. Considering the distributed setting that we are dealing with, we are motivated to adopt the following design principle: use eﬃcient cryptographic

…… …………

C1 v1
C2 v2

v11 v1m v12
v21 v22
v2m

Cn vn
Clients

vn1 vn2
vnm

Feed back

r1

S1

∑v*1

r2

S2

∑v*2

Sm ∑v*m

Feed back
Servers

Aggregator

Fig. 1. The overview of PEM architecture.

tools whenever possible to eliminate the need for noise. Instead of adding noise at every client, we adopt secret sharing over small ﬁeld to perform the aggregation. This paradigm allows us to only perturb the ﬁnal aggregates, as secret sharing itself protects each client’s input. Since the number of servers is far less than the number of clients, the ﬁnal noise is much smaller.
4.1 System Architecture
There are three roles in PEM, and Figure 1 shows an overview of the system. Clients. The clients are the owners of the data sources. The PEM client module uses secret sharing protocol to privatize the data while providing useful tools for privacy tracing and model updates. Servers. The servers compute partial sums and add noise to the sums. Each server is logically a single component, while it can be implemented as a distributed system for scalability and fault tolerance. Aggregator. The aggregator performs all data aggregation and applicationspeciﬁc computation. It completes an iteration of model update based on the noisy aggregates and sends the model back to the clients.
4.2 Data Processing Protocol
On the start of the job, PEM distributes important parameters and the application executables to all the nodes, such as the privacy parameter and the list of participants. Then the data processing follows four major steps in PEM: 1) (Clients-Servers) Using secret sharing to aggregate client data. Each client secret-shares each data vector into m random vectors, one for each server. Speciﬁcally, for a vector vi held by client Ci (here we assume vi is an integer vector, while real-value vectors can be converted by discretization), the client can ﬁrst generate m − 1 random vectors vij ∈ Zdφ(j = 1, 2, . . . , m − 1), where φ

is a 32- or 64-bit prime and Zφ is the additive group of integers module φ. Then

it computes the m-th vector as vim = vi −

m−1 j=1

vij

mod φ. Obviously, the

sum of the m vectors equals to vi mod φ. The client Ci sends a random vector

vij to the server Sj(j = 1, 2, . . . , m), and Sj receives a random vector from each

client. Each server computes a sum vector using vectors from all clients.

2) (Servers) Generating Laplace noise to achieve privacy. To preserve

diﬀerential privacy, each server automatically determines the level of noise, gen-

erates Laplace noise following the approaches in [10] and adds it to its partial

sum. Each server Sj(j = 1, 2, . . . , m) generates a d-dimensional random vector

rj following Lapd(λ), and sends

n i=1

vij

+ rj

to

the

aggregator.

3) (Servers - Aggregator) Computing the sum of all vectors. At this

step, each server sends its perturbed partial sum to the aggregator for ﬁnal

aggregation. On receiving the sums from servers, the aggregator sums up the

vectors and obtains the ﬁnal sum, v =

n i=1

vi

+

m j=1

rj .

The

noise

prevents

the aggregator from guessing the original sum.

4) (Aggregator - Clients) Update model and complete an iteration. The aggregator runs the application code to generate or update the model. As many learning algorithms can be written in the form in Eq. 1, the sum obtained from the previous step is suﬃcient for a single iteration in the model update. Then the aggregator sends back necessary information, e.g., the latest models, back to each client for the next iteration.

4.3 -splitting and dynamic L setting

Privacy parameter ( ) splitting. As mentioned above, the servers add noise r ∼ Lapd(λ) to a d-dimensional sum. This works in many cases. However, some-

times we want to apply diﬀerent noise levels to each dimension. For example,

when we use k-means for clustering, we should calculate the sum of and the count

of each class to get the mean of these records. As the sensitivity of the records

and the sensitivity of the count of each class are diﬀerent, it is reasonable to add

noise of diﬀerent levels to them. PEM allows users to specify diﬀerent privacy

parameters to diﬀerent dimensions. Speciﬁcally, users can assign privacy param-

eter as [d1 : 1, d2 : 2, . . . , dk : k] where di is the subset of dimensions, as long

as ∀i,jdi ∩ dj = ∅, ∪idi = {1, 2, . . . , d} and

k i=1

i=

. Servers automatically

compute the amount of noise according to the manner of splitting.

Dynamic sensitivity (L) setting. PEM splits the (s) equally to each iteration in advance, and thus the noise levels of diﬀerent iterations can be calculated independently according to the sensitivity L and other parameters. However, sometimes the vectors to be added up from the clients may change over iterations, leading to a change of the sensitivity. In this case, L is set dynamically over each iteration, making the servers generate noise of diﬀerent levels. PEM allows users to specify L for each iteration. Then the λ will change accordingly and the servers will update the level of noise automatically.

4.4 Automatically computing the noise level (λ)
A key feature of PEM is the automatical computation of λ based on user settings, such as and L. As mentioned, each server generates noise following Laplace noise Lapd(λ), where λ is the noise level. In PEM, the servers automatically compute the noise level according to iteration count T , and L. As diﬀerent parameter setting means diﬀerent ways to generate noise, we consider the following cases:
– In the simplest case, where there is no -splitting or dynamic L setting, each server Sj computes λ = T · L/ and generates noise rj = Lapd(λ).
– If -splitting is speciﬁed, i.e., the privacy parameter is assigned as [d1 : 1, d2 : 2, . . . , dk : k], each server Sj computes λk = T · Lk/ k where Lk is the sensitivity of elements in dk and generates noise rj = [rj,1, rj,2, . . . , rj,k] = [Lap|d1|(λ1), Lap|d2|(λ2), . . . , Lap|dk|(λk)].
– If dynamic L setting is speciﬁed, i.e., the sensitivity of iteration t is L(t), each server Sj computes λ(t) = T · L(t)/ and generates noise rj = Lapd(λ(t)).
4.5 Optimizations
PEM is a federated system across multiple administrative domains. To handle the privacy-related requirements, we make the following extensions.
Extending servers to clusters. Although these operations are simple, we can still accelerate them to make PEM more eﬃcient. We can extend each server to a cluster (e.g., a Spark cluster) to parallize the communications and computations such that the servers are unlikely to be bottlenecks.
Handling client failures. If a server does not hear from a client for an extended period, it informs the aggregator about the client’s failure, and the aggregator notiﬁes all servers to remove the client.
Intuitive conﬁgurations and programing interfaces. The application developers only need to provide a few intuitive parameters, such as the sensitivity L, privacy strength , and the switch of sparse vector technique. PEM automatically determines other settings. The programmers only need to implement three functions: init, mapInClient and reduceInAggregator (see Sec. 6 for details).
Online processing. PEM also supports online processing. Clients send the data streams to servers for online processing. We model a data stream as a sequence of mini-batches, to reduce the communication overhead. In a mini-batch iteration, each client buﬀers the data records locally and sends them out periodically.
Algorithm speciﬁc optimizations. There are often algorithm speciﬁc optimizations to further reduce the required level of noise, such as sampling and sparse vector technique [12]. Sampling is commonly used to deal with imbalanced data, and can introduce more uncertainty about the raw data, reducing the level of noise required for diﬀerential privacy [15]. Sparse vector technique selects the queries above a pre-deﬁned threshold τ and set the unselected elements to 0. Thus we can compress the proposed sparse vector and save communication cost with a small accuracy loss [12]. PEM allows users to enable these optimizations, and PEM automatically performs them and adjusts the noise level accordingly.

5 Analysis

Provable diﬀerential privacy. We ﬁrst prove that we can make the algorithms diﬀerentially private to the aggregator and clients, and then prove that the algorithms are also diﬀerentially private to the servers. Formally, we determine the parameter λ using the sensitivity method [10]. Assuming the sensitivity is no more than L, i.e. |vi − vj| ≤ L, we have the following theorem.

Theorem 1 For any v ∈ Rd, v +

m j=1

rj

is

-diﬀerentially private if, for j =

1, 2, . . . , m, rj is drawn independently from Lapd(L/ ).

Proof. According to [10], if we add only one of rj(j = 1, 2, . . . , m), the algorithm is already -diﬀerentially private. As rj(j = 1, 2, . . . , m) are independent, adding another random noise to the result can still achieve -diﬀerential privacy.

We now show that the mechanism in PEM for calculating the sum is diﬀerentially private. Speciﬁcally, in PEM, as long as there are at least two servers that do not collude with others, the noisy sum calculated by the aggregator is -diﬀerentially private to all roles. The reason is straightforward: both the aggregator and the clients see the true vector sum plus at least two pieces of noise generated independently according to Lapd(L/ ). By Theorem 1, the aggregation is -diﬀerentially private to the aggregator and clients. Meanwhile, according to [9], PEM leaks no information beyond the intermediate sums to the servers as the servers’ view of the intermediate sums is uniformly random and contains no information about the raw data. On the other hand, as there are at least two semi-honest servers generating noise faithfully, each server sees the result perturbed by at least one piece of noise following Lapd(L/ ) even if it excludes its own noise from the sum, which is enough for preserving -diﬀerential privacy.
Cost and complexity. In PEM, there are m servers and n clients. We have m n in general. To preserve the privacy of data, m servers generate independent Laplace noise. As the variance of Laplace distribution Lap(λ) is 2λ2, the variance of the sum of m independent noises is 2mλ2. Comparing to the methods where all clients add noise (e.g. [3,24]), where the overall variance of the sum is 2nλ2, our method reduces much noise as m n.
As most of operations in PEM are vector addition and noise generation, the computational overhead grows nearly linearly with the number of clients and the data dimensionality. Thus, PEM is suitable for processing high-dimensional vectors. In PEM, noise generation and addition is orders of magnitudes faster, comparing to many homomorphic-encryption-based approaches, especially those related to high-dimensional vectors.

6 Sample Algorithms in PEM
To implement privacy-preserving data mining algorithms in PEM, users only need to implement the following necessary functions:

Function Init(): Initialize and L as constants. Initialize the iteration count T , the weight w,
loss function φ and the learning rate η.

Function mapInClient(x, y, w, φ): return ∂φ(x, y, w)/∂w.

Function reduceInAggregator (v, x, η, t):

update w as w = w − η √1 · t

1 n i=1

bi

v.

Algorithm 1: The user functions of gradient descent.

– init: initialize the parameters, including privacy parameter and the sensitivity L. Note that can be an array and L may change over the iterations.
– mapInClient: map the records of the local database to the vectors. – reduceInAggregator : analyze the sum of the vectors from the clients and
update the parameters.
We present three algorithms we have implemented on PEM: logistic regression, k-means and Apriori as examples.

6.1 Gradient descent

Gradient descent (GD) is a commonly-used algorithm for ﬁnding a local min-

imum of a function. Given an optimization problem, we want to minimize the

loss function φ(x, y, w), where x is the input, y is the expected output and w is

the parameter. In GD, to estimate the optimal w, we ﬁrst calculate the gradient

∇φ(w) on the dataset, then update w as wt+1 = wt − ηt · ∇φ(wt) where ηt is

the

learning

rate

in

iteration

t.

We

set

ηt

to

O( √1 ),
t

like

[30]

does.

In PEM, servers add Laplace noise to the sum of the gradients from the

clients. The formula for update becomes wt+1 = wt −ηt

m j=1

rj )

where

bi

is

the

size

of

sub-dataset

Di

held

by

1

n i=1

bi

client

(

n i=1

∇φi

(wt

)

+

Ci, ∇φ(w) is the

sum of gradients on Di and rj is the Laplace noise. Usually, GD consists of mul-

tiple iterations. Assuming the number of iterations is T , rj follows Lapd(T L/ ).

Algorithm 1 shows the user functions for gradient descent.

6.2 k-means

k-means is a simple yet popular algorithm for clustering. In standard k-means,

if we want to partition the d-dimensional records to l clusters, we ﬁrst initialize

l centroids c1, c2, . . . , cl of diﬀerent clusters. In each iteration, we assign each

record x to the cluster whose centroid c is the closest to it. Then we update

each

centroid

to

the

mean

of

the

records

in

its

cluster:

ci

=

1 |Si |

x∈Si x where

ci is the centroid of cluster i, Si is the set of records belonging to cluster i

and |Si| is the number of elements in Si. In one iteration, c1, c2, . . . , cl should

Function Init(): Initialize the iteration count T and the number of centroids l. Then initialize
the rest parameters as follows: c = [c1, c2, . . . , cl], d1 = {1 to ld}, d2 = {ld + 1 to ld + l}, = [d1 : 1, d2 : 2] and L = [d1 : 2Lx, d2 : 2].
Function mapInClient(x, c): First we initialize the gradient g := [g1, g2, · · · , gl, g1, g2, · · · , gl], where
gi = 0 and gi = 0 for each i ∈ {1 to l}. Then we calculate k = f indCluster(x, c) and set gi = x and gi = 1. Return g as the result.
Function reduceInAggregator (v): Represent v as [v1, v2, · · · , vl, v1, v2, · · · , vl] and update the centroid
c = [v1/v1, v2/v2, . . . , vl/vl]. Algorithm 2: The user functions of k-means.

be updated all together, which means that the clients should jointly calculate

( x∈S1 x, x∈S2 x, . . . , x∈Sl x, |S1|, |S2|, . . . , |Sl|).
As the change of a record belonging to cluster i may change x∈Si x and |Si| simultaneously, the servers in PEM should not only add noise to perturb the cen-

troids, but also add noise to perturb the number of records of each cluster. Specif-

ically, we modify the formula for centroid update as ci = |Si|+

( 1

m j=1

rj,2

x∈Si x+

m j=1

rj,1)

where

rj,1

is

the

noise

for

the

sum

of

records

and

rj,2

is

the

noise

for

the number of records in each cluster.

The records themselves and record-count obviously carry diﬀerent amount

of privacy information, and thus we add diﬀerent levels of noises to the records

dimensions and the count dimension. Using PEM’s -spliting feature, we set the

values of 1 and 2 as the users want, as long as their sum is . Denote L1 to be
the sensitivity of the sum of records ( x∈S1 x, x∈S2 x, . . . , x∈Sl x) and L2 to be the sensitivity of the number of records in the clusters (|S1|, |S2|, . . . , |Sl|). It
is easy to see that L1 = 2 · Lx and L2 = 2 · 1, where Lx is the maximum 1-norm
of x. There is a coeﬃcient 2 here is because the change of an record may aﬀect at most two clusters. Then we have rj,1 ∼ Lapld(T L1/ 1) and rj,2 ∼ Lapl(T L2/ 2)
where T is the number of iterations. Algorithm 2 shows the pseudo-code.

6.3 Apriori
Apriori is an algorithm for frequent itemset mining [17]. The support of a set of items (itemset) is the fraction of records containing the itemset respect to the database D. If the support of an itemset is above a preassigned minimum support, we call the itemset a large itemset. The target of frequent itemset mining is to ﬁnd all the large itemsets. We denote Ik as the set of large itemsets of length k. To ﬁnd large itemsets Ik, Apriori uses a function called apriori-gen, which takes Ik−1 as an argument and generates candidates of k-itemsets denoted by Ik∗. Then Apriori calculates the count of each itemset in Ik∗ and reserves the

Function Init(): Set lm as the length of the longest record, I1 as the set of atomic items, and
minsup. Initialize the iteration count T and the privacy parameter . Then set the sensitivity function L(k, Ik∗) accordingly.
Function mapInClient(x, Ik−1): Set Ik∗ = apriori-gen(Ik−1) and return count(Ik∗, x).
Function reduceInAggregator (v): Here the parameter v is count(Ik∗, x), i.e. the output of mapInClient. First we
set Ik∗ = apriori-gen(Ik−1) and Ik as the empty set {}. Then for each element e ∈ Ik∗, if v[e] > minsup, put it in Ik. Ik is the set of large items of length k.
Algorithm 3: The user functions of Apriori.

itemsets whose supports are above the minimum support. The set of reserved

large k-itemsets from Ik∗ is Ik. As the change of a record will aﬀect the count of itemsets, we should add noise

to the count of the itemsets to preserve privacy. In PEM, in the situation where

the dataset is distributed in multiple clients, apriori-gen can be done locally in

each client given Ik−1 and the count of each itemset in Ik−1. Filtering Ik∗ to

get Ik involves no raw data and thus can be done in the aggregator without

privacy issue. We only need to add noise to the count of each itemset in Ik∗.

Formally, we represent Ik∗ itemset. In each iteration

as [t1, t2, . . . , t|Ik∗|] for generating Ik,

where ti ∈ Ik∗ is a candidate large count(Ik∗, D) = [c1, c2, . . . , c|Ik∗|] is

calculated, where ci is the count of ti respect to the dataset D. The sensitivity

L of k-itemset counts is diﬀerent for diﬀerent k values. A record of length l

contains at most

l k

=

l! k!(l−k)!

itemsets

of

length

k.

The

change

of

a

record

may

aﬀect the counts of at most 2 ·

l k

itemsets. Then if we know the length lm of

the longest record beforehand, we can calculate the sensitivity of count(Ik∗, D) as

Lk

= min(2·

lm k

, |Ik∗|). Algorithm 3 shows the user functions. We enable dynamic

L setting and thus λ is modiﬁed in each iteration along with the sensitivity Lk.

7 Evaluation
7.1 Evaluation Setup
We empirically evaluate PEM in a cloud computing environment. We setup one aggregator and two servers (m = 2). Each of them runs on a virtual machine (VM) node. Each VM has 8 Xeon CPU cores, 16GB RAM and 10GE ethernet. We emulate 100 ∼ 1000 clients, each of which uses a sperate VM node with the same conﬁguration. We use three open datasets from UCI Machine Learning Repository [1] for evaluation. We partition each dataset evenly onto the clients, emulating a horizontally partitioned dataset setting.

– The Adult dataset contains information of many people, including gender, age and salary. We clean the dataset and ﬁnally get 48,842 data records, each of which has 124 dimensions.
– The Nursery dataset is derived from a hierarchical decision model originally for nursery schools. There are 21,960 instances and 8 categorical attributes.
– The Mushroom dataset contains information of hypothetical samples corresponding to 23 species of gilled mushrooms. There are 8,124 instances with 22 attributes, each of which describes some shape information.
In our evaluation, we compare our algorithms with no-privacy versions. Meanwhile, many approaches add adequate noise on all clients, which means 100 ∼ 1000 Laplace noises in our experiment setting. We call the approach noise-only approach, and we show the comparison with ours. Finally we show the performance overhead of PEM.

7.2 Performance of Algorithms

Distributed gradient descent. We use logistic regression as the example of gradient descent shown in Algorithm 1. With the Adult dataset, we construct a logistic regression model to predict whether each person has high income (> 50K) or not (≤ 50K). We preprocess the dataset using one-hot encoding and the sensitivity L is 28 here. Each accuracy number is obtained from a 10-fold validation with 1000 iterations. Using diﬀerent values of , we compare the model prediction accuracy using diﬀerent approaches including no privacy, noise-only (100 clients) and PEM approach. Using diﬀerent numbers of clients, we compare the model prediction accuracy with = 1. Figure 2(a) shows the comparison. As expected, the noise-only approach needs to add signiﬁcant noise, reducing the prediction accuracy signiﬁcantly. Using the severs to add noise signiﬁcantly lowers the loss in model accuracy, even for small . In comparison, noise-only approach with too many clients will cause too much noise.

Distributed clustering. We use k-means for clustering as shown in Algo-

rithm 2, with the Nursery dataset. There are 8 categorical attributes in the

dataset. We preprocess using one-hot encoding and ﬁnally get 27 binary at-

tributes. It is easy to see that Lx = 8, and thus L1 = 16. In this experiment,

we partition the records into ﬁve clusters, i.e., l = 5. We split equally to

1 and

2, i.e.

1=

2

=

1 2

.

We

calculate

the

loss

function

φ

as

φ(D, c)

=

l i=1

x∈Si x−ci 2. As our goal is to compare the loss of diﬀerent approaches,

we use the relative loss to evaluate the performance of diﬀerent approaches,

which is calculated as φr(D, c) =

φ(D,c)−φ0 φ0

where φ0

is

the

loss

of the approach

without noise. Each training has 50 iterations. Figure 2(b) shows the result.

Distributed frequent itemset mining. We perform distributed frequent item-

set mining on the Mushroom dataset using Algorithm 3. Considering the item-

sets and the counts of them, we deﬁne the loss function as φ(I) =

k t∈Ik∪Ik0 |Ik(t)−Ik0(t)| k t∈Ik∪I0k |Ik(t)+Ik0(t)|

where I = [I2, I3, . . . ], Ik0 is the set of large itemsets of length k generated with-

out noise, and Ik(t) is the (perturbed) count of t for t ∈ Ik. In this experiment, we

Accuracy

0.9

0.8

0.7

0.6

no noise

PEM

noise-only

0.5 0.2 0.4 0.6 0.8 1 3 5 7 9

0.9

0.8

0.7

0.6

no noise

PEM

noise-only

0.5 20 40 6c0li8e0n1t0c0o30u0n5t00 700 900

(a) LR, Adult

Relative Loss φr

Relative Loss φr

1.2

PEM

1.0

noise-only

0.8

0.6

0.4

0.2

0.0 0.2 0.4 0.6 0.8 1 3 5 7 9

1.2

PEM

1.0

noise-only

0.8

0.6

0.4

0.2

0.0 20 40 6c0li8e0n1t0c0o30u0n5t00 700 900

(b) k-means, Nursery

Loss φ

Loss φ

1.0 0.8 0.6 0.4 0.2
PEM noise-only
0.0 0.2 0.4 0.6 0.8 1 3 5 7 9
1.0 0.8 0.6 0.4 0.2
PEM noise-only
0.0 20 40 6c0li8e0n1t0c0o30u0n5t00 700 900
(c) Apriori, Mushroom

Accuracy

Fig. 2. Evaluation of algorithms using the according datasets with diﬀerent values of (client count = 100) and diﬀerent numbers of clients (ﬁxing = 1).

consider the large itemsets of length no more than 4, which means I = [I2, I3, I4]. We set minsup to 0.01. Figure 2(c) shows the result.
7.3 Scalability
The main computation workload of PEM comes from two parts: 1) computation overhead: vector addition and noise generation; and 2) communication overhead. We show that the overhead is small in most of the cases. First, we increase the number of clients from 0 to 10000 and Figure 3(a) shows the computation time for a scalar on diﬀerent roles. The aggregator computation time is independent of the number of clients, as it only receives a vector from each server. Thus the aggregator is unlikely to become the bottleneck. The server workload increases linearly with the number of clients. Fortunately, we can increase each logical server capacity by adding more nodes. Figure 3(b) shows that the computation time is linear to the dimensionality of the feature vector, as expected. Finally, we record the overall overhead of vector addition, including the computation and communication overhead. Figure 3(c) shows that, compared with the communication time, the computation (i.e. vector addition) time can be negligible.
8 Conclusion and Future Work
We present PEM as a practical tradeoﬀ among privacy, utility and computation overhead. PEM is practical in that 1) it has simple assumptions: it only requires a few semi-honest servers and there is no restriction on the dataset itself; 2) it supports a large range of common applications; 3) PEM entails low computation overhead and is scalable to a large number of clients; and 4) all user-visible

time(ms) time(ms) time(ms)

2.5 client
2.0 server aggregator
1.5 1.0 0.5
0.00 2000 c4l0ie00nt6c00o0un80t00 10000
(a)

12

10

client server

8 aggregator

6

4

2

00 nu20m0be4r00of 6d0im0 e8n0s0io1n0s00

(b)

1400 dim = 100

1200 1000

dim = 500 dim = 1000

800

600

400

200

00 1000 c20li0e0n3t00c0o4u0n00t 5000 6000

(c)

Fig. 3. (a) computation overhead w.r.t. number of clients. (b) computation overhead w.r.t the dimensionality of feature vectors. (c) overall overhead.

conﬁguration parameters are intuitive and PEM automatically determines other internal parameters. Using algorithms on real datasets, we show that we can achieve the same level of privacy without the amount of accuracy degradation that previous systems suﬀer from. Our system also has low computation and communication cost, and thus is very practical.
There are lots of future directions along the lines of privacy. Firstly, we are extending our system to support more operations, such as handling vertically partitioned datasets. Secondly, we will provide the ﬂexibility allowing the clients to choose diﬀerent trust assumptions, so that the application programmers can choose their own tradeoﬀs. Last but not least, we will provide a permission system allowing diﬀerent clients to see diﬀerent levels of private content, like CryptDB [20] does, but on a much larger scale.
Acknowledgement
This research is supported in part by the National Natural Science Foundation of China (NSFC) grant 61532001, Tsinghua Initiative Research Program Grant 20151080475, MOE Online Education Research Center (Quantong Fund) grant 2017ZD203, and gift funds from Huawei and Ant Financial.
References
1. A. Asuncion and D.J. Newman. UCI machine learning repository, 2007. 2. A. Ben-David, N. Nisan, and B. Pinkas. FairplayMP: a system for secure multi-
party computation. In CCS ’08. ACM, 2008. 3. A. Berlioz, A. Friedman, M.A. Kaafar, R. Boreli, and S. Berkovsky. Applying
diﬀerential privacy to matrix factorization. In RecSys. ACM, 2015. 4. R. Bhaskar, A. Bhowmick, V. Goyal, S. Laxman, and A. Thakurta. Noiseless
database privacy. In Advances in Cryptology–ASIACRYPT 2011. Springer, 2011. 5. D. Bogdanov, S. Laur, and J. Willemson. Sharemind: A framework for fast privacy-
preserving computations. In ESORICS. Springer, 2008. 6. M. Burkhart, M. Strasser, and D. e.t.c. Many. SEPIA: Privacy-preserving aggre-
gation of multi-domain network events and statistics. Network, 1, 2010.

7. K. Chaudhuri, A.D. Sarwate, and K. Sinha. A near-optimal algorithm for diﬀerentially-private principal components. JMLR, 14(1), 2013.
8. Y. Duan. Diﬀerential privacy for sum queries without external noise. In ACM Conference on Information and Knowledge Management (CIKM), 2009.
9. Y. Duan, J. Canny, and J. Zhan. P4P: Practical Large-scale Privacy-preserving Distributed Computation Robust Against Malicious Users. In Proceedings of USENIX Security. USENIX Association, 2010.
10. C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography. Springer, 2006.
11. C. Dwork, M. Naor, T. Pitassi, G.N. Rothblum, and S. Yekhanin. Pan-Private Streaming Algorithms. In ICS, 2010.
12. C. Dwork and A. Roth. The algorithmic foundations of diﬀerential privacy. Foundations & Trends in Theoretical Computer Science, 9(34), 2014.
13. A. Friedman, I. Sharfman, D. Keren, and A. Schuster. Privacy-Preserving Distributed Stream Monitoring. In NDSS, 2014.
14. Q. Jia, L. Guo, Z. Jin, and Y. Fang. Privacy-preserving data classiﬁcation and similarity evaluation for distributed systems. In ICDCS. IEEE, 2016.
15. N. Li, W. Qardaji, and D. Su. On sampling, anonymization, and diﬀerential privacy or, k-anonymization meets diﬀerential privacy. In Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security. ACM, 2012.
16. A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. l-diversity: Privacy beyond k-anonymity. ACM TKDD, 1(1), 2007.
17. M.H. Margahny and A.A. Mitwaly. Fast algorithm for mining association rules. Proc.int.conf.very Large Databases (VLDB), 23(3), 1994.
18. F.D. McSherry. Privacy integrated queries: an extensible platform for privacypreserving data analysis. In Proceedings of SIGMOD. ACM, 2009.
19. M. Pathak, S. Rane, and B. Raj. Multiparty diﬀerential privacy via aggregation of locally trained classiﬁers. In NIPS, 2010.
20. R.A. Popa, C.M.S. Redﬁeld, N. Zeldovich, and H. Balakrishnan. Cryptdb: Protecting conﬁdentiality with encrypted query processing. In ACM SOSP, 2011.
21. V. Rastogi and S. Nath. Diﬀerentially private aggregation of distributed time-series with transformation and encryption. In Proceedings of SIGMOD. ACM, 2010.
22. R. Sarathy and K. Muralidhar. Evaluating Laplace Noise Addition to Satisfy Diﬀerential Privacy for Numeric Data. Transactions on Data Privacy, 4(1), 2011.
23. A. Shamir. How to share a secret. Communications of the ACM, 22(11), 1979. 24. R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In ACM Conference
on Computer and Communications Security, 2015. 25. D. Su, J. Cao, N. Li, E. Bertino, and H. Jin. Diﬀerentially private k-means clus-
tering. In Proceedings of CODASPY. 2016. 26. H. Takabi, S. Koppikar, and S. T. Zargar. Diﬀerentially private distributed data
analysis. In Collaboration and Internet Computing (CIC), 2016 IEEE 2nd International Conference on, pages 212–218. IEEE, 2016. 27. K. Xu, H. Yue, L. Guo, Y. Guo, and Y. Fang. Privacy-preserving machine learning algorithms for big data systems. In Distributed Computing Systems, 2015. 28. Andrew C Yao. Protocols for secure computations. In Foundations of Computer Science, 1982. SFCS’08. 23rd Annual Symposium on. IEEE, 1982. 29. N. Zhang, M. Li, and W. Lou. Distributed data mining with diﬀerential privacy. In Communications (ICC), 2011 IEEE International Conference on, pages 1–5. IEEE, 2011. 30. M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. ICML, 2003.

