{"indexedPages":15,"totalPages":15,"version":"514","text":"algorithms\nArticle\nA Forecast Model of the Number of Containers for Containership Voyage\nYuchuang Wang 1, Guoyou Shi 1,* and Xiaotong Sun 2 1 Key Laboratory of Navigation Safety Guarantee of Liaoning Province, Navigation College, Dalian Maritime University, Dalian 116026, China; wyc2017@dlmu.edu.cn 2 Shipping Economics and Management College, Dalian Maritime University, No. 1 Linghai Road, Dalian 116026, China; sunxiaotong@dlmu.edu.cn * Correspondence: nsgi@dlmu.edu.cn; Tel.: +86-411-8472-5168\nReceived: 20 October 2018; Accepted: 26 November 2018; Published: 28 November 2018\nAbstract: Container ships must pass through multiple ports of call during a voyage. Therefore, forecasting container volume information at the port of origin followed by sending such information to subsequent ports is crucial for container terminal management and container stowage personnel. Numerous factors inﬂuence container allocation to container ships for a voyage, and the degree of inﬂuence varies, engendering a complex nonlinearity. Therefore, this paper proposes a model based on gray relational analysis (GRA) and mixed kernel support vector machine (SVM) for predicting container allocation to a container ship for a voyage. First, in this model, the weights of inﬂuencing factors are determined through GRA. Then, the weighted factors serve as the input of the SVM model, and SVM model parameters are optimized through a genetic algorithm. Numerical simulations revealed that the proposed model could effectively predict the number of containers for container ship voyage and that it exhibited strong generalization ability and high accuracy. Accordingly, this model provides a new method for predicting container volume for a voyage.\nKeywords: container transportation; prediction of voyage container volume; SVM; GRA\n\n1. Introduction\nContainer transportation is a highly complicated process and involves numerous parties, necessitating close cooperation between ports, ships, shipping companies, and other relevant departments. Therefore, container transportation management is characterized by extremely detailed planning [1,2]. For example, container terminals must formulate strategies such as berthing plans [3], container truck dispatch plans, yard planning systems, and yard stowage plans [4–6]. In addition, ships or shipping companies must formulate voyage stowage plans for container ships at the port of departure. The number of containers in the subsequent port must be predicted, and such prediction information forms a crucial basis for the subsequent plan. These processes must be completed before the development of a stowage system for full-route container ships [7].\nChanges in the number of containers allocated to container ships are inﬂuenced by several factors, which are characterized by uncertain information; this thus engenders a complex nonlinear relationship between the number of allocated containers and inﬂuencing factors [8]. The number of allocated containers is inﬂuenced by the port of call, local GDP, port industrial structures, and collection and distribution systems; by shipping company-related factors such as the capacity of a company, inland turnaround time of containers, seasonal changes in cargo volume, and quantity of containers managed by the company; and by ship-related factors such as the transportation capacity of a single ship and the full-container-loading rate of the ship. Each of these factors exerts distinct effects on the number of containers allocated to a container ship for one voyage; therefore, describing these factors by using\n\nAlgorithms 2018, 11, 193; doi:10.3390/a11120193\n\nwww.mdpi.com/journal/algorithms\n\nAlgorithms 2018, 11, 193\n\n2 of 15\n\nan accurate mathematical model is difﬁcult [9]. Traditional methods such as time series forecasting (including exponential smoothing [10], gray prediction [11], moving average [12], and seasonal periodic variation [13] approaches) and regression forecasting [14] typically rely on certain mathematical theories and assumptions and necessitate the establishment of mathematical models through deductive reasoning without considering inﬂuencing factors. By contrast, neural networks—constituting a nonlinear and nonparametric model—can describe the nonlinear relationship between a premeasured quantity and inﬂuencing factors, and they have self-learning and self-adaptation abilities to effectively avoid prediction errors caused by assumptions; accordingly, neural networks have been extensively applied in various project prediction processes. However, neural networks [15] have drawbacks such as an indeterminable network structure, overﬁtting, local minimum, and “curse of dimensionality”. Support vector machines (SVMs) [16] solve the aforementioned drawbacks of neural networks by minimizing structural risk, and SVMs are highly suitable for predicting the number of containers allocated to a container ship for one voyage, a problem that involves characteristics such as nonlinearity, high dimensionality, and a small-scale sample [17–20]. An SVM entails the selection of a kernel function, thus demonstrating the nonlinear processing ability of the learning machine; because a kernel function is selected [21], a feature space is deﬁned [22,23]. Nevertheless, a single kernel function cannot afford learning ability and generalization ability simultaneously in an SVM [24]. This paper proposes mixed kernels, which can effectively improve the predictive performance of an SVM model with weighted arrays of polynomial and radial basis kernel functions.\nAs mentioned, each of the aforementioned factors exerts distinct effects on the various factors inﬂuencing the number of allocated containers in one voyage. If such differences are neglected, the prediction results would be distorted [25]. Therefore, this paper proposes a model based on gray relational analysis (GRA) theory [26] and mixed kernel SVM for predicting container allocation to a container ship. In this model, GRA is applied to obtain the gray relational ordinal of each inﬂuencing factor, thus determining the weight of each factor. Subsequently, the weighted inﬂuencing factors serve as inputs to the SVM model, thus resulting in the mixed kernel SVM prediction model. To solve more complex parameter optimization problem in the mixed kernel SVM model, a genetic algorithm (GA) is adopted for SVM parameter optimization [27,28]. Simulations are presented herein to demonstrate the effectiveness of this method. The novelty of this paper is that it proposes a mixed kernel SVM model for predicting the number of containers allocated to a container ship for a voyage.\n\n2. Gray Relation Analysis\n\nGray relation analysis is the serialization and patterning of the gray relation between an operating mechanism and its physical prototype, which is either not clear at all or certainly lacks a physical prototype. The “essence” of the analysis is an overall comparison of the measurements with a reference system [29]. The technical connotation of gray relation analysis is: (i) acquiring information about the differences between sequences and establishing a difference information space; (ii) establishing and calculating the differences to compare with the measurements (gray correlation degree); and (iii) establishing the order of the relation among the factors to determine the weight of each inﬂuencing factor [30]. The calculation steps are as follows [31]:\nStep 1: Set a sequence X0 = (x0(1), x0(2), · · · , x0(k), · · · , x0(n)) as the reference sequence, i.e., the object of study and Xi = (xi(1), xi(2), · · · , xi(k), · · · , xi(n),)(i = 1, 2, · · · , m) as the comparative sequence, i.e., the influencing factors.\nStep 2: Data conversion or dimensionless processing for which, the initialization conversion is adopted in this study, wherein the ﬁrst variable of each sequence is used to remove all the other variables to obtain the initial value of the image Yi(k):\n\nYi (k)\n\n=\n\nXi (k) Xi (1)\n\n(1)\n\nwhere i = 0, 1, 2, · · · , m and k = 1, 2, · · · , n.\n\nAlgorithms 2018, 11, 193\n\n3 of 15\n\nStep 3: Calculate the gray relation coefﬁcient γ(Y0(k), Yi(k)) of Y0(k) and Yi(k):\n\nγ(Y0(k), Yi(k))\n\n=\n\nmin\ni\n\nmin\nk\n\n∆0i\n\n(k)\n\n+\n\nζ\n\n∆0i (k)\n\n+\n\nζ\n\nmax\ni\n\nmax\ni\nmax\nk\n\nmax\nk\n\n∆0i (k)\n\n∆0i (k)\n\n(2)\n\n∆0i(k) = |Y0(k) − Yi(k)|\n\n(3)\n\nwhere ζ ∈ [0, 1] is the resolution coefﬁcient. The resolution coefﬁcient ζ determines the result of the\n\ncorrelation analysis. The literature [32] shows that when ζ ≤ 0.05, the resolution of the correlation\n\ndegree changes more obviously, so ζ = 0.05 is selected in this paper.\n\nStep 4: Calculate the correlation degree γ(Y0, Yi) of the subsequence Yi(k) of the parent sequence\n\nY0(k):\n\nγ(Y0, Yi)\n\n=\n\n1 n\n\nn\n∑\nk=1\n\nγ(Y0\n\n(k),\n\nYi\n\n(k))\n\n(4)\n\nCompare γ(Y0, Yi) with γ Y0, Yj (i = j), and if γ(Y0, Yi) > γ Y0, Yj , it is indicated that the ith factor has a greater impact on the results than the jth factor.\nStep 5: Calculate the weight of the various inﬂuencing factors:\n\nw(Yi) =\n\nγ(Y0, Yi)\nm\n\n(5)\n\n∑ γ(Y0, Yi)\n\ni=1\n\n3. Mixed Kernel Function SVM Prediction Model\n\n3.1. Support Vector Machine for Regression\nA support vector machine (SVM) was ofﬁcially proposed by Cortes & Vapnik in 1995, which was a signiﬁcant achievement in the ﬁeld of machine learning. Vapnik et al. [16,17] introduced an insensitive loss function ε based on the SVM classiﬁcation and obtained a support vector machine for regression (SVR), in an attempt to solve the regression problem. The structural diagram of the SVR is shown in Figure 1 in which the number of allocated containers of the output container ship for one voyage, g(x), is a linear combination of intermediate nodes [33]. Each intermediate node corresponds to a support vector, x1, x2, · · · , xl represents the input variable, αi∗ − αi is the network weight, and K(xi, x) is the inner-product kernel function [34].\nThe algorithm is as follows: Step 1: Given a training set, T = {(x1, y1), · · · , (xl, yl)} ∈ (Rn × Y)l, where xi ∈ Rn, yi ∈ Y = R, i = 1, · · · , l. Step 2: Select the appropriate kernel function K(x, x ), the appropriate precision ε > 0 and penalty parameter C > 0. The kernel function effects the transformation from space Rn to Hilbert space Φ : x → Φ(x) , Φ : x → Φ(x ) i.e., it replaces the inner product in the original space, K(x, x ) = (Φ(x) · Φ(x )). The insensitive loss function. c is as given below:\n\nc(x, y, g(x)) =\n\n0,\n\n|y − g(x)| < ε\n\n|y − g(x)| − ε,\n\nothers\n\n(6)\n\nε is a positive number selected in advance and when the difference between the observed value y and predicted value g(x) of ε point does not exceed a given value set in advance, the predicted value g(x) at that point is considered to be lossless, although the predicted value g(x) and the observed value y may not be exactly equal. An image of the insensitive loss function ε is shown in Figure 2.\n\nAlgorithms 2018, 11, x FOR PEER REVIEW\n\n4 of 15\n\n is a positive number selected in advance and when the difference between the observed value y\n\nandpriesdaicptoedsitvivaeluneumg bxer soeflectedpoininatddvoaenscneoatnedxcweehdenatghievedniffvearleunecseebteintwaedevnatnhcee,otbhseerpvredivcateludevaylue\n\nangdpxredaictttehdavt aplouientgis xconosfiderepdotinotbdeoleossnsloetses,xcaeltehdoaughivtehnevparluedeiscetet dinvaadlvuaencge,xthe apnrdedtihcteedobvsaelruveed Alggorvitxahmlusea20ty1tm8h, a11ty,p1n9oo3itnbteisexcaocntslyideeqruedal.toAnbeimloasgseleossf,thalethinosuegnhsitihveeplorsesdfiuctnecdtivoanlue igs sxhowandintFhieguobr4esoe2fr.1v5ed\n\nvalue y may not be exactly equal. An image of the insensitive loss function  is shown in Figure 2.\n\nFigure 1. Structure of the SVR. Figure 1. Structure of the SVR.\nFigure 1. Structure of the SVR.\n\nFiguFreig2u.rIen2se. nInssiteinvseitloivses lfousnscftuionnc.tion.\n\nFigure 2. Insensitive loss function.\nStepS3te: pC3o:nCstornuscttrauncdt asnodlvseoclvoenvceoxnvqeuxadqruaatdicraptriocgprraomgrmaimngming\n\n                  Step\n\n3: Construct mimn in\nα(∗)m∈i*nRR22ll 1\n\n1122ailn,ij∑,=dljl11so*αlvi∗i* e−coαini ve*αx∗j*jq−uaαdjjKraKKtixcxxip,,ixrx,ojxgjr+amεi∑=mll1iiln1αgi∗*+i* αi\n\n− i \n\nl\ni∑=l 1 i\n\nl\nyi\ny1\n\nyαi i∗ \n\n−* i\n\nαi\n\n* \n\n i\n\nα  * i R2il\n\n2 i, j 1\n\ni\n\ni\n\nj\n\nj\n\nij\n\ni\n\ni\n\nii\n\ni\n\ni 1\n\ni 1\n\n   s.t.\n\nl i\n∑s.t.αi∗\n\nl\n−l αi\n\n=i*\n\n0\n\n i\n\n0\n\n   0s≤i.=t.1αi(∗)\n\n i1 *    i0≤1C, ii*= 1i,C· ·, i·\n\n0 , l1,,\n\nl\n\ni\n\n(7)(7) (7)\n\n  0   *  C, i  1,, l\n\nThe\n\ni\nTsholeustoioluntiisongiivsegnivbeynthbey\n\nethxpe reexspsiroenssaio(∗n)\n\n=a *\n\nα1,1α,1∗,1* ,\n\n·· ·,,\n\nαl ,l, αl*∗l  .\n\nT\n.\n\n  SteTphSe4te:spoCl4au:ltcCiuoalnlactiusiolganitviooefnnbo:bfSyebtlhe:ecSteetxlhepecrtecotshsmieopcnoonmaep*notnαejno1t,rα1*∗k,j oofra,(∗)k*ilno,ftl*hae*.o piennthinetoerpveanl i(n0t,eCrv).aIlf α0j,iCs . If\n\nselectedSi,tsetshpeel4ne:cCteadlc,uthlaetnion of j\n\nb : Select the component\nl\n\n j\n\nor\n\n * k\n\nof\n\na *\n\nin the open interval\n\n0,C  . If\n\n is selected, then j\n\nb = yj − ∑ (αi∗ − αi)K xi, xj + ε\n\n(8)\n\ni=1\n\nand if α∗k is selected, then\n\nl\n\nb = yk − ∑ (αi∗ − αi)K(xi, xk) − ε\n\n(9)\n\ni=1\n\nAlgorithms 2018, 11, 193\n\n5 of 15\n\nStep 5: Construct the decision function\n\nl\n\ny = g(x) = ∑ (αi∗ − αi)K(xi, x) + b\n\n(10)\n\ni=1\n\n3.2. Construction of Mixed Kernel Function\nThe assessment of the learning performance and generalization performance of the SVM depends on the selection of the kernel functions. Two types of kernel functions are widely used: (1) the q polynomial kernel function, K(x, x ) = [(x · x ) + 1]q, (q = 1, 2, · · ·) and (2) the Gaussian radial basis kernel function, K(x, x ) = exp − x − x 2/σ2 , (σ > 0) [35].\nThe polynomial kernel function is a global kernel function with strong generalization ability but weak learning ability [36], whereas the Gaussian radial basis kernel function is a local kernel function with strong learning ability but weak generalization ability. It is difﬁcult to obtain good results in regression forecasting [37] by using only a single kernel function. Moreover, there are certain limitations in using the SVM with a single kernel function to predict the non-linear change in the data of the number of allocated containers of the container ship for one voyage.\nA mixed kernel function is a combination of single kernel functions, integrating their advantages while compensating for the drawbacks, to obtain a performance that can not be achieved by a single kernel function. The mixed function proposed in this study is based on a comprehensive consideration of the local and global kernel functions. According to Mercer’s theorem, the convex combination of two Mercer kernel functions is a Mercer kernel function, and thus, the following kernel functions given by Equation (11) are also kernel functions:\n\nK x, x = (1 − ρ) exp − x − x 2/σ2 + ρ x · x + 1 q\n\n(11)\n\nwhere 0 < ρ < 1, and ρ is the weight adjustment factor. In Equation (11), the ﬂexible combination of the radial basis kernel function and polynomial\nkernel function is obtained by adjusting the value of ρ [38]. When ρ > 0.5, the polynomial kernel function is dominant and the mixed function shows strong generalization ability and when ρ < 0.5, the radial basis kernel function is dominant, and the mixed kernel function shows strong learning ability. Therefore, the mixed kernel function SVM exhibits a better overall performance in predicting the number of allocated containers of the container ship for one voyage.\n\n3.3. Parameter Optimization\nThe prediction accuracy of the mixed kernel function SVM is related to the insensitive loss parameter ε, penalty parameter C, polynomial kernel function parameter q, width of the radial basis kernel function σ, and the weight adjustment factor ρ. At present, when the SVM is used for regression ﬁtting prediction, the methods for determining the penalty parameters and kernel parameters mainly include the experimental method [39], grid method [40], ant colony algorithm [41], and particle swarm algorithm [42]. Although the relevant parameters for the experiment can be obtained by a large number of calculations, the efﬁciency is low and the selected parameters do not necessarily measure up to the global optimum. By setting the step size for the data within the parameter range, the grid method sequentially optimizes and compares the results to obtain the optimal parameter values. If the parameter range is large and the set step size is small, the time spent in the optimization process is too long, and the result obtained may be a local optimum. As a general stochastic optimization method, the ant colony algorithm has achieved good results in a series of combinatorial optimization procedures. However, the parameter setting in the algorithm is usually determined by experimental methods resulting in a close interdependence between the optimization performance of the method and human experience, making it difﬁcult to optimize the performance of the algorithm. Due to the loss of diversity of species in search space, the particle swarm algorithm leads to premature convergence and poor local\n\nAlgorithms 2018, 11, 193\n\n6 of 15\n\noptimization ability [43,44]. Therefore, it is of great importance to apply the appropriate optimization\n\nalgorithm for optimal combinatorial results of the parameters of the support vector of the mixed kernel\n\nfunctions to obtain the SVM with the best performance [45], which will ensure an accurate prediction\n\nof the number of allocated containers of the container ship for one voyage.\n\nThe GA [46] is the most widely used successful algorithm in intelligent optimization. It is a\n\ngeneral optimization algorithm with a relatively simple coding technique using genetic operators.\n\nIts optimization is not restrained by restrictive conditions and its two most prominent features are\n\nimplicit parallelism and global solution space search. Therefore, GA is used in this study to optimize\n\nthe parameter combination (ε C q σ ρ) consisting of 5 parameters.\n\nIn the optimization of the SVM parameter combination of the mixed kernel function by using GA,\n\neach chromosome represents a set of parameters and the chromosome species search for the optimal\n\nsolution through the GA (including interlace operation and mutation operation) and strategy selection.\n\nAs the objective of optimizing the SVM parameters of the mixed kernel function is to obtain better\n\nprediction accuracy, the mixed kernel function (εn Cn qn σn ρn) SVM model is trained and then tested\n\nby 5 × cross validation. Proportional selection is the selection strategy adopted in this study. After the\n\nprobability is obtained, the roulette wheel is used to determine the selection operation and hence, the\n\nﬁtness function is deﬁned as the reciprocal of the prediction error of the mixed kernel function SVM as\n\ngiven below:\n\nF= 1\n\n∑ ∑ 1 5\n5 i=1\n\n1 NP m j=1\n\nPˆij − Pij\n\n2\n\n(12)\n\nwhere NP is the number of data in each sample subset, Pˆij is the predicted value, and Pij is the actual value. Thus, the chromosome with the minimum ﬁtness function value in the whole chromosome swarm as well as its index among the chromosome swarm is determined.\nThe step-wise process of optimizing the parameters (ε C q σ ρ) by using GA is given below and the ﬂow diagram is illustrated in Figure 3.\nStep 1: Data preprocessing, mainly including normalization processing and dividing the sample data into training data and test data.\nStep 2: Initialize various parameters of the GA and determine the range of values of the various parameters of the mixed kernel function SVM. First, set the maximum number of generations (gen = 50), population size (NP), individual length, generation gap (GGAP = 0.95), crossover rate (Px = 0.7), and mutation rate (Pm = 0.01). Next, set the range of the parameters (ε C q σ ρ). Since this optimization model (GA) is not the highlight in this paper, the criteria for parameter selection, i.e., gen = 50, GGAP = 0.95, Px = 0.7, Pm = 0.01, are not given here in detail, and the selection of parameters is based on the empirical practice provided in reference [46]. Moreover, the setting of these parameters has achieved good results in this paper.\nStep 3: Encode the chromosomes and generate the initial population. Encode the chromosomes in a 7-bit binary and randomly generate NP individuals (s1, s2, · · · , sNP) to form the initial population S (S = {s1, s2, · · · , sNP}).\nStep 4: Calculate the ﬁtness of each individual. Find the minimum mean squared error (MSE) among the GA swarm.\nStep 5: If the termination condition is satisﬁed, the individual with the greatest ﬁtness in S is the most sought after result which is then decoded to obtain the optimal parameters (ε C q σ ρ). The optimized parameters (ε C q σ ρ) are used to train the SVM model, which generates the prediction result. This marks the end of the algorithm.\nStep 6: Proportional selection is performed by the roulette wheel method, and the selected probability is calculated by using Equation (13). 95% of the individuals are selected from the parent population S to form the progeny population S1 (as GGAP = 0.95). Genetic operations are performed\n\n C q    . The optimized parameters  C q    are used to train the SVM model,\n\nwhich generates the prediction result. This marks the end of the algorithm.\n\nStep 6: Proportional selection is performed by the roulette wheel method, and the selected\n\nprobability is calculated by using Equation (13). 95% of the individuals are selected from the parent\n\nAplogporuithlamtsio20n18,S11, t1o93 form the progeny population\n\nS 1\n\n(as GGAP = 0.95). Genetic operations7 oaf r1e5\n\nperformed on new populations, crossover operations are performed using single tangent points, and\n\nmonuntaetwionpooppuerlaattiioonnss,acrreospseorvfoerrmopederuastiionngsbaarseicpbeirtfovramriaetdiounsionpgersaintigolnest.angent points, and mutation\n\noperations are performed using basic bit variation opeNrPations.\n\n P i\n\n\n\nFi\n\nF NP i\n\nPi = Fi ∑i1Fi\n\n(13) (13)\n\ni=1\n\nStep 7: Subsequent to the genetic manipulation, a new population\n\nS 3\n\nis obtained and the\n\nparamSteetper7s: SubsCequqent to theargeenceatliccumlataendi.puTlhateioSnV, Ma nmewodpeol piuslatthieon Str3ainseodbtwaiinthedthanedntehwe\n\nppaarraammeetteerrss.(ε C q σ ρ) are calculated. The SVM model is then trained with the new parameters.\n\nSStteepp 88::\n\nSS 3 3\n\niiss nnooww ccoonnssiiddeerreedd ttoo bbeetthheenneewwggeenneeraratitoionnppooppuulaltaitoinon, ,i.ei..e, .,SS iiss rreeppllaacceeddbbyy SS33,,\n\nggeenn == ggeenn ++11,,aannddtthheepprroocceessssiissrreeppeeaatteedd ffrroomm sstteepp 44..\n\nFFiigguurree 33.. FFllooww ddiiaaggrraamm ooff GGAA..\n4. Example Analysis\nThis study set X0 = (x0(1), x0(2), · · · , x0(k), · · · , x0(n)) as the reference sequence (i.e., the number of containers for a voyage (the study object)) and Xi = (xi(1), xi(2), · · · , xi(k), · · · , xi(n),)(i = 1, 2, · · · , m) as the comparative sequence (i.e., factors influencing the number of allocated containers for one voyage during the voyage period). Parameter n represents the number of samples and m represents the number of influencing factors; in this study, m = 9. GRA was applied. The weighted influencing factors were then used as the input of the mixed kernel function SVM.\n\nAlgorithms 2018, 11, 193\n\n8 of 15\n\n4.1. Data Samples\nTo establish a model for forecasting the number of containers allocated to a container ship for one voyage, factors inﬂuencing the number of allocated containers must be analyzed and an index system for forecasting the number of allocated containers must be established. Numerous factors inﬂuence the number of containers allocated to a container ship for one voyage; such factors include the port of call, the company (ﬂeet) to which the ship belongs, and the ship itself. A predictive index for forecasting container allocation is outlined as follows (i represents the ith inﬂuencing factor, i = 1, 2, · · · , m):\n(1) X1, local GDP of the region in which the port of call is located, which can be calculated on the basis of the formula actual amount/100 million yuan;\n(2) X2, changes in port industrial structures, which can calculated according to the percentage occupied by the tertiary industry;\n(3) X3, completeness of the collection and distribution system, which can calculated according to the actual annual throughput of containers per million twenty-foot equivalent units (TEU) at the port of call;\n(4) X4, company’s capacity, which can be calculated according to the actual number of containers/10,000 TEU;\n(5) X5, inland turnaround time of containers, which can be calculated according to the actual number of days;\n(6) X6, seasonal changes in cargo volume, which can be calculated as a percentage; (7) X7, quantity of containers handled by the company, which can be calculated according to the\nactual number of containers/10,000 TEU; (8) X8, transport capacity for a single ship, which can be calculated according to the actual number\nof containers/TEU; and (9) X9, full-container-loading rate of the ship, which can be calculated as a percentage.\nFor different shipping lines, ports, and container ships, collecting actual data pertaining to the nine aforementioned factors is difﬁcult. Moreover, information on some of these factors is treated as conﬁdential by company or ship management teams. To verify the practicality of the model, this study simulated a set of data. To ensure that the sample data were reasonable and approached real situations, this study sought information from the literature [8,9], in addition to consulting the department heads of shipping lines and stowage operators.\nThe selected training samples are presented in Table 1.\n\nNo.\n\nX1\n\nX2\n\n1\n\n2395\n\n75\n\n2\n\n27,689\n\n66\n\n3\n\n29,960\n\n77\n\n4\n\n29,841\n\n82\n\n5\n\n13,562\n\n63\n\n6\n\n17,369\n\n59\n\n7\n\n14,650\n\n71\n\n8\n\n30,550\n\n58\n\n9\n\n25,103\n\n54\n\n10\n\n14,650\n\n65\n\n11\n\n14,023\n\n49\n\n12\n\n19,776\n\n67\n\nTable 1. Training samples.\n\nX3\n\nX4\n\nX5\n\nX6\n\nX7\n\nX8\n\nX9\n\nX0\n\n2461\n\n10.3\n\n11\n\n20\n\n21\n\n5200\n\n77\n\n1100\n\n776\n\n85\n\n38\n\n10\n\n170 1700\n\n85\n\n279\n\n2521 102.3 10\n\n15 230 4700\n\n69\n\n1739\n\n4123\n\n162\n\n49\n\n13\n\n201 3410\n\n64\n\n177\n\n2357\n\n68.9\n\n39\n\n20\n\n150 1200\n\n73\n\n110\n\n2037\n\n60.3\n\n22\n\n26 120 800\n\n62\n\n205\n\n1521\n\n59\n\n13\n\n29\n\n147 2800\n\n73\n\n347\n\n798\n\n47.7\n\n26\n\n15\n\n128 3600\n\n88\n\n561\n\n567\n\n110.3\n\n13\n\n10\n\n235 2000\n\n65\n\n850\n\n668\n\n85.6\n\n25\n\n12\n\n164 2590\n\n86\n\n496\n\n732\n\n77.7\n\n16\n\n30\n\n139 2810\n\n59\n\n594\n\n651\n\n56\n\n30\n\n21\n\n98\n\n1400\n\n71\n\n350\n\n4.2. Determining the Weight of Inﬂuencing Factors\nAs indicated by the data in the table, the order of magnitude of the sequences was quite different, and the two sequences were standardized using Equation (1). The correlation between each inﬂuencing\n\nAlgorithms 2018, 11, 193\n\n9 of 15\n\nfactor and the number of containers allocated to the container ship for one voyage was calculated using Equations (2), (3) and(4), and the calculation results are presented in Table 2.\n\nTable 2. Correlation between each inﬂuencing factor and study object.\n\nFactors\nX1 X2 X3\n\nRelevance\n0.1669 0.6672 0.7084\n\nFactors\nX4 X5 X6\n\nRelevance\n0.1773 0.3998 0.6206\n\nFactors\nX7 X8 X9\n\nRelevance\n0.1770 0.8345 0.6232\n\nAs shown in Table 2, the correlation degrees of X1, X4, and X7 were all approximately 0.17, indicating that the three inﬂuencing factors had the lowest effect on the number of allocated containers for one voyage and could be ignored. The correlation degree of X5 was 0.3998, signifying that this factor had little effect on the number of allocated containers for one voyage; the correlation degrees of X2, X6, X3, and X9 were higher than 0.6, indicating that these three factors had a signiﬁcant effect on the number of allocated containers for one voyage. However, the correlation degree of X8 was 0.8345, signifying that this factor had the greatest effect on the number of allocated containers for one voyage. The weight of each inﬂuencing factor was calculated using Equation (5), and Table 3 shows the results.\n\nFactors\nX1 X2 X3\n\nTable 3. Weight of each inﬂuencing factor.\n\nWeight\n0.038 0.153 0.162\n\nFactors\nX4 X5 X6\n\nWeight\n0.041 0.091 0.142\n\nFactors\nX7 X8 X9\n\nWeight\n0.040 0.191 0.142\n\nAs shown in Table 3, the weight values of X1, X4, and X7 were relatively low (all lower than 0.091), and the weight values of the other inﬂuencing factors were higher than 0.14, with no signiﬁcant difference. This is mainly because the other factors had greater effects on the number of allocated containers for one voyage, and their weight values were scattered.\n\n4.3. Prediction of Number of Allocated Containers for One Voyage Using Mixed Kernel SVM\n\nWeighted factors could be derived by multiplying the inﬂuencing factors by the\n\ncorresponding weights:\n\nXi = wiXi, i = 1, 2, · · · , 9\n\n(14)\n\nwhere Xi is the weighted factor inﬂuencing the number of allocated containers for one voyage. When Xi in the composition vector Q = X1, X2, · · · , X9 T was considered the input variable and X0 was considered the corresponding output variable, a mixed kernel SVM for predicting the number of allocated containers for one voyage was constructed.\nAll data were normalized to the interval [0, 1]. The data presented in Table 1 served as training samples, whereas those presented in Table 4 served as test samples.\n\nTable 4. Test samples.\n\nNo.\n\nX1\n\nX2\n\nX3\n\nX4\n\nX5\n\nX6\n\nX7\n\nX8\n\nX9\n\nX0\n\n1\n\n4365\n\n76\n\n1596\n\n24\n\n13\n\n18\n\n43\n\n5400\n\n72\n\n1250\n\n2 23,560 69\n\n882\n\n87\n\n29\n\n12\n\n185 1900\n\n83\n\n900\n\n3\n\n9841\n\n81\n\n2143 112 12\n\n17\n\n251 4580\n\n70\n\n750\n\n4 25,590 74 4265 159 50\n\n14\n\n211 3390\n\n66\n\n500\n\n5\n\n18,763 62\n\n1983\n\n73\n\n41\n\n21\n\n163 1080\n\n74\n\n310\n\nAlgorithms 2018, 11, 193\n\n10 of 15\n\nThe GA control parameters were as follows: ε, C, q, σ, ρ were binary coded, with the optimal ranges being set to ε ∈ 10−10, 10−1 , C ∈ [0, 50], q ∈ [0, 20], σ ∈ [0, 500], and ρ ∈ [0, 1], respectively.\n\nThe series size was 50, maximum evolution algebra was 50 generations, crossover probability was 0.7, mutation probability was 0.01, and the judgment termination accuracy was 10−4.\n\nThis study applied MSE, mean absolute percentage error (eMAPE), and correlation coefﬁcients\n\n(R) to evaluate the predictive performance of the model. R was set to the interval [0, 1].\n\nLower MSE and eMRE values and R values approaching 1 were considered to indicate higher model\n\npredictive performance.\n\nMSE\n\n=\n\n1 l\n\nl\n∑ (yi\ni=1\n\n− yi)2\n\nR=\n\neMAPE\n\n=\n\n1 l\n\nl\n∑\n\ni=1\n\nyi −yi yi\n\n· 100%\n\nl\n\nl\n\nl\n\n2\n\nl ∑ yiyi− ∑ yi ∑ yi\n\ni=1\n\ni=1 i=1\n\nl\n\nl\n∑\n\ny2i −\n\nl\n\n2\n\n∑ yi\n\ni=1\n\ni=1\n\nl\n\nl\n\n2\n\nl\n\n∑\ni=1\n\ny2i\n\n−\n\n∑ yi\ni=1\n\n(15)\n\nwhere l is the number of samples, yi(i = 1, 2, · · · l) is the real value of the ith sample, and yi(i = 1, 2, · · · l) is the predicted value of the ith sample.\n\n4.4. Simulation Results and Analysis\n\nTAlhgoeritphmasra20m18e, 1t1e,rxsFOεˆRCPˆEqEˆRσˆRρEˆVIoEWf the mixed kernel SVM were obtained through GA opt1i1mofi1z5ation, which was used to establish the mixed kernel SVM model and predict the number of voyage containers\nvoyage containers in the test samples. The various input variables affected the predictive in thepeterfsotrsmamanpceleosf. tThheemvoadreilo,uasndintphue tspveacriifaicbrleessualftsfeacrteedprtehseenpterdedinicTtiavbelep5erafnodrmFiagnucree 4o.f the model, and the speciﬁc results are presented in Table 5 and Figure 4.\n\nNo. Actual No. Actual\n\n11 22 33\n44\n55\n\n12152050 909000 757050\n505000\n313010\n\nMSMESE\neMAePMEAPE RR\n\nTable 5. Comparison of predictions for different input variables.\n\nTable 5. Comparison of predictions for different input variables.\n\nSVM-Mixed\n\nGRA-SVM-Mixed\n\nGRA-SVM-Mixed-D\n\nSVM-MRixeeldative\n\nGRA-SVMRe-Mlatiixveed\n\nGRA-SVRMe-lMatiixveed-D\n\nPredictive\n\nPredictive\n\nPredictive\n\nPredictive RelaEtrirvoerError Predictive ReElrartoivre Error Predictive REerrloatrive Error\n\n11440066 887700\n\n1122.4.488 −−3.33.333\n\n12162363 898999\n\n1.014.04 −0−.101.11\n\n12614264 781781\n\n1.12 1.12 −13.−2213.22\n\n880077\n\n77.6.600\n\n767969\n\n2.523.53\n\n757757\n\n0.93 0.93\n\n526\n\n55.2.200\n\n474979\n\n−4−.240.20\n\n484484\n\n−3.2−0 3.20\n\n328\n\n55.8.811\n\n313414\n\n1.219.29\n\n325325\n\n4.84 4.84\n\n55889977 66.8.888 00.9.9990088\n\n1971.967.6 1.813.83 0.909.999393\n\n29772.9477.4 4.664.66 0.9870.79877\n\nNUMBER OF CONTAINERS\n\n1500 1300 1100 900\n\nActual Value\nSVM-Mixed\nGRA-SVMMixed\n\n700\n\n500\n\n300\n\n1\n\n2\n\n3\n\n4\n\n5\n\nSAMPLE NUMBER\n\nFigure 4. Comparison of values for different input variables. Figure 4. Comparison of values for different input variables.\n\nThe calculation results revealed that under the same model parameters, changing the input variables engendered different predictive performance levels. The input variables of the GRA-SVMMIXED model were constituted by all the weighted influencing factors (see Table 3 for weight values); the input variables of the SVM-Mixed model were constituted by all the unweighted factors. For the GRA-SVM-Mixed-D model, influencing factors with correlation degrees lower than 0.6 were\n\nAlgorithms 2018, 11, 193\n\n11 of 15\n\nThe calculation results revealed that under the same model parameters, changing the input variables engendered different predictive performance levels. The input variables of the GRA-SVM-MIXED model were constituted by all the weighted inﬂuencing factors (see Table 3 for weight values); the input variables of the SVM-Mixed model were constituted by all the unweighted factors. For the GRA-SVM-Mixed-D model, inﬂuencing factors with correlation degrees lower than 0.6 were eliminated, and the remaining inﬂuencing factors were considered the model input variables. As presented in Table 5 and Figure 4, the maximum (minimum) error, MSE, and eMRE of the GRA-SVM-Mixed model were signiﬁcantly lower than those of the GRA-SVM-mixed-D and SVM-mixed models; in addition, the GRA-SVM-Mixed model had the highest correlation coefﬁcient R, indicating that the GRA-SVM-mixed model exhibited higher predictive performance than did the other two models. As illustrated in Figures 4 and 5, the GRA-SVM-Mixed model provided closer predictions to the actual values in the test sample than did the other two models, and no large inﬂection point was observed. Furthermore, the maximum relative error observed for the GRA-SVM-Mixed model was −4.2%, minimum error was −0.11%, and correlation coefﬁcient was as high as 0.9993, showing higher predictive performance. This is because after the inﬂuencing factors were subjected to gray correlation analysis, different weights were assigned to the input variables, the intrinsic correlation characteristics between the inﬂuencing factors and the number of allocated containers for one voyage were fully explored, and the inﬂuencing factors with low correlation degree were eliminated. The maximum rAelglaortiitvhmese2r0r1o8,r1o1,bxsFeOrvRePdEEfRorREthVeIEWGRA-SVM-Mixed-D model was −13.22%, minimum relative12erorfo1r5 was 0.93%, and correlation coefﬁcient was 0.9877, which was the smallest among the three models, amnoddtehlsis, acnoduldthibsecaotutrlidbubteedatttroibthueteedlitmoitnhaetieolnimoifniantﬂiounenocfiinngflfuaecntocirnsgwfiatchtolorswwciothrrelolawtiocnordreelgarteioens. Adeltghroeuesg.hAelltihmoiungahtinegliminﬂinuaetnincginignfflaucetonrcsinwgitfhaclotowrscworirtehlaltoiowncdoergrerelaetsiocnoudldegsriemeps lcifoyutlhdessimtrupcltifuyrethoef tshtreupcrteudreictoiof nthmeopdreel,dtihcetiopnredmicotdiveel,ptehrefoprmreadniccetivoef thpeermfoordmeal nwcaes oreflathtievemlyopdoeol rwbaecsaruesleatiitvceoluyldponoort rbeeﬂceacutstehiet dcoifufeldrennocet sreafmleoctntghtehdeiffafecrteonrsc.es among the factors.\n\nRELATIVE ERROR/%\n\n11\n\n6\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n-4\n\n-9\n\n-14 SAMPLE NUMBER\n\nSVM-Mixed\n\nGRA-SVM-Mixed\n\nGRA-SVM-Mixed-D\n\nFigure 5. Relative prediction error for different input variables. Figure 5. Relative prediction error for different input variables.\nOn the basis of the same sample in this study, the methods in [8,9] were used to construct models for pOrendtihcteinbgastihs eofntuhemsbaemreosfaamllpolceaitnedthicsosntutadiny,etrhsefmoreothnoedvsoinya[8g,9e], wwehriechuswedertoe cdoennsotrtuedct amsoBdPelsanfodr SpVreMdi,crtiensgpetchteivneulym. Abesriloluf satlrlaotceadteidn Fcoignutarien6e,rsthfeorGoRnAe-SvVoyMag-Me,ixwehdicmhowdeelrehaddenmooterde satsabBlPe parneddiScVtiMon, rreessupeltcstiavnedlym. AosreilaluccsturraatetedpinreFdiigcutiroen6s,ththaenGdRidAt-hSeVBMP-ManixdeSdVmModmeol dhealds.mTohreetestsatbinledpicraetdoircstiionnTraebsulelt6s faunrdthmerorceoancﬁcrumrattehepsreedﬁinctdioinngs st.haAnsdpidretsheenBtePdainndTSaVblMe 6m, oadlletlsh.eTthhereteestminoddieclastocrosuilnd Tparbolvei6defugrothoedr pcorendfiircmtiothnesreesfuinldtsinagnsd. Asastpisrfeascetnotreyd Min STEabalend6, eaMllAthPeE tvharelueems.odTehles cGoRulAd-pSrVoMvid-Me gixoeodd mproeddiecltieoxnhriebsiuteltds hanigdhesratpisrfeadctiocrtiyveMpSeErfoarnmdaneMcAeP.E Mvaolrueeosv. eTr,hteheGRGAR-ASV-SMV-MM-iMxeidxedmomdoedl eelxwhiabsiteddetherigmhienredprteodihcativvee\nspiegrnfoiﬁrmcaanntcaed. vMaonrteaogveesro, vthere tGheRAot-hSeVrMtw-Mo imxeoddemlsofdreolmwaastimdeetsearmviinngedpetrospheacvteivsei.gnificant advantages\nover the other two models from a timesaving perspective.\n\nLATIVE ERROR/%\n\n6\n\n-4\n\n1\n\n2\n\n3\n\n4\n\n5\n\n-14 SAMPLE NUMBER\n\nand more accurate predictions than did the BP and SVM models. The test indicators in Table 6 further\n\nconfirm these findings. As presented in Table 6, all the three models could provide good prediction results\n\nand satisfactory MSE and\n\ne MAPE\n\nvalues. The GRA-SVM-Mixed model exhibited higher predictive\n\nperformance. Moreover, the GRA-SVM-Mixed model was determined to have significant advantages\n\noAvlgeorritthhmeso2t0h1e8,r1t1w, 1o93models from a timesaving perspective.\n\n12 of 15\n\nRELATIVE ERROR/%\n\n6\n\n-4\n\n1\n\n2\n\n3\n\n4\n\n5\n\n-14 SAMPLE NUMBER\n\nBP\n\nGRA-SVM-Mixed\n\nSVM\n\nFigure 6. Comparison of values of different models. Figure 6. Comparison of values of different models.\n\nTable 6. Comparison of predictive performance of different models.\n\nTable 6. Comparison of predictive performance of different models.\n\nNo.\nNo. BP\n\nRelative Error\nRSelVaMtive ErroGrRA-SVM-Mixed\n\n1\n\n−B8.P88 SVM3.12 GRA-SVM-M1.0ix4ed\n\n2 1 −28.2.828 3.124.56\n\n1.04 −0.11\n\n3 2 −21.232.6 4.5−6 2.80\n\n−0.11 2.53\n\n43 54\nMES\nR5\n\n−−133..66 −4.38.46\n4734.8\n04.9.88843\n\n−2.890.80 9.8−0 0.97\n1210.6\n−0.90.79969\n\n2.53 −4.20 −4.20 1.29\n197.6\n1.29 0.9993\n\nt/sMES 45773.643.8 12104.56.61\n\n197.6 27.53\n\nR 0.9883 0.9969\n\n0.9993\n\nThe maximum relative errt/osrs of 5th7e.6p3redi4c5ti.o6n1s of the thr2e7e.5m3odels were −13.6%, 2.53%, and 9.8%,\n\nand the minimum relative errors were −2.22%, −0.11%, and −0.97%. The predictive performance of\n\na model can be expressed by the MSE and correlation coefﬁcient R. As shown in Table 6, the MSE\n\nof the GRA-SVM-Mixed model was 197.6 and the R was 0.9993, which was closer to 1 compared\n\nwith those of the other two models. This is because under small samples, the BP neural network\n\nmodel adopts empirical risk minimization, whereas the minimum expected risk cannot be guaranteed.\n\nMoreover, the BP neural network model can only guarantee convergence to a certain point in the\n\noptimization process and cannot derive a global optimal solution. By contrast, the SVM model adopts\n\nstructural risk minimization and VC dimension theory, which not only minimizes the structural risk\n\nbut also minimizes the boundary of the VC dimension under a small sample, effectively narrowing\n\nthe conﬁdence interval, thus achieving the minimum expected risk and improving the generalization\n\nability and promotion ability of the model. The GRA-SVM-Mixed model applies parameter ρ to adjust\n\nthe ﬂexible use of radial basis and polynomial kernel functions in order to improve its robustness\n\nand generalization ability. In addition, the model applies gray correlation analysis for weighting\n\ninput variables, strengthening the internal feature space structure and reﬂecting the differences among\n\ninﬂuencing factors. In this study, this model exhibited good performance in predicting the number of\n\ncontainers allocated to a container ship for one voyage.\n\n5. Conclusions\nThis paper proposes a model for predicting the number of containers allocated to a container ship for one voyage. First, GRA theory is applied to determine the correlation between inﬂuencing factors and the forecasting sequence. Subsequently, different weights are allocated to each inﬂuencing factor to reﬂect their differences and highlight their internal characteristics. The weighted inﬂuencing factors serve as the input variables of the SVM prediction model, and a radial basis kernel function and polynomial kernel function are applied to improve the generalization ability and promotion ability of the SVM model. Finally, a GA is used to optimize the SVM parameters, and samples are trained using the optimized parameters to improve the predictive performance of the model. Simulations revealed that compared with an SVM model with a single kernel function and without gray correlation\n\nAlgorithms 2018, 11, 193\n\n13 of 15\n\nprocessing, the proposed model exhibited higher performance, with the minimum relative error rates being −0.11% and −0.97%, respectively. Additionally, compared with a BP neural network model, the GRA-SVM-Mixed model exhibited superior generalization ability, according to a relative error analysis. Accordingly, the proposed model provides an effective method for predicting the number of containers allocated to a container ship for one voyage.\nAuthor Contributions: The idea for this research work is proposed by Y.W. the MATLAB code is achieved by G.S., and the paper writing and the data analyzing are completed by X.S.\nFunding: This research was funded by the National Natural Science Foundation of China, grant number 51579025; the Natural Science Foundation of Liaoning Province, grant number 20170540090; and the Fundamental Research Funds for the Central Universities, grant number 3132018306.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Helo, P.; Paukku, H.; Sairanen, T. Containership cargo proﬁles, cargo systems, and stowage capacity: Key performance indicators. Marit. Econ. Logist. 2018, 1–21. [CrossRef]\n2. Gosasang, V.; Chandraprakaikul, W.; Kiattisin, S. An application of neural networks for forecasting container throughput at Bangkok port. In Proceedings of the World Congress on Engineering, London, UK, 30 June–2 July 2010; pp. 137–141.\n3. Iris, Ç.; Pacino, D.; Ropke, S.; Larsen, A. Integrated berth allocation and quay crane assignment problem: Set partitioning models and computational results. Transp. Res. Part E Logist. Transp. Rev. 2015, 81, 75–97. [CrossRef]\n4. Jiang, J.; Wang, H.; Yang, Z. Econometric analysis based on the throughput of container and its main inﬂuential factors. J. Dalian Marit. Univ. 2007, 1. [CrossRef]\n5. Chou, C. Analysis of container throughput of major ports in Far Eastern region. Marit. Res. J. 2002, 12, 59–71. [CrossRef]\n6. Meersman, H.; Steenssens, C.; Van de Voorde, E. Container Throughput, Port Capacity and Investment; SESO Working Papers 1997020; University of Antwerp, Faculty of Applied Economics: Antwerpen, Belgium, 1997.\n7. Karsten, C.V.; Ropke, S.; Pisinger, D. Simultaneous optimization of container ship sailing speed and container routing with transit time restrictions. Transp. Sci. 2018, 52, 769–787. [CrossRef]\n8. Weiying, Z.; Yan, L.; Zhuoshang, J. A Forecast Model of the Number of Containers for Containership Voyage Based on SVM. Shipbuild. China 2006, 47, 101–107.\n9. Li, S. A forecast method of safety container possession based on neural network. Navig. China 2002, 3, 56–60. 10. Grudnitski, G.; Osburn, L. Forecasting S&P and gold futures prices: An application of neural networks.\nJ. Futures Mark. 1993, 13, 631–643. [CrossRef] 11. White, H. Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary\nmappings. Neural Netw. 1990, 3, 535–549. [CrossRef] 12. Chakraborty, K.; Mehrotra, K.; Mohan, C.K.; Ranka, S. Forecasting the behavior of multivariate time series\nusing neural networks. Neural Netw. 1992, 5, 961–970. [CrossRef] 13. Lapedes, A.; Farber, R. Nonlinear signal processing using neural networks: Prediction and system modelling.\nIn Proceedings of the IEEE International Conference on Neural Networks, San Diego, CA, USA, 21 June 1987. 14. Yu, J.; Tang, G.; Song, X.; Yu, X.; Qi, Y.; Li, D.; Zhang, Y. Ship arrival prediction and its value on daily\ncontainer terminal operation. Ocean Eng. 2018, 157, 73–86. [CrossRef] 15. Hsu, C.; Huang, Y.; Wong, K.I. A Grey hybrid model with industry share for the forecasting of cargo volumes\nand dynamic industrial changes. Transp. Lett. 2018, 1–12. [CrossRef] 16. Vapnik, V.N. An overview of statistical learning theory. IEEE Trans. Neural Netw. 1999, 10, 988–999. [CrossRef]\n[PubMed] 17. Cortes, C.; Vapnik, V. Support-vector networks. Mach. Learn. 1995, 20, 273–297. [CrossRef] 18. Müller, K.R.; Smola, A.J.; Rätsch, G.; Schölkopf, B.; Kohlmorgen, J.; Vapnik, V. Predicting time series\nwith support vector machines. In Proceedings of the Artiﬁcial Neural Networks—ICANN’97, Lausanne, Switzerland, 8–10 October 1997; Springer: Berlin/Heidelberg, Germany, 1997; pp. 999–1004. [CrossRef] 19. Gunn, S.R. Support vector machines for classiﬁcation and regression. ISIS Tech. Rep. 1998, 14, 5–16.\n\nAlgorithms 2018, 11, 193\n\n14 of 15\n\n20. Joachims, T. Making Large-Scale SVM Learning Practical. Technical Report, SFB 475. 1998, Volume 28. Available online: http://hdl.handle.net/10419/77178 (accessed on 15 June 1998).\n21. Smits, G.F.; Jordaan, E.M. Improved SVM regression using mixtures of kernels. In Proceedings of the 2002 International Joint Conference on Neural Networks, Honolulu, HI, USA, 12–17 May 2002; pp. 2785–2790. [CrossRef]\n22. Jebara, T. Multi-task feature and kernel selection for SVMs. In Proceedings of the Twenty-First International Conference on Machine Learning, Banff, AB, Canada, 4–8 July 2004; pp. 329–337. [CrossRef]\n23. Tsang, I.W.; Kwok, J.T.; Cheung, P. Core vector machines: Fast SVM training on very large data sets. J. Mach. Learn. Res. 2005, 6, 363–392.\n24. Lu, Y.L.; Lei, L.I.; Zhou, M.M.; Tian, G.L. A new fuzzy support vector machine based on mixed kernel function. In Proceedings of the 2009 International Conference on Machine Learning and Cybernetics, Baoding, China, 12–15 July 2009; pp. 526–531. [CrossRef]\n25. Xie, G.; Wang, S.; Zhao, Y.; Lai, K.K. Hybrid approaches based on LSSVR model for container throughput forecasting: A comparative study. Appl. Soft Comput. 2013, 13, 2232–2241. [CrossRef]\n26. Deng, J.L. Introduction to Grey System Theory. J. Grey Syst. 1989, 1, 1–24. 27. Al-Douri, Y.; Hamodi, H.; Lundberg, J. Time Series Forecasting Using a Two-Level Multi-Objective Genetic\nAlgorithm: A Case Study of Maintenance Cost Data for Tunnel Fans. Algorithms 2018, 11, 123. [CrossRef] 28. Weiwei, W. Time series prediction based on SVM and GA. In Proceedings of the 2007 8th International\nConference on Electronic Measurement and Instruments, Xi’an, China, 16–18 August 2007; pp. 307–310. [CrossRef] 29. Yin, Y.; Cui, H.; Hong, M.; Zhao, D. Prediction of the vertical vibration of ship hull based on grey relational analysis and SVM method. J. Mar. Sci. Technol. 2015, 20, 467–474. [CrossRef] 30. Kuo, Y.; Yang, T.; Huang, G. The use of grey relational analysis in solving multiple attribute decision-making problems. Comput. Ind. Eng. 2008, 55, 80–93. [CrossRef] 31. Tosun, N. Determination of optimum parameters for multi-performance characteristics in drilling by using grey relational analysis. Int. J. Adv. Manuf. Technol. 2006, 28, 450–455. [CrossRef] 32. Shen, M.X.; Xue, X.F.; Zhang, X.S. Determination of Discrimination Coefﬁcient in Grey Incidence Analysis. J. Air Force Eng. Univ. 2003, 4, 68–70. 33. Yao, X.; Zhang, L.; Cheng, M.; Luan, J.; Pang, F. Prediction of noise in a ship’s superstructure cabins based on SVM method. J. Vib. Shock 2009, 7. [CrossRef] 34. Huang, C.; Chen, M.; Wang, C. Credit scoring with a data mining approach based on support vector machines. Expert Syst. Appl. 2007, 33, 847–856. [CrossRef] 35. Duan, K.; Keerthi, S.S.; Poo, A.N. Evaluation of simple performance measures for tuning SVM hyperparameters. Neurocomputing 2003, 51, 41–59. [CrossRef] 36. Van der Schaar, M.; Delory, E.; André, M. Classiﬁcation of sperm whale clicks (Physeter Macrocephalus) with Gaussian-Kernel-based networks. Algorithms 2009, 2, 1232–1247. [CrossRef] 37. Luo, W.; Cong, H. Control for Ship Course-Keeping Using Optimized Support Vector Machines. Algorithms 2016, 9, 52. [CrossRef] 38. Wei, Y.; Yue, Y. Research on Fault Diagnosis of a Marine Fuel System Based on the SaDE-ELM Algorithm. Algorithms 2018, 11, 82. [CrossRef] 39. Bian, Y.; Yang, M.; Fan, X.; Liu, Y. A Fire Detection Algorithm Based on Tchebichef Moment Invariants and PSO-SVM. Algorithms 2018, 11, 79. [CrossRef] 40. Cherkassky, V.; Ma, Y. Practical selection of SVM parameters and noise estimation for SVM regression. Neural Netw. 2004, 17, 113–126. [CrossRef] 41. Aiqin, H.; Yong, W. Pressure model of control valve based on LS-SVM with the fruit ﬂy algorithm. Algorithms 2014, 7, 363–375. [CrossRef] 42. Du, J.; Liu, Y.; Yu, Y.; Yan, W. A prediction of precipitation data based on support vector machine and particle swarm optimization (PSO-SVM) algorithms. Algorithms 2017, 10, 57. [CrossRef] 43. Wang, R.; Tan, C.; Xu, J.; Wang, Z.; Jin, J.; Man, Y. Pressure Control for a Hydraulic Cylinder Based on a Self-Tuning PID Controller Optimized by a Hybrid Optimization Algorithm. Algorithms 2017, 10, 19. [CrossRef] 44. Liu, D.; Niu, D.; Wang, H.; Fan, L. Short-term wind speed forecasting using wavelet transform and support vector machines optimized by genetic algorithm. Renew. Energy 2014, 62, 592–597. [CrossRef]\n\nAlgorithms 2018, 11, 193\n\n15 of 15\n\n45. Shevade, S.K.; Keerthi, S.S.; Bhattacharyya, C.; Murthy, K.R.K. Improvements to the SMO algorithm for SVM regression. IEEE Trans. Neural Netw. 2000, 11, 1188–1193. [CrossRef] [PubMed]\n46. Holland, J.H. Adaptation in Natural and Artiﬁcial Systems: An Introductory Analysis with Applications to Biology, Control, and Artiﬁcial Intelligence; MIT Press: Cambridge, MA, USA, 1992.\n© 2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n\n"}