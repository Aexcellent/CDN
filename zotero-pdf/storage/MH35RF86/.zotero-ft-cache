arXiv:1909.07972v1 [cs.NI] 17 Sep 2019

1
A Joint Learning and Communications Framework
for Federated Learning over Wireless Networks
Mingzhe Chen, Zhaohui Yang, Member, IEEE, Walid Saad, Fellow, IEEE, Changchuan Yin, Senior Member, IEEE, H. Vincent Poor, Fellow, IEEE, and
Shuguang Cui, Fellow, IEEE
Abstract
In this paper, the problem of training federated learning (FL) algorithms over a realistic wireless network is studied. In particular, in the considered model, wireless users execute an FL algorithm while training their local FL models using their own data and transmitting the trained local FL models to a base station (BS) that will generate a global FL model and send it back to the users. Since all training parameters are transmitted over wireless links, the quality of the training will be affected by wireless factors such as packet errors and the availability of wireless resources. Meanwhile, due to the limited wireless bandwidth, the BS must select an appropriate subset of users to execute the FL algorithm so as to build a global FL model accurately. This joint learning, wireless resource allocation, and user selection problem is formulated as an optimization problem whose goal is to minimize an FL loss function that captures the performance of the FL algorithm. To address this problem, a closed-form expression for the expected convergence rate of the FL algorithm is ﬁrst derived to quantify the impact of wireless factors on FL. Then, based on the expected convergence rate of the FL algorithm, the optimal transmit
M. Chen is with the Chinese University of Hong Kong, Shenzhen, 518172, China, and also with the Department of Electrical Engineering, Princeton University, Princeton, NJ, 08544, USA, Email: mingzhec@princeton.edu.
Z. Yang is with the Centre for Telecommunications Research, Department of Informatics, King’s College London, WC2B 4BG, UK, Email: yang.zhaohui@kcl.ac.uk.
W. Saad is with the Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, 24060, USA, Email: walids@vt.edu.
C. Yin is with the Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, 100876, China, Emails: ccyin@ieee.org.
H. V. Poor is with the Department of Electrical Engineering, Princeton University, Princeton, NJ, 08544, USA, Email: poor@princeton.edu.
S. Cui is with the Shenzhen Research Institute of Big Data and School of Science and Engineering, the Chinese University of Hong Kong, Shenzhen, 518172, China, Email: robert.cui@gmail.com
This work was supported in part by the U.S. National Science Foundation under Grants CNS-1836802 and CCF-0939370.

2
power for each user is derived, under a given user selection and uplink resource block (RB) allocation scheme. Finally, the user selection and uplink RB allocation is optimized so as to minimize the FL loss function. Simulation results show that the proposed joint federated learning and communication framework can reduce the FL loss function value by up to 10% and 16%, respectively, compared to: 1) An optimal user selection algorithm with random resource allocation and 2) a standard FL algorithm with random user selection and resource allocation. Index Terms— Federated learning; wireless resource allocation; user selection.
I. INTRODUCTION Standard machine learning approaches require centralizing the training data on one machine or in a data center [2]–[4]. However, due to privacy and limited communication resources for data transmission, it is impractical for all users engaged in learning to transmit all of their collected data to a data center or a cloud. This, in turn, motivates the development of distributed learning frameworks that allow devices to use individually collected data to train a learning model locally. One of the most promising of such distributed learning frameworks is the so-called federated learning (FL) algorithm developed in [5]. FL is a distributed machine learning algorithm that enables users to collaboratively learn a shared prediction model while keeping their collected data on their devices [6]–[10]. However, to train an FL algorithm in a distributed manner, the users must transmit the training parameters over wireless links which can introduce training errors, due to the limited wireless resources (e.g., bandwidth) and the inherent unreliability of wireless links.
A. Related Works Recently, a number of existing works such as in [5], [11]–[21] have studied important problems
related to the implementation of FL over wireless networks. The works in [5] and [11] provided a comprehensive survey on the design of FL algorithms and introduced various challenges, problems, and solutions for enhancing FL effectiveness. In [12], the authors developed two update methods to reduce the uplink communication costs for FL. The work in [13] presented a practical update method for a deep FL algorithm and conducted an extensive empirical evaluation for ﬁve different FL models using four datasets. An echo state network-based FL algorithm is developed
A preliminary version of this work [1] appears in the proceedings of IEEE GLOBECOM.

3
in [14] to analyze and predict the location and orientation for wireless virtual reality users. In [15], the authors proposed a novel FL algorithm that can minimize the communication cost. The authors in [16] studied the problem of joint power and resource allocation for ultra-reliable low latency communication in vehicular networks. The work in [17] developed a new approach to minimize the computing and transmission delay for FL algorithms. In [18], the authors used FL algorithms for trafﬁc estimation so as to maximize the data rates of users. While interesting, these prior works [5] and [11]–[18] assumed that wireless networks can readily integrate FL algorithms. However, in practice, due to the unreliability of the wireless channels and to the wireless resource limitations (e.g., in terms of bandwidth and power), FL algorithms will encounter training errors due to the wireless links [19]. For example, symbol errors introduced by the unreliable nature of the wireless channel and by resource limitations can impact the quality and correctness of the FL updates among users. Such errors will, in turn, affect the performance of FL algorithms, as well as their convergence speed. Moreover, due to the wireless bandwidth limitations, the number of users that can perform FL is limited; a design issue that is ignored in [5] and [11]– [18]. Furthermore, due to limited energy consumption of each user’s device and strict delay requirement of FL, not all wireless users can perform FL algorithms. Therefore, one must select the appropriate users to perform FL algorithms and optimize the performance of FL. In practice, to effectively deploy FL over real-world wireless networks, it is necessary to investigate how the wireless factors affect the performance of FL algorithms. Here, we note that, although some works such as [7] and [19]–[21] have studied communication aspects of FL, these works are limited in several ways. First, the works in [7] and [19] only provide a high-level exposition of the challenges of communication in FL. Meanwhile, the authors in [20] and [21] do not consider the effect of packet transmission errors on the performance of FL. Moreover, none of these prior works provided a comprehensive design and optimization of the joint wireless and FL performance.
B. Contributions The main contribution of this paper is, thus, a novel framework for enabling the implemen-
tation of FL algorithms over wireless networks by jointly taking into account FL and wireless metrics and factors. To our best knowledge, this is the ﬁrst work that provides a fundamental connection between the performance of FL algorithms and the underlying wireless network. Our

4
key contributions include: • We propose a novel FL model in which cellular-connected wireless users transmit their locally trained FL models to a base station (BS) that generates the global FL model and transmits it back to the users. For the considered FL model, the bandwidth for uplink transmission is limited and, hence, the BS needs to select appropriate users to execute the FL algorithm so as to minimize the FL loss function. In addition, the impact of the wireless packet transmission errors on the parameter update process of the FL model is explicitly considered. • In the developed joint communication and FL model, the BS must optimize its resource allocation and the users must optimize their transmit power allocation so as to decrease the packet error rates of each user thus improving the performance of federated learning. To this end, we formulate this joint resource allocation and user selection problem for FL as an optimization problem whose goal is to minimize the value of the FL loss function while meeting the delay and energy consumption requirements. Hence, our framework jointly considers learning and wireless networking metrics. • To solve this problem, we ﬁrst derive a closed-form expression for the expected convergence rate of the FL algorithm so as to build an explicit relationship between the packet error rates and the performance of the FL algorithm. Based on this relationship, the optimization problem can be simpliﬁed as an mixed-integer nonlinear programming problem. To solve this simpliﬁed problem, we ﬁrst ﬁnd the optimal transmit power under given user selection and resource block (RB) allocation. Then, we transform the original optimization problem into a bipartite matching problem that is solved using a Hungarian algorithm which ﬁnds the optimal, FL-aware user selection and RB allocation strategy. • To further reduce the effect of the packet transmission errors on the performance and convergence speed of FL, we perform fundamental analysis on the expression of expected convergence rate of FL algorithms, which shows that, the transmit power, RB allocation, and user selection will signiﬁcantly affect the convergence speed and performance of FL algorithms. Meanwhile, by appropriately setting the learning rate and selecting the number of users that perform FL algorithms, the effect of the transmission errors on FL algorithm can be reduced and the convergence of FL can be guaranteed.

5
$CUGUVCVKQP Global FL model

Local FL model

Data User 1

User 2

User U

Fig. 1. The architecture of an FL algorithm that is being executed over a wireless network with multiple devices and a single base station.

Simulation results show that the transmit power, RB allocation, the number of users will jointly affect the performance of FL over wireless networks. In particular, the simulation result shows that the proposed FL algorithm that considers the wireless factors can achieve up to 10% and 16% reduction in the FL loss function compared, respectively, to an optimal user selection algorithm with random resource allocation and a standard FL algorithm (e.g., such as in [12]) FL with random user selection and resource allocation.
The rest of this paper is organized as follows. The system model and problem formulation are described in Section II. The expected convergence rate of FL algorithms is studied in Section III. The optimal resource allocation and user selection are determined in Section IV. Simulation results are analyzed in Section V. Conclusions are drawn in Section VI.
II. SYSTEM MODEL AND PROBLEM FORMULATION Consider a cellular network in which one BS and a set U of U users cooperatively perform an FL algorithm for data analysis and inference. For example, the network can execute an

6
The BS transmits the global FL model to the users
User updates the local FL model by its collected data
Each user transmits the local FL model to the BS
The BS updates the global FL model

No

Converge?

Fig. 2. The learning procedure of an FL algorithm.
FL algorithm to sense the wireless environment and generate a holistic radio environment mapping [22]. The use of FL for such applications is important because the data related to the wireless environment is distributed across the network [9] and the BS cannot collect all of this scattered data to implement a centralized learning algorithm. FL enables the BS and the users to collaboratively learn a shared learning model while keeping all of the training data at the device of each user. In an FL algorithm, each user will use its collected training data to train an FL model. For example, for radio environment mapping, each user will collect the data related to the wireless environment for training an FL model. Hereinafter, the FL model that is trained at the device of each user (using the data collected by the user itself) is called the local FL model. The BS is used to integrate the local FL models and generate a shared FL model. This shared FL model is used to improve the local FL model of each user so as to enable the users to collaboratively perform a learning task without training data transfer. Hereinafter, the FL model that is generated by the BS using the local FL models of its associated users is called the global FL model. As shown in Fig. 1, the uplink from the users to the BS is used to transmit the parameters related to the local FL model while the downlink is used to transmit the parameters

7

TABLE I LIST OF NOTATIONS.

Notation U Xi yik PB Pi R g U a λ R γT wi γE K

Description Number of users Data collected by user i
Output of xik Transmit power of BS Transmit power of user i
Number of RBs Global FL model
Set of users User selection vector
Learning rate RB allocation vector of all users
Delay requirement Local FL model of user i Energy consumption requirement Total number of training data samples

Notation liU (ri, Pi)
xik Pmax cUi (ri, Pi) Ki BD
cDi liD Z (g) qi (ri, Pi) Z (wi) f (g (a, R) , xik, yik) ei (ri, Pi) ri BU

Description Uplink transmission delay FL input vector implemented by user i Maximum transmit power of each user Uplink data rate of user i Number of samples collected by user i Total downlink bandwidth of each BS Downlink data rate of user i Downlink transmission delay Data size of global FL model Packet error rate of user i Data size of local FL model
Loss function of FL Energy consumption of user i RB allocation vector of user i
Bandwidth of each RB

related to the global FL model.
A. Machine Learning Model
In our model, each user i collects a marix Xi = [xi1, . . . , xiKi] of input data, where Ki is the number of the samples collected by each user i and each element xik is an input vector of the FL algorithm. The size of xik depends on the speciﬁc FL task. Our approach, however, is applicable to any generic FL algorithm and task. Let yik be the output of xik. For simplicity, we consider an FL algorithm with a single output, however, our approach can be readily generalized to a case with multiple outputs [12]. The output data vector for training the FL algorithm of user i is yi = [yi1, . . . , yiKi]. We assume that the data collected by each user i is different from the other users, i.e., (xi = xn, i = n, i, n ∈ U ). We deﬁne a vector wi to capture the parameters related to the local FL model that is trained by xi and yi. In particular, wi determines the local FL model of each user i. For example, in a linear regression learning algorithm, xTikwi represents the predicted output and wi is a weight vector that determines the performance of the linear regression learning algorithm. The training process of an FL algorithm is done in a way to solve

8

the following optimization problem:

1 U Ki

min K w1,...,wU

f (wi, xik, yik),

(1)

i=1 k=1

s. t. w1 = w2 = . . . = wU = g, ∀i ∈ U ,

(1a)

U
where K = Ki is total size of training data of all users and g is the global FL model that
i=1
is generated by the BS and f (wi, xik, yik) is a loss function. The loss function captures the
performance of the FL algorithm. For different learning tasks, the FL performance captured

by the loss function is different. For example, for a prediction learning task, the loss function

captures the prediction accuracy of FL. In contrast, for a classiﬁcation learning task, the loss

function captures the classiﬁcation accuracy. Meanwhile, for different FL algorithms, different

loss functions can be deﬁned [23]. For example, for a linear regression FL, the loss function is

f

(wi, xik, yik)

=

1 2

xTikwi − yik 2. As the prediction errors (i.e., xTikwi − yik) increase, the loss

function f (wi, xik, yik) increases. Constraint (1a) is used to ensure that, once the FL algorithm

converges, all of the users and the BS will share the same FL model for their learning task.

This captures the fact that the purpose of an FL algorithm is to enable the users and the BS

to learn an optimal global FL model without data transfer. To solve (1), the BS will transmit

the parameters g of the global FL model to its users so that they train their local FL models.

Then, the users will transmit their local FL models to the BS to update the global FL model.

The detailed procedure of training an FL algorithm [24] to minimize the loss function in (1) is

shown in Fig. 2. In FL, the update of each user i’s local FL model wi depends on the global model g while the update of the global model g depends on all of the users’ local FL models.

The update of the local FL model wi depends on the learning algorithm. For example, one can use gradient descent, stochastic gradient descent, or randomized coordinate descent [12] to

update the local FL model. The update of the global model g is given by [12]:

U

Kiwi

g = i=1

.

(2)

K

During the training process, each user will ﬁrst use its training data Xi and yi to train the local FL model wi and then, it will transmit wi to the BS via wireless cellular links. Once the BS

receives the local FL models from all participating users, it will update the global FL model

9
based on (2) and transmit the global FL model g to all users to optimize the local FL models. As time elapses, the BS and users can ﬁnd their optimal FL models and use them to minimize the loss function in (1). Since all of the local FL models are transmitted over wireless cellular links, once they are received by the BS, they may contain erroneous symbols due to the unreliable nature of the wireless channel, which, in turn, will have a signiﬁcant impact on the performance of FL. Meanwhile, the BS must update the global FL model once it receives all of the local FL models from its users and, hence, the wireless transmission delay will signiﬁcantly affect the convergence of the FL algorithm. In consequence, to deploy FL over a wireless network, one must jointly consider the wireless and learning performance and factors.

B. Transmission Model

For uplink, we assume that an orthogonal frequency division multiple access (OFDMA)

technique in which each user occupies one RB. The uplink rate of user i transmitting its local

FL parameters to the BS is given by:





R

cUi (ri, Pi) =

ri,nB

Ulog2

1 

+

n=1

Pihi

,

Pi hi + BUN0

(3)

i ∈Un

where ri = [ri,1, . . . , ri,R] is an RB allocation vector with R being the total number of RBs,
R
ri,n ∈ {0, 1} and ri,n = 1; ri,n = 1 indicates that RB n is allocated to user i, and ri,n = 0,
n=1
otherwise; Un represents the set of users that are located at the other service areas and transmit
data over RB n; BU is the bandwidth of each RB and Pi is the transmit power of user i; hi is

the channel gain between user i and the BS; N0 is the noise power spectral density; Pi hi
i ∈Un
is the interference caused by the users that are located in other service areas (e.g., other BSs

not participating in the FL algorithm) and use the same RB. Note that, although we ignore the

optimization of resource allocation for the users located at the other service areas, we must

consider the interference caused by the users in other service areas (if they are sharing RBs with

the considered FL users), since this interference may signiﬁcantly affect the packet error rates

and the performance of FL.

Similarly, the downlink data rate achieved by the BS when transmitting the parameters of

10

global FL model to each user i is given by:





cDi

=

BDlog2

1+ 

PB hi

,

PBhij + BDN0

(4)

j∈B

where BD is the bandwidth that the BS used to broadcast the global FL model of each user i;

PB is the transmit power of the BS; B is the set of other BSs that cause interference to the

BS that performs the FL algorithm; hij is the channel gain between user i and BS j. Given the

uplink data rate cUi in (3) and the downlink data rate cDi in (4), the transmission delays between

user i and the BS over uplink and downlink are respectively speciﬁed as:

liU

(ri, Pi)

=

Z (wi) cUi (ri, Pi)

,

(5)

liD

=

Z (g) cDi ,

(6)

where function Z (x) is the data size of x which is deﬁned as the number of bits that the users

or the BS require to transmit vector x over wireless links. In particular, Z (wi) represents the

number of bits that each user i requires to transmit local FL model wi to the BS while Z (g) is

the number of bits that the BS requires to transmit the global FL model g to each user. Here,

Z (wi) and Z (g) are determined by the type of implemented FL algorithm. From (2), we see

that the number of elements in the global FL model g is similar to that of each user i’s local

FL model wi. Hence, we assume Z (wi) = Z (g).

C. Packet Error Rates

For simplicity, we assume that each local FL model wi will be transmitted as a single packet

in the uplink. A cyclic redundancy check (CRC) mechanism is used to check the data errors in

the received local FL models at the BS. In particular, C (wi) = 0 indicates that the local FL

model received by the BS contains data errors; otherwise, we have C (wi) = 1. The packet error

rate experienced by the transmission of each local FL model wi to the BS is given by [25]:

R

qi (ri, Pi) = ri,nqi,n,

(7)

n=1





 

m

Pi hi +BUN0

where qi,n = 1 − exp −





i ∈Un
Pi hi

 

is

the

packet

error

rate

over

RB

n

with

m



being a waterfall threshold [25].

11

In the considered system, whenever the received local FL model contains errors, the BS will not use it for the update of the global FL model. We also assume that the BS will not ask the corresponding users to resend their local FL models when the received local FL models contain data errors. Instead, the BS will directly use the remaining correct local FL models to update the global FL model. As a result, the global FL model in (2) can be given by:

U

KiaiwiC (wi)

g (a, P , R) = i=1

,

(8)

U

KiaiC (wi)

i=1

where a = [a1, . . . , aU ] is the vector of the user selection index with ai = 1 indicating that

user i performs the FL algorithm and ai = 0, otherwise, R = [r1, · · · , rU ], P = [P1, · · · , PU ],
U
KiaiC (wi) is the total number of training data samples, which depends on the user selection
i=1
vector a and packet transmission C (wi), KiwiC (wi) = 0 indicates that the local FL model

of user i contains data errors and, hence, the BS will not use it to generate the global FL

model, and g (a, P , R) is the global FL model that explicitly incorporates the effect of wireless

transmission. From (8), we see that the global FL model also depends on the resource allocation

matrix R, user selection vector a, and transmit power vector P .

D. Energy Consumption Model

In our network, the energy consumption of each user consists of the energy needed for two purposes: a) Transmission of the local FL model and b) Training of the local FL model. The energy consumption of each user i is given by [26]:

ei (ri, Pi) = ςωiϑ2Z (Xi) + PiliU (ri, Pi) ,

(9)

where ϑ is the frequency of the central processing unit (CPU) clock of each user i, ωi is the number of CPU cycles required for computing per bit data of user i, and ς is the energy consumption coefﬁcient depending on the chip of each user i’s device [26]. In (9), ςωiϑ2Z (Xi) is the energy consumption of user i training the local FL model at its own device and PiliU (ri, Pi) represents the energy consumption of local FL model transmission from user i to the BS. Note

that, since the BS can have continuous power supply, we do not consider the energy consumption

of the BS in our optimization problem.

12

E. Problem Formulation

To jointly design the wireless network and the FL algorithm, we now formulate an optimization

problem whose goal is to minimize the FL loss function, while factoring in the wireless network

parameters. This minimization problem includes optimizing transmit power allocation as well as

resource allocation for each user. The minimization problem is given by:

1 U Ki

min a,P ,R K

f (g (a, P , R) , xik, yik)

(10)

i=1 k=1

s. t. ai, ri,n ∈ {0, 1} , ∀i ∈ U , n = 1, . . . , R,
R
ri,n = ai, ∀i ∈ U ,
n=1
liU (ri, Pi) + liD ≤ γT, ∀i ∈ U ,
ei (ri, Pi) ≤ γE, ∀i ∈ U ,
ri,n ≤ 1, ∀n = 1, . . . , R,
i∈U
0 ≤ Pi ≤ Pmax, ∀i ∈ U ,

(10a) (10b) (10c) (10d) (10e) (10f)

where γT is the delay requirement for implementing the FL algorithm, γE is the energy consumption of the FL algorithm, and B is the total downlink bandwidth. (10a) and (10b) indicates that each user can occupy only one RB for uplink data transmission. (10c) is the delay needed to execute the FL algorithm. (10d) is the energy consumption requirement of performing an FL algorithm. (10e) indicates that each uplink RB can be allocated to at most one user. (10f) is a maximum transmit power constraint.
From (7) and (8), we see that the transmit power and resource allocation determine the packet error rate, thus affecting the update of the global FL model. In consequence, the loss function of the FL algorithm in (10) depends on the resource allocation and transmit power. Moreover, (10c) shows that, in order to perform an FL algorithm, the users must satisfy a speciﬁc delay requirement. In particular, in an FL algorithm, the BS must wait to receive the local model of each user before updating its global FL model. Hence, transmission delay plays a key role in the FL performance. In a practical FL algorithm, it is desirable that all users transmit their local FL models to the BS simultaneously. From (10d), we see that to perform the FL algorithm, a given user must have enough energy to transmit and update the local FL model throughout the

13

FL iterative process. If this given user does not have enough energy, the BS should choose this user to participate in the FL process. In consequence, in order to implement an FL algorithm in a real-world network, the wireless network must provide low energy consumption and latency, and highly reliable data transmission.

III. ANALYSIS OF THE PERFORMANCE OF FEDERATED LEARNING

To solve (10), we ﬁrst need to analyze how the packet error rate affects the performance of the

federated learning. To ﬁnd the relationship between the packet error rates and the performance

of the federated learning, we must ﬁrst analyze the convergence rate of FL. However, since the

update of the global FL model depends on the instantaneous signal-to-interference-plus-noise

ratio (SINR), we can analyze only the expected convergence rate of FL. Here, we ﬁrst analyze

the expected convergence rate of FL. Then, we show how the packet error rate affects the

performance of the FL in (10).

In the studied network, the users adopt a standard gradient descent method to update their

local FL models as done in [12]. Therefore, during the training process, the update of user i’s

local FL model wi at time t is given by:

λ Ki

wi,t+1 = gt (a, P , R) − Ki k=1 ∇f (gt (a, P , R) , xik, yik),

(11)

where λ is the learning rate and ∇f (gt (a, P , R) , xik, yik) is the gradient of f (gt (a, P , R) , xik, yik)

with respect to gt (a, P , R).

U Ki

Ki

We

assume

that

F (g)

=

1 K

f (g, xik, yik) and Fi (g) = f (g, xik, yik) where g is

i=1 k=1

k=1

short for g (a, P , R). Based on (11), the update of global FL model g at time t can be given

by:

gt+1 = gt − λ (∇F (gt) − o) ,

(12)

U

Ki ai wi C (wi )

where

o

=

∇F (gt)

−

i=1 U

with

Ki ai C (wi )

i=1



 1, C (wi) =

with probability 1 − qi (ri, Pi) ,

(13)

0, with probability qi (ri, Pi) .

We also assume that the FL algorithm converges to an optimal global FL model g∗ after the

learning steps. To derive the expected convergence rate of FL, we ﬁrst make the following

assumptions:

14

• First, we assume that the gradient ∇F (g) of F (g) is uniformly Lipschitz continuous with respect to g [27]. Hence, we have:

∇F (gt+1) − ∇F (gt) ≤ L gt+1 − gt ,

(14)

where L is a positive constant and gt+1 − gt is the norm of gt+1 − gt. • Second, we assume that F (g) is strongly convex with positive parameter µ, such that:

F

(gt+1)

≥

F

(gt)

+

(gt+1

−

gt)T

∇F

(gt)

+

µ 2

gt+1 − gt

2.

(15)

• We also assumed that F (g) is twice-continuously differentiable. Based on (14) and (15),

we have:

µI ∇2F (g) LI.

(16)

• We also assume that ∇f (gt, xik, yik) 2 ≤ ζ1 + ζ2∇ F (gt) 2 with ζ1 ≥ 0 and ζ2 ≥ 1. These assumptions can be easily satisﬁed by the general FL loss functions such as linear or logistic loss functions. The expected convergence rate of the FL algorithms can now be obtained by the following theorem.

Theorem 1. Given the transmit power vector P , RB allocation matrix R, user selection vector a,

optimal

global

FL

model

g∗,

and

the

learning

rate

λ

=

1 L

,

the

upper

bound

of

E[F (gt+1)−F (g∗)]

can be given by:

E[F (gt+1) − F (g∗)] ≤

ζ1 2LK

U

1 − At Kiqi (ri, Pi) 1 − A

+AtE(F (g0) − F (g∗)),

(17)

i=1

Impact of wireless factors on FL convergence

U

where

A

=

1

−

2µ L

+

µζ2 LK

Kiqi (ri, Pi).

i=1

Proof. See Appendix A.

From

Theorem

1,

we

see

that,

when

the

learning

rate

λ

is

a

constant

(λ

=

1 L

),

the

FL

algorithm

that considers the effect of the packet error rates will ﬁnally converge as t increases. However,

U

a

gap,

ζ1 2LK

Ki (1 − ai + aiqi (ri, Pi)), exists between E[F (gt)] and E[F (g∗)]. This gap is

i=1

caused by the packet errors and user selection policy. As the packet error rate decreases, the gap

between E[F (gt)] and E[F (g∗)] decreases. Meanwhile, as the number of users that implement

the FL algorithm increases, the gap also decreases. Moreover, as the packet error rate decreases,

15

the value of A also decreases, which indicates that the convergence speed of the FL algorithm improves. Hence, it is necessary to optimize resource allocation, user selection, and transmit power for the implementation of any FL algorithm over a realistic wireless network.
According to Theorem 1, the following result is derived to guarantee the convergence of the FL algorithm.

Proposition

1.

Given

the

learning

rate

λ

=

1 L

,

to

guarantee

convergence

and

reduce

the

effect

of packet errors on the FL algorithm, ζ2 must satisfy:

1 < ζ2 < 2.

(18)

Proof. From Theorem 1, we see that when A < 1, At = 0. Hence, E[F (gt+1) − F (g∗)] =

U

Kiqi

(ri,

Pi)

1 1−A

and

the

FL

algorithm

converges.

In

consequence,

to

guarantee

the

con-

i=1

U

U

vergence,

we

only

need

to

make

Amax

=

1−

2µ L

+

µζ2 LK

Ki < 1. Since

Ki = K. we

i=1

i=1

have

Amax

=

1−

2µ L

+

µζ2 L

.

From

(16),

we

see

that

µ

<

L

and,

hence,

µ L

<

1.

To

make

Amax

<

1,

we

only

need

to

ensure

that

µζ2 L

−

2µ L

<

0.

Therefore,

we

have

ζ2

<

2.

To

enable

∇f (gt, xik, yik) 2 ≤ ζ1 + ζ2∇ F (gt) 2, we have ζ2 > 1. This completes the proof.

From Proposition 1, we see that the convergence of the FL algorithm depends on the parameters related to the approximation of ∇ F (gt) 2. Using Proposition 1, we can determine the convergence of the FL algorithm based on the approximation of ∇ F (gt) 2. From Theorem 1 and Proposition 1, we can also see that the number of training data samples will not affect the convergence of the FL algorithm but it affects the value that the FL algorithm converges to.
Based on Theorem 1, next, we can also derive the convergence rate of an FL algorithm when there are no packet errors.

Lemma

1.

Given

the

optimal

global

FL

model

g∗

and

the

learning

rate

λ

=

1 L

,

the

upper

bound

of E[F (gt+1) − F (g∗)] for an FL algorithm without considering packet errors and user selection

is given by:

E[F (gt+1) − F (g∗)] ≤

2µ 1−
L

t
E(F (g0) − F (g∗)).

(19)

Proof. Since the FL algorithms do not consider the packet error rates and user selection, we

U

have

qi (ri, Pi)

=

0,

ai

=

1,

A

=

1−

2µ L

.

Hence,

ζ1 2LK

Ki

(1

−

ai

+

aiqi

(ri,

Pi))

1−At 1−A

=

0.

i=1

16

Then (19) can be derived based on (17).
From Lemma 1, we can observe that, if we do not consider the packet transmission errors, the FL algorithm will converge to the optimal global FL model without any gaps. This result also corresponds to the result in the existing works (e.g., [27]). In the following section, we show how one can leverage the result in Theorem 1 to solve the proposed problem (10).

IV. OPTIMIZATION OF PREDICTION ERRORS FOR FEDERATED LEARNING ALGORITHM

In this section, our goal is to minimize the FL loss function when considering the underlying

wireless network constraints. To solve the problem in (10), we must ﬁrst simplify it. From

Theorem 1, we can see that, to minimize the loss function in (10), we need to only minimize the

U

gap,

ζ1 2LK

Ki

(1

−

ai

+

aiqi

(ri, Pi))

1−At 1−A

.

When

A

≥

1,

the

FL

algorithm

will

not

converge.

i=1

In consequence, here, we only consider the minimization of the FL loss function when A < 1.

Hence, as t is large enough, which captures the asymptotic convergence behavior of FL, we

have At = 0. The gap can be rewritten as follows:

U

ζ1 2LK

U i=1

1 − At Ki (1 − ai + aiqi (ri, Pi)) 1 − A

=

ζ1 2LK

Ki (1 − ai + aiqi (ri, Pi))

i=1

.

U

2µ L

−

µζ2 LK

Ki (1 − ai + aiqi (ri, Pi))

(20)

i=1

U

From

(20),

we

can

observe

that

minimizing

ζ1 2LK

Ki

(1

−

ai

+

aiqi

(ri,

Pi))

1−At 1−A

only

re-

i=1

U

R

quires minimizing Ki (1 − ai + aiqi (ri, Pi)). Meanwhile, since ai = ri,n and qi (ri, Pi) =

i=1

n=1

R

ri,nqi,n, when ai = 1, qi (ri, Pi) ≤ 0 and when ai = 0, qi (ri, Pi) = 0. In consequence, we

n=1

have aiqi (ri, Pi) = qi (ri, Pi). The problem in (10) can be simpliﬁed as follows:

U
min Ki
P ,R i=1

R
1 − ri,n + qi (ri, Pi)
n=1

(21)

s. t. (10c) – (10f).

ri,n ∈ {0, 1} , ∀i ∈ U , n = 1, . . . , R,

(21a)

R
ri,n ≤ 1, ∀i ∈ U .
n=1

(21b)

Next, we ﬁrst ﬁnd the optimal transmit power for each user given the uplink RB allocation

matrix R. Then, we ﬁnd the uplink RB allocation to minimize the FL loss function.

17

A. Optimal Transmit Power The optimal transmit power of each user i can be determined by the following lemma.

Proposition 2. Given the uplink RB allocation vector ri of each user i, the optimal transmit power of each user i, Pi∗ is given by:

Pi∗ = min {Pmax, Pi,γE} ,

(22)

where

Pi,γE

satisﬁes

the

equality

ςωiϑ2Z (Xi) +

Pi,γE Z(wi)
( ) cUi ri,Pi,γE

=

γE.

Proof. See Appendix B.

From Proposition 2, we see that the optimal transmit power depends on the size of the collected data Z (Xi), the size of the local FL model Z (wi), and the interference in each RB. In particular, as the size of the collected data and local FL model increases, each user must spend more energy for training FL model and, hence, the energy that can be used for data transmission decreases. In consequence, the value of the FL loss function increases.

B. Optimal Uplink Resource Block Allocation

Based on Proposition 2 and (7), the optimization problem in (21) can be simpliﬁed as follows:

U
min Ki R i=1

R

R

1 − ri,n + ri,nqi,n

n=1

n=1

(23)

s. t. (10a), (10b), and (10e), liU (ri, Pi∗) + liD ≤ γT, ∀i ∈ U , ei (ri, Pi∗) ≤ γE, ∀i ∈ U .

(23a) (23b)

Obviously, the objective function (23) is a mixed-integer linear programming problem, which can be solved by using bipartite matching algorithm [28]. Compared to traditional convex optimization algorithms, using bipartite matching to solve problem (23) does not require computing the gradients of each variable nor dynamically adjusting the step size for convergence.
To use a bipartite matching algorithm for solving problem (23), we ﬁrst transform the optimization problem into a bipartite matching problem. We construct a bipartite graph A = (U × R, E) where R is the set of RBs that can be allocated to each user, each vertex in U represents a user

18

and each vertex in R represents an RB, and E is the set of edges that connect to the vertices

from each set U and R. Let ϑin ∈ E be the edge connecting vertex i in U and vertex n in R with ϑin ∈ {0, 1}, where ϑin = 1 indicates that RB n is allocated to user i, otherwise, we have ϑin = 0. Let matching T be a subset of edges in E, in which no two edges share a common vertex in R, such that each RB n can only be allocated to one user (constraint (10e) is satisﬁed). Nevertheless, in T , all of the edges associated with a vertex i ∈ U will not share a common

vertex n ∈ R, such that each user i can occupy only one RB (constraint (10b) is satisﬁed). The

weight of edge ϑin is given by:



ψin =  Ki (qi,n − 1) , liU (ri,n, Pi∗) + liD ≤ γT and ei (ri,n, Pi∗) ≤ γE,

(24)

 +∞,

otherwise.

From (24), we can see that when RB n is allocated to user i, if the delay and energy requirements cannot be satisﬁed, we will have ψin = +∞, which indicates that RB n will not be allocated to user i. The goal of this formulated bipartite matching problem is to ﬁnd an optimal matching set T ∗ that can minimize the weights of the edges in T ∗. A standard Hungarian algorithm [29] can be used to ﬁnd the optimal matching set T ∗. When the optimal matching set is found, the

optimal RB allocation is determined.

C. Implementation and Complexity
Next, we ﬁrst analyze the implementation of the Hungarian algorithm. To implement the Hungarian algorithm for ﬁnding the optimal matching set T ∗, the BS must ﬁrst calculate the packet error rate qi,n, total delay liU (ri,n, Pi∗) + liD, and the energy consumption ei (ri,n, Pi∗) of each user transmitting the local FL model over each RB n. To calculate the packet error rate qi,n and total delay liU (ri,n, Pi∗) + liD, the BS must know the SINR over each RB and the data size of FL model. The BS can use channel estimation methods to learn the SINR over each RB. The data size of the FL model depends on the learning task. To implement an FL mechanism, the BS must ﬁrst send the FL model information and the learning task information to the users. In consequence, the BS will learn the data size of FL model before the execution of the FL algorithm. To calculate the energy consumption ei (ri,n, Pi∗) of each user, the BS must learn each user’s device information such as CPU. This device information can be learned by the BS when the users initially connect to the BS. Given the packer error rate qi,n, total delay liU (ri,n, Pi∗)+liD,

19
and the energy consumption ei (ri,n, Pi∗) of each user, the BS can compute ψin according to (24). Given ψin, i ∈ U, n ∈ R, the Hungarian algorithm can be used to ﬁnd the optimal matching set T ∗. Since (23) is a mixed-integer linear programming problem, it admits an optimal matching set T ∗ and the Hungarian algorithm will ﬁnally ﬁnd the optimal matching set T ∗.
With regards to the complexity of the Hungarian algorithm, it must ﬁrst use U R iterations to calculate the packer error rate, total delay, and energy consumption of each user over each RB. After that, the Hungarian algorithm will update the values of ψin so as to ﬁnd the optimal matching set T ∗. The worst complexity of the hungarian algorithm to ﬁnd the optimal matching set T ∗ is O (U 2R) [30]. In contrast, the best complexity is O (U R). In consequence, the major complexity lies in calculating the weight of each edge and updating the edges in the matching set T . However, in the Hungarian algorithm, we need to only perform simple operations such as Ki (qi,n − 1) without calculation for the gradients of each valuables nor adjusting the step sizes as done in the optimization algorithms. Meanwhile, Algorithm 1 is implemented by the BS in a centralized manner and the BS will have sufﬁcient computational resources to implement it.
V. SIMULATION RESULTS AND ANALYSIS
For our simulations, we consider a circular network area having a radius r = 500 m with one BS at its center servicing U = 20 uniformly distributed users. The other parameters used in simulations are listed in Table I. The data used to train the FL algorithm is generated randomly from [0, 1]. The input x and the output y follow the function y = −2x + 1 + n × 0.4 where n follows a Gaussian distribution N (0, 1). The FL algorithm is used to model the relationship between x and y (i.e., FL is used as a linear regression). For comparison purposes, we use two baselines: a) an FL algorithm that optimizes user selection with random resource allocation and b) an FL algorithm that randomly determines user selection and resource allocation. Baseline a) is actually an FL algorithm without consideration of wireless factors. Baseline b) is a conventional FL in [12] without consideration of wireless factors nor optimizing FL performance.
Fig. 3 shows an example of using FL for linear regression. In this ﬁgure, the red crosses are the data samples. In the optimal FL, the optimal RB allocation, user association, and transmit power powers are derived using a heuristic search method. From Fig. 3, we see that the proposed FL algorithm can ﬁt the data samples more accurately than baselines a) and b). This is due to the fact that the proposed FL algorithm jointly considers the learning and wireless factors and,

20

TABLE II SYSTEM PARAMETERS

Parameter α PB M σi f ς ωi

Value 2
1W 64 1 109 10−27 40

Parameter N0 BD BU Pmax Ki γT γE

Value -174 dBm/Hz
20 MHz 150 kHz 0.01 W [12,10,8,4,2] 100 ms
0.02 J

Output of the FL algorithm

2

Data samples

Proposed algorithm

Optimal FL

1

Baseline a)

Baseline b)

0

-1

-2

0

0.2

0.4

0.6

0.8

1

Input of the FL algorithm

Fig. 3. An example of implementing FL for linear regression.

hence, it can optimize user selection and resource allocation to reduce the effect of wireless transmission errors on training FL algorithm and improve the performance of the FL algorithm. Fig. 3 also shows that the proposed algorithm can reach the same performance as the optimal FL, which veriﬁes that the proposed algorithm can ﬁnd an optimal solution using the Hungarian algorithm.
Fig. 4 shows how the value of the FL loss function changes as the total number of users varies. In this ﬁgure, an appropriate subset of users is selected to perform the FL algorithm. From Fig. 4, we can observe that, as the number of users increases, the value of the loss function decreases.

21

Value of the loss function

0.17 0.16 0.15 0.14 0.13 0.12

Proposed algorithm Baseline a) Baseline b)

5

10

15

20

Number of users

Fig. 4. Value of the loss function as the number of users varies.

Moreover, as the number of users increases, the effect of packet errors on the global FL model decreases. This is due to the fact that an increase in the number of users leads to more data available for the FL algorithm training and, hence, improving the accuracy of approximation of the gradient of the loss function. Fig. 4 also shows that the proposed algorithm reduces the loss function by, respectively, up to 10% and 16% compared to baselines a) and b). The 10% reduction of the loss function stems from the fact that the proposed algorithm optimizes the resource allocation. The 16% reduction stems from the fact that the proposed algorithm joint considers learning and wireless effects and, hence, it can optimize the user selection and resource allocation to reduce the FL loss function. Fig. 4 also shows that when the number of users is less than 12, the value of the loss function decreases quickly. In contrast, as the number of users continues to increase, the value of the FL loss function decreases slowly. This is because, for a higher number of users, the BS will have enough data samples to accurately approximate the gradient of the loss function.
In Fig. 5, we show how the transmission errors affect the convergence of the global FL model. From Fig. 5, we see that, as the number of iterations increases, the global FL model of all considered learning algorithms decreases ﬁrst and, then remains unchanged. Here, the global FL model remains unchanged which shows that the global FL model converges. From Fig. 5, we

22

Value of global FL model

0

Proposed algorithm

-0.5

Baseline a)

Baseline b)

-1

-1.5

-2

-2.5

-3 10 20 30 40 50 60 70 80
Iteration (t)

Fig. 5. Value of the loss function as the number of iteration varies.

can also see that the decrease speed in the value of the global FL model is different during each iteration. This is due to the fact that the local FL models that are received by the BS may contain data errors and the BS may not be able to use them for the update of the global FL model. In consequence, at each iteration, the number of local FL models that can be used for the update of the global FL model will be different. Fig. 5 also shows that a gap exists between the proposed algorithm and baselines a) and b). This gap is caused by the packet errors. Meanwhile, Fig. 5 clearly shows that the proposed algorithm can converge faster than both baselines a) and b). This is because the proposed algorithm can optimize the user selection and resource allocation to improve the convergence speed.
Fig. 6 shows how the value of the FL loss function changes as the number of data samples of each user varies. From this ﬁgure, we observe that, as the number of data samples of each user increases, the values of the FL loss function of all of considered FL algorithms decrease. This is due to the fact that, as the number of data samples increases, all of the considered learning algorithms can use more data samples for training. Fig. 6 also demonstrates that, when the number of data samples is less than 30, the value of the loss function decreases quickly. However, as the number of data samples continues to increase, the value of the loss function remains unchanged. This is due to the fact that as the number of data samples is over 30, the

23

0.165

Proposed algorithm Baseline a) Baseline b)

Value of the loss function

0.16

0.155

0.15

10

20

30

40

50

Number of data samples per user

Fig. 6. Value of the loss function as the number of data samples per user varies.

Number of iterations

140

R =10

R =15
120

100

80

60

40

20

5

10

15

20

25

Number of users

Fig. 7. Number of iterations as the number of users varies.

BS has enough data samples to approximate the gradient of the loss function. In Fig. 7, we show the number of iterations that the Hungarian algorithm needs to ﬁnd the
optimal RB allocation as a function of the number of users. From this ﬁgure, we can see that, as the number of users increases, the number of iterations needed to ﬁnd the optimal RB allocation

24

y-axis (m)

800 600 400 200
0 -200 -400
-500

Coverage of the BS The BS The users that perform the FL The users that cannot perform the FL

User 1

User 2
User 3 User 4

User 5

0

500

x-axis (m)

Fig. 8. An example of the users that perform an FL algorithm over a wireless network.

increases. This is because, as the number of users increases, the size of the edge weight matrix in (24) increases and, hence, the Hungarian algorithm needs to use more iterations to ﬁnd the optimal RB allocation. Fig. 7 also shows when the number of users is smaller than the number of RBs, the number of iterations needed to ﬁnd the optimal RB allocation increases slowly. However, as the number of users continues to increase, the number of iterations signiﬁcantly increases. Fig. 7 also shows that, when the number of users is larger than 10, the number of iterations needed to ﬁnd the optimal RB allocation for a network with 10 RBs is larger than that of a network with 15 RBs. This is due to the fact that as the number of users is larger than 10, the gap between the number of users and the number of RBs for a network with 10 RBs is larger than that for a network with 15 RBs.
Fig. 8 shows an example of the users that participate in the FL algorithm over a wireless network with 20 users. In this ﬁgure, the blue points indicate the users that are selected to perform the FL algorithm while the black points indicate users that are not selected for the implementation of the FL algorithm. In particular, due to the energy consumption and delay requirements, users 2, 3, and 4 are not selected to perform the FL algorithm. Users 1 and 5 were also not selected for the implementation of the FL algorithm due to the limited number of RBs.

25
VI. CONCLUSION In this paper, we have developed a novel framework that enables the implementation of FL algorithms over wireless networks. We have formulated an optimization problem that jointly considers user selection and resource allocation for the minimization of the value of FL loss function. To solve this problem, we have derived the closed-form expression of the expected convergence rate of the FL algorithm that considers the wireless factors. Based on the derived expected convergence rate, the optimal transmit power is determined given the user selection and uplink RB allocation. Then, the Hungarian algorithm is used to ﬁnd the optimal user selection and RB allocation so as to minimize the FL loss function. Simulation results have shown that the joint federated learning and communication framework yields signiﬁcant improvements in the performance compared to the existing implementation of the FL algorithm that does not account for the wireless factors.

APPENDIX

A. Proof of Theorem 1

To prove Theorem 1, we ﬁrst rewrite F (gt+1) using the second-order Taylor expansion, which

can be expressed by:

F (gt+1)

=

F (gt)

+

(gt+1

−

gt)T ∇F (gt)

+

1 2 (gt+1

−

gt)T ∇2F (g)(gt+1

−

gt),

≤

F (gt)

+

(gt+1

−

gt)T

∇F (gt)

+

L 2

gt+1 − gt

2,

(25)

where

the

inequality

stems

from

the

assumption

in

(16).

Given

the

learning

rate

λ

=

1 L

,

based

on (12), the expected optimization function E[F (gt+1)] can be expressed as:

E[F (gt+1)] ≤E

F (gt)

−

λ(∇F (gt)

−

o)T

∇F (gt)

+

Lλ2 2

∇F (gt) − o

2

,

(=a)E

(F

(gt))

−

1 2L

∇F (gt)

2

+

1 2L E

o2 ,

(26)

26

where

(a)

stems

from

the

fact

that

Lλ2 2

∇F (gt) − o

2

=

1 2L

∇F (gt)

2

−

1 L

oT

∇F

(gt

)

+

1 2L

o

2.

Next, we derive E ( o 2), which can be given as follows:



U Ki

2

ai∇f (g, xik, yik)C (wi)

E

o2



=

E

 

∇F (gt) − i=1 k=1

U

 , 



KiaiC (wi)



i=1

 U Ki

U Ki

2

∇f (g, xik, yik)

ai∇f (g, xik, yik)C (wi)



=

E

 

i=1 k=1

K

− i=1 k=1 U

 , 



KiaiC (wi)



i=1

 U Ki

2

∇f (g, xik, yik) (1 − aiC (wi))



≤

E

 

i=1 k=1

K

 , 

(27)





U Ki

U Ki

Since

∇f (gt, xik, yik) 2 ≤ K

∇f (gt, xik, yik 2) and ∇f (gt, xik, yik) 2 ≤

i=1 k=1

i=1 k=1

ζ1 + ζ2∇ F (gt) 2, (27) can be simpliﬁed as follows:

E

o 2 ≤E

1U K

Ki

∇f (g, xik, yik) 2 (1 − aiC (wi))

,

i=1 k=1

≤ E ζ1 + ζ2∇ F (gt) 2 (1 − aiC (wi)) ,

1 =
K

U

Ki

ζ1 + ζ2∇

F (gt) 2

(1 − ai + aiqi (ri, Pi)).

(28)

i=1

Therefore, we have:

1 E[F (gt+1)] ≤ E(F (gt)) − L

∇F (gt)

2+ 1 2LK

U

Ki

ζ1 + ζ2∇

F (gt)

2

(1 − ai + aiqi (ri, Pi)),

i=1

1 = E(F (gt)) − L

1 − ζ2 2K

U

Ki (1 − ai + aiqi (ri, Pi))

i=1

∇F (gt) 2

+ ζ1 2LK

U

Ki (1 − ai + aiqi (ri, Pi)) .

i=1

(29)

27

Subtract E[F (g∗)] in both sides of (29), we have:

E[F

(gt+1)

−

F

(g∗)]

≤E(F

(gt)

−

F

(g∗))

+

ζ1 2LK

U

Ki (1 − ai + aiqi (ri, Pi))

i=1

1 −
L

1 − ζ2 2K

U

Ki (1 − ai + aiqi (ri, Pi))

∇F (gt) 2.

(30)

i=1

By minimizing both sides of (15) with respect to gt+1, we have:

min
gt+1

F

(gt+1)

≥

min
gt+1

F (gt)

+

(gt+1

−

gt)T ∇F

(gt)

+

µ 2

gt+1 − gt

2

.

(31)

The minimization of the left-hand side is achieved by gt+1 = g∗, while the minimization of the

right-hand

side

is

achieved

by

gt+1

=

gt

−

1 µ

∇F

(gt).

Minimizing

(31)

yields:

F (g∗)

≥

F (gt)

−

1 2µ

∇F (gt)

2.

(32)

Hence, we have

∇F (gt) 2 ≥ 2µ(F (gt) − F (g∗)).

(33)

Substituting (33) into (30), we have:

E[F (gt+1)

−

F (g∗)]

≤

ζ1 2LK

U

Ki (1 − ai + aiqi (ri, Pi))

i=1

+

1 − 2µ + µζ2 L LK

U

Ki (1 − ai + aiqi (ri, Pi))

i=1

E(F (gt) − F (g∗)). (34)

U

Let

A

=

1

−

2µ L

+

µζ2 LK

Ki (1 − ai + aiqi (ri, Pi)). Applying (34) recursively, we have:

i=1

E[F

(gt+1)

−

F

(g∗)]

≤

ζ1 2LK

U

t−1
Ki (1 − ai + aiqi (ri, Pi)) Ak + AtE(F (g0) − F (g∗)),

i=1

k=1

= ζ1 2LK

U

Ki

(1

−

ai

+

aiqi

(ri, Pi))

1 − At 1−A

+

AtE(F (g0)

−

F (g∗)).

i=1

(35)

This completes the proof.

28

B. Proof of Proposition 2

To prove Proposition 2, we ﬁrst prove that ei (ri, Pi) is an increasing function of Pi. Based

on (3) and (9), we have:

ei (ri, Pi) = ςωiϑ2Z (Xi) + R

Pi

,

(36)

ri,nBUlog2 (1 + κi,nPi)

n=1

where κi,n =

Pi

hi hi

. +BUN0

The

ﬁrst

derivative

of

ei (ri, Pi)

with

respect

to

Pi

is

given

by:

i ∈Un

R

∂ei

(ri, Pi) ∂Pi

=

(ln 2)
n=1

((1 ri,n
1+κi,n Pi R

+

κi,nPi) ln(1 +

κi,nPi)
2

−

κi,nPi) .

ri,nBUln (1 + κi,nPi)

n=1

(37)

Since

∂ei(ri,Pi) ∂Pi

is

always

positive,

ei

(ri, Pi)

is

a

monotonically

increasing

function.

Contradiction

is used to prove Proposition 2. We assume that Pi (Pi = Pi∗) is the optimal transmit power of

user i. In (10d), ei (r∗i , Pi,γE) is a monotonically increasing function of Pi. Hence, as Pi > Pi∗,

ei (r∗i , Pi ) > γE, which does not meet the constraint (10f). From (7), we see that, the packer

error rates decrease as the transmit power increases. Thus, as Pi < Pi∗, we have qi (ri, Pi∗)

qi (ri, Pi ). In consequence, as Pi < Pi∗, Pi cannot minimize the function in (21). Hence, we

have Pi = Pi∗. This completes the proof.

REFERENCES
[1] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “Performance optimization of federated learning over wireless networks,” in Proc. of IEEE Global Communications Conference (GLOBECOM), Waikoloa, HI, USA, December 2019.
[2] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artiﬁcial neural networks-based machine learning for wireless networks: A tutorial,” IEEE Communications Surveys & Tutorials, to appear, 2019.
[3] Y. Sun, M. Peng, Y. Zhou, Y. Huang, and S. Mao, “Application of machine learning in wireless networks: Key techniques and open issues,” IEEE Communications Surveys & Tutorials, to appear, 2019.
[4] Y. Liu, S. Bi, Z. Shi, and L. Hanzo, “When machine learning meets big data: A wireless communication perspective,” arXiv preprint arXiv:1901.08329, Jan. 2019.
[5] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,” arXiv preprint arXiv:1902.01046, Mar. 2019.
[6] V. Smith, C. K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task learning,” in Proc. of Neural Information Processing Systems, Long Beach, CA, USA, Dec. 2017.
[7] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge AI: Intelligentizing mobile edge computing, caching and communication by federated learning,” IEEE Network, to appear, 2019.

29
[8] Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato, “Mobile edge computing, blockchain and reputation-based crowdsourcing IoT federated learning: A secure, decentralized and privacy-preserving system,” arXiv preprint arXiv:1906.10893, June 2019.
[9] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems: Applications, trends, technologies, and open research problems,” IEEE Network, to appear, 2019.
[10] E. Jeong, S. Oh, J. Park, H. Kim, M. Bennis, and S. L. Kim, “Multi-hop federated private data augmentation with sample compression,” in Proc. of International Joint Conference on Artiﬁcial Intelligence Workshop on Federated Machine Learning for User Privacy and Data Conﬁdentiality, Macao, China, Aug. 2019.
[11] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning: Challenges, methods, and future directions,” arXiv preprint arXiv:1908.07873, Aug. 2019.
[12] J. Konecˇny`, H. B. McMahan, D. Ramage, and P. Richtárik, “Federated optimization: Distributed machine learning for on-device intelligence,” arXiv preprint arXiv:1610.02527, Oct. 2016.
[13] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized data,” arXiv preprint arXiv:1602.05629, Feb. 2017.
[14] M. Chen, O. Semiari, W. Saad, X. Liu, and C. Yin, “Federated echo state learning for minimizing breaks in presence in wireless virtual reality networks,” arXiv preprint arXiv:1812.01202, Dec. 2018.
[15] J. Konecˇny`, B. McMahan, and D. Ramage, “Federated optimization: Distributed optimization beyond the datacenter,” arXiv preprint arXiv:1511.03575, Nov. 2015.
[16] S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Distributed federated learning for ultra-reliable low-latency vehicular communications,” arXiv preprint arXiv:1807.08127, Aug. 2018.
[17] S. Ha, J. Zhang, O. Simeone, and J. Kang, “Coded federated computing in wireless networks with straggling devices and imperfect CSI,” arXiv preprint arXiv:1901.05239, Jan. 2019.
[18] O. Habachi, M. A. Adjif, and J. P. Cances, “Fast uplink grant for NOMA: A federated learning based approach,” arXiv preprint arXiv:1904.07975, Mar. 2019.
[19] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network intelligence at the edge,” arXiv preprint arXiv:1812.02858, Dec. 2018.
[20] Q. Zeng, Y. Du, K. K. Leung, and K. Huang, “Energy-efﬁcient radio resource allocation for federated edge learning,” arXiv preprint arXiv:1907.06040, July 2019.
[21] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan, “Adaptive federated learning in resource constrained edge computing systems,” IEEE Journal on Selected Areas in Communications, vol. 37, no. 6, pp. 1205–1221, June 2019.
[22] S. Bi, J. Lyu, Z. Ding, and R. Zhang, “Engineering radio maps for wireless resource management,” IEEE Wireless Communications, to appear, 2019.
[23] C. Hennig and M. Kutlukaya, “Some thoughts about the design of loss functions,” REVSTAT–Statistical Journal, vol. 5, no. 1, pp. 19–39, March 2007.
[24] J. Konecny, H. B. McMahan, F. X. Yu, P. Richtarik, A. Theertha Suresh, and D. Bacon, “Federated learning: Strategies for improving communication efﬁciency,” in Proc. of NIPS Workshop on Private Multi-Party Machine Learning, Barcelona, Spain, Dec. 2016.
[25] Y. Xi, A. Burr, J. Wei, and D. Grace, “A general upper bound to evaluate packet error rate over quasi-static fading channels,” IEEE Transactions on Wireless Communications, vol. 10, no. 5, pp. 1373–1377, May 2011.

30
[26] Y. Pan, C. Pan, Z. Yang, and M. Chen, “Resource allocation for D2D communications underlaying a NOMA-based cellular network,” IEEE Wireless Communications Letters, vol. 7, no. 1, pp. 130–133, Feb 2018.
[27] M. P. Friedlander and M. Schmidt, “Hybrid deterministic-stochastic methods for data ﬁtting,” SIAM Journal on Scientiﬁc Computing, vol. 34, no. 3, pp. A1380–A1405, May 2012.
[28] M. Mahdian and Q. Yan, “Online bipartite matching with random arrivals: An approach based on strongly factor-revealing LPs,” in Proc. of the ACM symposium on Theory of computing, San Jose, California, USA, June 2011.
[29] R. Jonker and T. Volgenant, “Improving the hungarian assignment algorithm,” Operations Research Letters, vol. 5, no. 4, pp. 171–175, 1986.
[30] N. Landman K. Moore and J. Khim, “Hungarian maximum matching algorithm,” https://brilliant.org/wiki/ hungarian-matching/.

