分类号 ：

Ｄ Ｕ Ｉ

Ｃ：

６ ２ ５ ＊
６．

学 校代码 ：

８ ３８ ０ １

密级 公 开   ：

？

中 国 铁 道 科 学 研 究 院  

硕 士 学 位 论 文  

基 于 深度 强 化 学 习 的 动 车 所 行 车调 度 计 划 编 制 智 能化 研 究

 Ｒ ｅ

ｓ

ｅａ

ｒ

ｃ

ｈ ｏ

ｎ ｉ ｌ

ｔ

ｈ

ｅ

Ｉ ｒｎ ｔ ｅ ｌ ｌ ｉ ｇ ｅ ｎ ｔ ｉ ｚ ａ ｔ ｉ ｏｎ ｏ ｆＴ ｒ ａ ｉ ｎＤ ｉ ｓ ｐ ａ ｔ ｃ ｈ ｉ ｎ ｇ  

Ｐ ｌ ａ ｎｆ Ｇ ｅ ｎ ｅ ｒ ａ ｔ ｉ ｎ ｇｆ ｏｒ ｔ ｈ ｅＥ ＭＵＤ ｅ ｐ ｏ ｔ Ｂａ ｓ ｅ ｄ ｏｎＤ ｅ ｅ ｐ

Ｒ ｅ ｉ ｎ ｆ ｏｒ ｃ ｅ ｍ ｅ ｎ ｔ． Ｌ ｅａ ｒ ： ｎ ｉ ｎ ｇ

作者姓名 ：

专

业 ：

研究方 向 ：

导 师姓名 ：

曹 达   交通 信 息 工 程及 控制 编 组 站 自 动 化   张 雪 松 研 究 员  

学 位 授 予 单 位 ： 中 国 铁 道科 学 研 究 院 论 文 提 交 日 期 ： ２ ０ Ｗ 年 ４ 月

分类 号 学 校代 码 ：

Ｕ２

９２

 ．

１

３

：

密 级  ＵＤＣ ； ６ ５ ６ ． ２

：

８ ３ ８ ０ １
公 开

中 国 铁道 科 学研 究 院
硕 士 学 位 论 文

基 于 深度 强化学 习 的 动 车所 行车调 度 计划编 制 智 能化 研究
Ｒ ｅ ｓ ｅ ａｒ ｃｈｏ ｎｔ ｈ ｅＩ ｎ ｔ ｅ ｌ ｌ ｉ ｇｅ ｎ ｔ ｉ ｚ ａｔ ｉ ｏｎ ｏｆＴ ｒ ａ ｉ ｎ Ｄ ｉ ｓｐ ａ ｔ ｃ ｈ ｉ ｎ ｇ   Ｐ ｌ ａ ｎＧ ｅ ｎ ｅ ｒａ ｔ ｉ ｎ ｇ ｆｏ ｒｔ ｈ ｅ ＥＭＵ Ｄ ｅ ｐ ｏｔ Ｂ ａ ｓ ｅｄ ｏｎＤ ｅ ｅ ｐ Ｒｅ ｉ ｎ ｆｏｒ ｃ ｅ ｍ ｅ ｎ ｔＬ ｅ ａ ｒｎ ｉ ｎ ｇ

作者姓名 曹 达

导 师姓 名 张雪 松



  

申 请学 位 硕 士 研 究 生培 养单 位 中 国 铁道 科 学 研 究 院

学 科专 业 交 通 信 息 工 程 及 控 制研 究 方 向 编 组 站 自 动 化  

答 辩 委 员 会 幸 席 ＾ ＇

评 阅 人 嘈 Ｖ炎伞敗  

论文提交 日 期 ：

年 ２ ０ １ ８

４ 月  

中 国 铁 道科 学 研 宄 院 硕 士学 位 论 文



  

中 国 铁道 科学研 宄 院学 位论文   原 创 性 声 明  

本 人 郑 重 声 明 ： 所 呈 交 的 学 位 论 文 ， 是 本 人 在 导 师 的 指 导 下 ， 独立进 行 研 究 工 作所 取 得 的 成 果 。 除文 中 已 经 注 明 引 用 的 内 容 外 ， 本 论 文 不 含 任何 其 他 个 人或 集体 已 经 发 表 或 撰 写 过 的 作 品 或 成 果 。 对本 文 的 研宄 做 出 重 要 贡 献 的 个 人 和 集 体 ， 均 已 在 文 中 以 明 确   方 式 标 明 。 本 声 明 的 法 律 结 果 由 本人 承 担 。  

＠ ｔ 论文 作者签名 ： 

期 日

：

ｉｄ 年

６月
／

日

关 于 学位 论文 使用 授 权说 明  
作 者 了 解 中 国 铁道 科 学 研 宂 院 研 宂 生 部 有 关 保 留 、 使用 学 位 论 文 的 规 定 ， 即 ：研 宄  
被可论工或识送可 道 文研 研   生在校攻读学位期间 文 作 的知 产权单位属中国铁 科学 宄院。中国铁道科学
究 院 有 权 保 留 并 向 国 家 有 关 部 门 机 构 交 论 文 的 纸 质 档 和 电 子 文 档 ，允 许 学 位 论 文  
遵 段   查 阅和借 阅， 以公 布学位论文 的全部或部分 内容 ， 以采用复 印、缩 印或其他 手  □ 至 保 留 、汇 编 学位 论 文 。 保 密 的学 位 论 文 在 解 密 后 守 此 规 定 。

、以 公 开

保 密 （保 密 期 ： ＿ 年 ＿ ＿ 月 ＿ ＿ 年 ＋ 月 ）

学 位论文作者 签名 ：

导 师 签 名 ： 找

签 字 日期 ：

签 字 曰 期 ：

中国铁道科学研究院硕士学位论文
致谢
在这里，首先要由衷感谢我的导师张雪松老师。张老师在我的论文选题、研究方向 确定、算法思路等方面提供了很大的帮助，为我的科研学习指引了方向。同样，还要由 衷感谢曹桂均老师在动车所业务、CCS 系统和 CTC 系统相关知识、人工智能知识等方 面给我的指导与帮助。可以说，在科研学习方面，能够同时得到两位恩师的指导，我是 幸运的；在生活与为人处世方面，能够同时得到两位恩师的关心与指引，我也是幸运的。 特别是在我论文完成与准备答辩期间，两位恩师不辞辛苦，在百忙之中抽出时间来指导 我的科研与论文工作，让我感动至深。从两位恩师身上，我也学到了踏实努力、不畏艰 难的科研态度和实事求是、认真缜密的科研精神。在今后的工作中，我会谨记两位恩师 对我的谆谆教导，以两位老师为榜样，继续努力。
其次，还要感谢张华副研究员这三年来对我的关心，特别是计算机硬件方面为我提 供的支持与帮助。
感谢林炳跃师兄这三年来对我的指导，他是我编程学习方面的“引路人”，在编程思 想、软件设计等方面给我提供了巨大的帮助。
感谢寇亚洲、刘隽对我论文提出的建议与指导，感谢项目组所有同事在这三年来对 我的照顾与帮助，感谢研究生部的各位老师对我学习生活方面的帮助，感谢我的家人在 我背后的无私奉献与支持。感谢所有帮助过我、鼓励过我的人，你们是我最坚强的后盾！
时光飞逝，转眼间三年的研究生学习生活即将结束。在此，还要感谢中国铁道科学 研究院，能够让我在这度过三年时光，学到知识，不断成长，定会让我铭记今生，受益 终生！
谢谢！

中国铁道科学研究院硕士学位论文
摘要
动车所是动车组日常停放与一级、二级检修的主要场所。随着我国高速铁路的蓬勃 发展，全路动车组数量不断增多，各铁路局所辖动车所陆续建成。动车所内存车线和库 线较多，站场情况复杂，动车所内行车调度计划的好坏，将直接影响到动车所的整体作 业效率，进而影响到所内配属动车组的运用情况。
目前，在动车段（所）控制集中系统（CCS）和 CTC 系统基础上，动车所内进路控 制已经实现了自动化，但行车调度计划仍旧由人工编制完成。为进一步提高动车所的作 业效率，充分发挥动车所检修设备的能力，本文对动车所行车调度计划编制进行了智能 化研究，在深入分析动车所行车调度业务的现状及难点的基础上，基于深度强化学习理 论，设计了一种动车所行车调度计划智能化编制算法，在对算法进行编程实现后编写仿 真软件进行了试验，试验结果表明，算法能够根据当前站场状态及车辆检修需求，自动 编制行车调度计划，算法的可行性得到验证。 关键词：动车所 行车调度 计划编制 人工智能 深度强化学习
I

中国铁道科学研究院硕士学位论文
Abstract
The EMU depot is a place for EMU trains’ parking and level 1&2 maintenance. With the rapid development of China’s high-speed railways, the total number of EMU trains keeps increasing, more and more EMU depots are built by different railway bureaus. The EMU depot usually contains lots of parking tracks, maintenance tracks and devices which are in complex situation. The quality of train dispatching plans would directly decides the efficiency of maintenance work in the EMU depot and has an influence on the use of EMU trains.
At present, based on the EMU depot Centralized Control System(CCS) and Centralized Traffic Control System(CTC), train route has been executed automatically, but the train dispatching plans are still made manually. In order to improve the efficiency of maintenance work and fully utilize the capabilities of maintenance devices in the EMU depot, the intelligentization of train dispatching plan generating was studied in this paper. After a comprehensive analysis of status quo and difficulties of train dispatching work, an intelligent algorithm for generating train dispatching plan was designed based on deep reinforcement learning. Then the algorithm was programmed and an EMU depot simulation software was made to test the algorithm. The test result showed that the algorithm could generate train dispatching plans automatically according to the status of the EMU depot and maintenance requirements of EMU trains. The feasibility of the algorithm was proved.
Key words: EMU Depot, Train Dispatching, Plan Generating, Artificial Intelligence, Deep Reinforcement Learning
II

中国铁道科学研究院硕士学位论文
目录
摘要··········································································································· I Abstract ··································································································· II 1 绪论 ······································································································ 1
1.1 选题背景 ························································································ 1 1.2 研究现状 ························································································ 1 1.3 研究目的及意义 ··············································································· 4 1.4 论文的主要内容与结构 ······································································ 5 2 动车所行车调度问题分析 ··········································································· 7 2.1 动车所行车调度业务与关键问题分析 ···················································· 7 2.2 动车所行车调度及进路控制现状 ·························································· 9 2.3 本章小结 ······················································································· 11 3 人工智能算法简介及算法选型 ····································································13 3.1 常用于调度问题的人工智能算法简介 ··················································· 13 3.2 算法适用性分析与算法选型 ······························································· 14 3.3 本章小结 ······················································································· 14 4 深度强化学习算法介绍 ············································································· 15 4.1 深度学习 ······················································································· 15 4.2 强化学习 ······················································································· 20 4.3 深度强化学习 ················································································· 27 4.4 本章小结 ······················································································· 30 5 动车所行车调度计划智能化算法设计 ··························································· 31 5.1 站场模拟环境设计 ··········································································· 31 5.2 特征归一化处理 ·············································································· 35 5.3 神经网络结构设计 ··········································································· 36 5.4 算法主体流程设计 ··········································································· 38 5.5 本章小结 ······················································································· 40 6 算法实现、仿真与验证 ············································································· 41 6.1 算法实现 ······················································································· 41
III

中国铁道科学研究院硕士学位论文
6.2 仿真软件设计 ················································································· 44 6.3 算法结果与分析 ·············································································· 52 6.4 本章小结 ······················································································· 65 7 总结与展望 ····························································································67 7.1 总结 ····························································································· 67 7.2 展望 ····························································································· 67 参考文献 ·································································································· 69 附录 1 作者简历及科研成果清单··································································73 附录 2 学位论文数据集 ··············································································75
IV

中国铁道科学研究院硕士学位论文
Contents
Abstract ·····································································································I Abstract ··································································································· II 1 Introduction ····························································································· 1
1.1 Background ······················································································ 1 1.2 Status······························································································ 1 1.3 Aim and Significance··········································································· 4 1.4 Main Frame and Structure ····································································· 5 2 Analysis of Train Dispatching in the EMU Depot··············································· 7 2.1 Anaysis of Work and Key Problem of Train Dispatching in the EMU Depot·········· 7 2.2 Status of Train Dispatching and Route Control in the EMU Depot ····················· 9 2.3 Summary ························································································ 11 3 Artificial Intelligence Algorithm Introduction and Algorithm Selection··················13 3.1 Introduction of Intelligent Algorithm Used for Scheduling Problems ·················13 3.2 Algorithm Applicability Analysis and Algorithm Selection ·····························14 3.3 Summary ························································································14 4 Introduction of Deep Reinforcement Learning·················································15 4.1 Deep Learning ·················································································· 15 4.2 Reinforcement Learning ······································································20 4.3 Deep Reinforcement Learning ······························································· 27 4.4 Summary ························································································30 5 Design of Intelligent Algorithm for Train Dispatching Plan ·································31 5.1 Design of Station Simulation Environment ················································31 5.2 Design of Feature Normalization ···························································· 35 5.3 Design of Neural Network Structure ························································ 36 5.4 Design of Main Flow of Algorithm·························································· 38 5.5 Summary ························································································40 6 Algorithm Implementation, Simulation and Verification ····································41 6.1 Algorithm Implementation····································································41
V

中国铁道科学研究院硕士学位论文
6.2 Design of Simulation Software ······························································ 44 6.3 Result and Analysis ············································································ 52 6.4 Summary ························································································65 7 Summary and Prospect ·············································································· 67 7.1 Summary ························································································67 7.2 Prospect··························································································67 References································································································· 69 Appendix1: Author’s Resume and List of Research ············································· 73 Appendix2: Data Sets of Dissertation ······························································· 75
VI

绪论
1 绪论
1.1 选题背景
高速铁路作为中国客运的“动脉”，承担了极其重要的客运职责。高速铁路，是指设 计开行 250 公里/小时（含预留）及以上动车组列车且初期运营速度不小于 200 公里/小 时的铁路客运专线[1]。从 2008 年至今，中国高铁经历了跨越式的发展，取得了令世人瞩 目的成就。截止到 2017 年底，我国铁路运营里程达到 12.7 万公里，其中高速铁路 2.5 万 公里，2017 年累计旅客发送量 308379 万人，动车组承担客运比重接近 50%。
根据《铁路“十三五”发展规划》，到 2020 年，我国铁路总里程计划修建完成 15 万 公里，高速铁路约占五分之一；到 2025 年，铁路网规模达到 17.5 万公里，其中高速铁 路 3.8 万公里[2]。在“四纵四横”高速铁路的基础上继续发展高速铁路和城际铁路，最 终形成以“八纵八横”主通道为骨架，区域连接线为衔接，城际铁路为补充的中国高速 铁路网[3]。
根据规划，高速铁路线路和城际线路全部采用动车组运营[4]。为了实现高速铁路的 安全高效运营，除了保障线路运行正常，动车组车体本身的检修和养护也是一项重要工 作。由于动车组技术的先进性和运用的特殊性，常规车辆段已无法很好的完成动车组的 检修任务，为此，各铁路局专门建设了动车运用所和动车检修基地（动车段），为动车组 的安全运营提供保障。其中，动车运用所主要负责动车组的 1~2 级修（运用检修）和清 洗等日常检修任务，动车检修基地主要负责动车组的 3~4 级修（高级检修）[5]。
另一方面，随着科学技术的不断进步，动车组运用的自动化和现代化也在逐步实现， 如何采用新技术提升动车所调度和检修效率，是当前铁路科研人员主攻的方向之一。日 益增长的动车组维护检修需求，也对动车所行车调度的自动化和智能化水平提出了更高 的要求。
1.2 研究现状
1.2.1 动车所行车调度智能化发展现状
随着高速铁路技术的不断发展，各国的动车组调度指挥系统也在不断完善。国外的 铁路调度系统主要侧重于综合化和集中化，如美国 BNSF 公司的列车管理及调度系统 （TMDS），日本 JR 东日本公司的综合运营管理系统（COSMOS）等[6-8]，均是以 CTC 系统为核心，构建了完整的铁路车辆管理系统[9-10]。但是由于国外高速铁路线路里程较
1

中国铁道科学研究院硕士学位论文
短，动车组数量不多，动车段和动车运用所规模较小，以规模较大的德国柏林动车段为 例，共设有检查库 5 条，存车线 28 条，调度压力较小。
在高速铁路蓬勃发展的宏观背景下，我国的动车组调度指挥系统也有了很大的进步。 目前我国的动车所无论从数量上还是站场规模上，都要比国外的动车所大很多，加之动 车组在动车所内调车作业较多，与动车组在正线和车站间的运行有所区别，因此，我国 动车所内行车调度目前形成了与正线调度集中系统分开管理，互相衔接的模式。动车组 从车站出发接入动车所后，由动车所内部进行统一管理并完成检修任务。
综合来看，国外由于动车所规模较小，配属车辆较少，因此在动车所内行车调度计 划智能化方面需求较小，鲜有专门针对此方面的研究。国内有部分学者对此问题进行过 研究，如：
文献[2]安琪将动车组运用问题转化为 TSP 问题并进行了运用计划的优化研究，但 是仅仅解决了动车组运用问题而没有涉及到行车调度计划。
文献[11]张维皎考虑动车所存车线列位占用相容性，基于模拟退火算法，对动车所 存车线运用方案进行了优化设计，以达到提高存车线利用率的优化目标。
文献[12]曹桂均基于矩阵理论给出了动车所内进路冲突检测的计算检测方法，并进 行了编程实现，提高了调度计划的执行效率。
文献[13]陈韬对动车所通过能力进行了数学建模和量化，提出了动车所调车作业的 优化指标，并采用基于最长活动链的混合邻域禁忌搜索算法，对指标进行了优化。
文献[14]郭晓乐以作业线数目、动车组数目、作业顺序及占用作业线时间为约束条 件，以动车所内动车组总延误时间最小为目标函数，采用微进化算法，对动车所调车计 划编制进行了优化。
文献[15]王家喜在考虑股道占用冲突的基础上，加入了调车进路冲突，对调车作业 计划重新进行了建模，并基于粒子群优化（PSO）算法进行了求解。
文献[16]王忠凯以动车所关键检修线区无效占用时间和调车路径费用为评价指标， 将问题转化为带有时空约束的车间调度问题，采用最大最小蚁群系统求解，验证了算法 的有效性。
文献[17]童佳楠以最后一列动车组检修完成时间最小为优化目标，对动车所一级检 修作业计划建立模型，并将问题转化为带特殊工艺约束的混合 flow-shop 调度问题，采 用遗传算法进行了求解。
文献[18]王忠凯基于调车计划智能优化算法，设计实现了动车组调车计划辅助编制
2

绪论
平台，提高了动车所调车计划编制的效率。 综合已有的研究，目前所有针对动车所调度计划的智能化研究，都只涉及到了存车
线分配与调车计划编制，实际上是将动车所行车调度业务割裂为接发车与调车两个部分； 另一方面，目前算法均采用了基于数学模型的搜索式的算法，对于不同的站场，需要重 新建立数学模型，设计搜索算法，对于多个出入口、站场咽喉占用、平行进路等问题都 没有很好的解决方案，缺乏普适性和实用性。动车所内行车调度的另一关键问题就是， 当站场中的某列车出现了特殊情况，造成了作业延误，如何根据当前站场的实际情况重 新适配调度计划，这一问题目前还没有研究涉及到。
1.2.2 深度强化学习算法发展现状
机器学习是一门综合性学科，它涵盖了概率论与数理统计、逼近论、矩阵论、计算 机编程技术等多个学科的知识，主要研究如何让计算机模拟人类的学习行为，从大量经 验数据中获取知识，形成学习能力的方法[19]，是目前人工智能的核心理论基础。而深度 强化学习算法则是目前机器学习领域的一个最前沿分支，其基础是强化学习 （Reinforcement Learning，RL）和深度学习（Deep Learning，DL）[20]。
强化学习的研究历史比较悠久，早在 1954 年，Minsky 就基于试凑学习提出了强化 学习的概念[21]，在之后的几十年间，强化学习算法的不同分支被逐步提出，如 1988 年 由 Sutton 提出的 TD 算法，1992 年由 Watkins 提出的 Q-Learning 算法，1994 年由 Rummery 提出的 SARSA 算法等等，这些分支的提出，为强化学习的不断发展提供了思路[22]。
深度学习的基础是人工神经网络，可以将其理解为更多隐层和更多神经元数目的神 经网络结构。复杂的神经网络需要极强的计算能力，早年受制于计算机硬件技术和梯度 弥散问题，人工神经网络一直没有突破性的进展，随着硬件技术的不断发展，近几年计 算机的计算能力取得了巨大的提升，尤其是 GPU 计算技术的发展，人工神经网络重新 开始受到人们的关注。2006 年，Hinton 提出了逐层训练的方法，解决了多隐层神经网络 的梯度弥散问题。之后，随着 TensorFlow、Caffe、Keras、Theano、Torch、MXNet 等一 大批深度学习框架的诞生[23]，深度学习在学习效率、计算资源利用率、用户易用性等方 面都取得了长足的进步。
2013 年，DeepMind 团队在 NIPS 会议上发表 DQN 算法[24]，完成了强化学习和深度 学习的结合，自此诞生了深度强化学习（Deep Reinforcement Learning，DRL），DQN 算 法采用经验回放技术增加了数据的使用效率，减少了数据间的相关性，随后，Double
3

中国铁道科学研究院硕士学位论文
DQN[25]、DPG[26]、DDPG[27]、A3C[28]等各种深度强化学习算法被提出，在围棋对弈、视 频游戏、连续动作控制等方面取得了前所未有的学习效果，成为近几年机器学习领域的 一个前沿方向[29]。
1.3 研究目的及意义
从 2004 年开始引进国外技术并联合设计生产第一代高速动车组开始，我国的高速 铁路事业近十几年经历了跨越式的大发展，全路动车组数量增速惊人，且从兼容第一代 引进的 CRH-1、CRH-2、CRH-3、CHR-5，到第二代动车组 CRH380 系列，再到完全自 主化的第三代 CR400 系列动车组，中国高速铁路调度系统的复杂度已远远超越世界各 国，配套的动车段和动车所无论在数量上还是站场规模上也远超各国。目前，全路共有 12 个动车段，53 个动车所建成并投入使用，共配属动车组 2869 组。我国的动车所站场 规模普遍较大，以贵阳北动车所为例，共设存车线 53 条，洗车线 3 条，临修库 1 条， 不落轮镟库一条，检修库 14 条，且所有存车线和检修库线均为双列位，可同时停放两 列 8 编组的动车组；动车段的规模则更加庞大，武汉动车段仅存车线就达到了 72 条。
根据《中长期铁路网规划》，我国将在未来继续大力发展高速铁路和城际铁路，动车 所数量和规模还将呈现继续增长的态势。我国动车所行车调度系统发展可以分为两个阶 段，在早期，动车所行车调度系统主要由 CTC+联锁系统组成，其中动车所内 CTC 系统 接入铁路局的 CTC 调度中心，由调度中心的行车调度员负责编制列车（接发车）计划， 并完成接发车进路的自动控制，计算机联锁系统接收 CTC 系统的进路指令并执行；由 于动车所内调车作业繁多且比较复杂，CTC 系统在所内调车作业中不具备进路自动控制 与动车组位置追踪功能，而调车作业在动车所行车调度业务中占的比重非常大，为解决 这一问题，动车段（所）控制集中系统（CCS）应运而生，该系统通过接收 CTC 调度中 心制定的列车计划和动车所车辆检修调度制定的调车计划，实现进路的按计划控制和自 动排路，从而实现了动车所内进路的自动控制[30]。目前，全路多个动车段和动车所都已 开始采用该系统，铁路总公司也出台了相应的技术条件，未来会有更多的动车所开始应 用这套系统。
但是，目前无论是以 CTC 为核心，还是以 CCS 系统为核心，其列车计划和调车计 划都由调度人员人工制定，既浪费了人力，又无法达到效率最优。对于站场规模较大的 动车所，特别是在春运等铁路客运最为繁忙的时期，动车所内调度计划的编制质量，是 动车所检修效率最关键的影响因素。动车所车辆进路的自动控制系统，就像一个人的手
4

绪论
臂，可以很精确的执行动作，但是无法像大脑一样判断并做出决策。 因此，实现动车所行车调度智能化，将会极大的提高动车所的生产效率，最大化动
车所检修设备的利用率。对于动车段而言，由于其主要的业务流程与动车所基本类似， 只是存车线和检修库数量更多，可以看作是一个更大规模的动车所，具有普适性的动车 所行车调度算法，将可以直接应用于动车段行车调度业务中，实现全路动车检修基地的 调度智能化。动车所行车调度效率的提升，将为全路动车组提供强有力的安全保障，为 提升我国高速铁路客运能力提供强有力的支撑，由此可见，研究动车所行车调度智能化 算法，具有非常重要的意义和研究价值。
1.4 论文的主要内容与结构
本文在前人已有研究的基础上，对动车所行车调度业务的主要流程和重点难点进行 了深入分析；在此基础上，分析了不同算法对此问题的适用性，并最终选定了一种合适 的算法深入研究，提出解决此问题的方法；对算法进行了编程实现和模拟仿真，验证了 算法的有效性；最后，对全文进行总结，并探讨当前算法仍旧未能解决的问题，为以后 的研究指引方向。整篇论文的结构如下：
第一章 绪论。本章主要介绍了本文的选题背景，介绍了动车所行车调度系统的发 展现状和深度强化学习算法的研究现状，分析了动车所行车调度智能化研究的意义，并 对研究的主要内容和方向进行了概括。
第二章 动车所行车调度问题分析。本章主要对动车所行车调度业务的主要内容、 重点难点进行分析，并对我国动车所目前主流的 CCS 系统进行介绍，细化本文要解决 的主要问题。
第三章 人工智能算法简介及算法选型。本章主要介绍常用于解决调度问题的各种 人工智能算法，并分析算法对本文问题的适用性，最终确定一种合适的算法。
第四章 深度强化学习算法介绍。本章主要介绍深度学习、强化学习的主要思想，以 及如何结合二者形成实现深度强化学习算法，并以伪代码的形式给出算法解决问题的一 般思路。
第五章 动车所行车调度计划智能化算法设计。本章以深度强化学习算法为基础， 对动车所站场进行建模，设计了一种动车所行车调度计划智能化编制的算法。
第六章 算法实现、仿真及结果分析。本章在上一章算法设计的基础上，对算法进行 了编程实现，编制了仿真软件，对算法进行仿真，并对仿真结果进行分析。
5

中国铁道科学研究院硕士学位论文
第七章 结论与展望。总结全文，分析算法的优点与局限性，为以后的研究提供参 考。
6

动车所行车调度问题分析
2 动车所行车调度问题分析
2.1 动车所行车调度业务与关键问题分析 目前国内的动车所都为尽头式站场，虽然各个动车所的站场形状各异，但是其主要
业务流程基本相同。以下图为典型示例分析动车所行车调度业务。
图 2.1 动车所站场示意图 Figure 2.1 Diagram of an EMU Depot
如图所示，从车辆进入动车所开始，按照车辆占用区段的先后顺序，动车所站场可 以划分为七个部分：出入口，咽喉一，存车线，咽喉二，洗车线，咽喉三，检修库。其 中，出入口、存车线、洗车线和检修库作为动车所行车调度计划的起始或目的股道，咽 喉一、咽喉二和咽喉三在车辆走行过程中被占用。进入动车所的动车组按照检修作业需 求安排调度计划，例如，某一列动车组仅仅需要停车过夜，动车所值班员只需要为其指 定一条存车线，并按照行调指定的接发车时间和线别完成接发车作业即可；另一列动车 组需要进行洗车和检修作业，其调度作业流程为：首先按时完成接车作业，将动车组在 存车线停稳，之后，根据洗车线和检修库的占用情况以及所经过的咽喉的占用情况，在 合适的时机进行调车作业，完成动车组清洗和检修需求，完成作业后回存车线停车，最 后按时完成发车作业。
不同车组的停车、清洗、检修、镟轮作业需求各不相同，因此调度计划需要根据每 列车的作业需求和接发车时间点进行综合安排。要实现调度计划自动化编制，保证各车 组能够按时完成接发车和检修作业，需要解决以下关键问题：
7

中国铁道科学研究院硕士学位论文
(1) 从建模角度看，动车所一类的铁路站场内行车调度问题类似于车间调度问题（JSP） [31-33]，即把动车组看作是工件，存车线、洗车线、检修库等作业线看作机器，由于不 同动车组检修需求不同，且其接发车时间有严格的规定，因此，行车调度问题是一类 “带有时间限制的多种工件调度”的车间调度问题，其复杂度将远远超过 JSP 问题， 同样面临着组合爆炸、作业线占用冲突等情况，因此也是一种 NP-Hard 问题。
(2) 站场设备的时空占用相容性。除了在问题 1 中就已经包含的作业线占用冲突问题外， 由于铁路站场的特殊性，动车所行车调度还存在另一类问题，即咽喉占用冲突问题。 以咽喉一为例，由于该站场有两个出入口，因此咽喉一又可分为上半部分和下半部 分。当某列车从出入口 1 接车到 5、7、9、11、13 中的任何一条存车线时，咽喉一的 上半部分被占用，下半部分是空闲的，可以完成出入口 2 与存车线 6、8、10、12、 14 间的接车或发车作业，即可以排通“平行进路”；当某列车从出入口 1 接车到 6、 8、10、12、14 中的任何一条存车线时，咽喉一的上下两部分将全部被占用，出入口 1 与所有存车线间的接发车作业将全部无法执行，即出现了“进路冲突”。而依据经 验，完成一钩接车作业的时间为 10-20 分钟，因此，算法对车辆走行过程中的咽喉占 用以何种方式处理，即是否允许平行进路，将对算法复杂度和作业总时间产生很大 的影响，二者需要取一个平衡状态。
(3) 动车所内业务繁忙，行车调度作业过程中经常会遇到很多不确定的状况，比如动车 组走行过程中意外停车，从而造成后续作业的延误，或单纯由于时间误差的累积造 成后续计划不能按期执行。因此，动车所行车调度计划智能化面临的另一个关键问 题是，如何在当前计划已经无法正常执行的情况下，能够在短时间内依据当前站场 状态重新编制一版新的计划，这对算法的性能和稳定性提出了很高的要求。
(4) 算法对动车所站场的适应性也是算法设计时需要重点考虑的问题。由于各个动车所 规模不同，各条作业线的排布方式也可能不同，如有些动车所的镟轮线与检修库类 型相同，位于站场的尽头，而有些动车所则采用了通过式的镟轮线，有些动车所如沈 阳北动车所，还设有专门针对高寒地区动车组的融冰融雪线。提高算法对于不同站 场的适应性，对于系统生产厂家具有很重要的商业意义。 以上四个关键问题，既是动车所行车调度业务中的重点难点，也是既有算法没能很
好解决的问题。
8

动车所行车调度问题分析
2.2 动车所行车调度及进路控制现状 在已经配备了 CCS 系统的动车所中，行车调度及进路控制业务主要由 CCS 系统及
其他系统配合完成，其输入输出关系大致如下图所示：

CTC中心

列车计划

调车建议 计划

CCS

动车所调度

按

码位

钮

及其

指

他信

令

息

联锁

图 2.2 我国动车所行车调度及进路控制相关系统主要结构图 Figure 2.2 Diagram of the Main Structure of Train Dispatching and Route Control System in the EMU Depot
of Our Country
CCS 系统作为动车所内行车调度的最上层，承担了最主要的调度任务，是车站值班 人员日常关注与操作最多的子系统[34]。CCS 系统结构如下图所示[35]：

图 2.3 CCS 系统主要结构图 Figure 2.3 Diagram of the Main Structure of the CCS System
9

中国铁道科学研究院硕士学位论文
系统各部分的功能如下： (1) 应用服务器 应用服务器中的软件包括自动执行软件和位置追踪软件，自动执行软件完成车辆进
路的优选、调整和进路控制命令自动下达，位置追踪软件接收识别定位服务器的位置数 据并匹配至站场图中，完成动车组位置的自动追踪并向各终端提供位置追踪数据。
(2) 识别定位服务器 通过与车号识别摄像头、列位传感器等定位装置的数据交互，完成动车组车号、型 号、编组类型等信息的识别与车组定位，位置数据发送给位置追踪软件。 (3) 网管服务器 监控系统局域网内所有计算机的通信状态，完成局域网内病毒防护等功能。 (4) 数据库服务器 用于保存各操作终端和服务器端软件的运行数据、运行记录，并承担一部分软件间 的数据交互任务。 (5) CTC 接口服务器 完成 CCS 系统与 CTC 系统的信息交互、时钟同步和协议转换等工作。 (6) 信息接口服务器 完成 CCS 系统与动车组信息管理系统的信息交互工作。 (7) 车站调度员终端 完成调车计划的接收、匹配和转发功能，具备调车计划编制、调整、下达和查询现 存车功能。 (8) 值班员终端 完成列车计划接收、存车股道与列位分配和调车计划调整功能，完成调度命令的接 收与签收；办理发车预告，完成计划和指令方案的展示、调整与进路下达，完成作业过 程监视、列车和调车日志管理等功能。 (9) 信号员终端 完成动车所站场设备实时状态显示、动车组车号与位置显示、信号设备操作、邻站 站场设备与车组实时状态显示等功能。 (10) 复示终端 向动车所调度复示站场各种状态信息。 (11) 大屏综合表示工作站
10

动车所行车调度问题分析
向动车所大屏幕综合调度表示墙提供实时站场信息。 (12) 电务维护终端 接入信号集中监测系统，记录 CCS 系统各软件和设备的工作状态与关键操作，并 提供记录查询、分析统计、操作回放等功能。 以 CCS 系统为核心的动车所行车调度系统，已能够完成依据列车和调车建议计划 实现进路自动控制、列车位置追踪、计划报点和记录查询回放等各项功能[36]，但仅仅是 完成了调度业务中计划执行的这一环，在计划制定方面，目前各动车所都由动调值班员 专门负责。由人工制定计划只能依靠经验，在站场规模较为庞大的动车所，动调值班员 的工作压力较大，且往往会成为影响动车所检修作业效率的最关键因素，在计划执行过 程中遇到突发情况或计划制定的有错误，还需要动调重新制定一版计划并重新输入 CCS 系统。 综上分析，可以总结出动车所行车调度计划智能化研究的主要需求：根据动车组检 修需求和接发车时间点，自动编制合适的调度计划，并可在短时间内根据当前站场情况 重新编制新的调度计划。 2.3 本章小结 本章通过对动车所行车调度业务的分析，从算法复杂度、铁路调度特殊性、计划动 态调整、算法适应性四个角度总结了动车所行车调度业务的重点难点问题，并结合 CCS 系统的应用现状，提出了动车所行车调度计划智能化的具体需求，完成了工程项目一般 流程中的问题分析与需求设计阶段。
11

中国铁道科学研究院硕士学位论文
12

人工智能算法简介及算法选型
3 人工智能算法简介及算法选型
3.1 常用于调度问题的人工智能算法简介
从上文的动车所行车调度系统业务分析可以看出，动车所行车调度问题可以归结为 车间调度问题，在已有的研究中，已经有很多人工智能算法被应用于解决此类问题，下 面对其中几种有代表性的算法进行简要介绍。
(1) 遗传算法 遗传算法的基本思想基于 Darwin 的进化理论和 Mendel 的遗传理论，是一种基于群 体寻优的优化方法。其基本运算过程为：首先采用随机方法或其他方法产生一个初始种 群，并根据要优化的目标函数，构造一个合适的适值函数用来量化个体对于环境的适应 能力，采用合适的编码方法对基因进行编码后，通过“交叉”和“变异”两种遗传算子， 以适值函数的值为依据进行选择和种群繁殖，迭代若干代以后，适应值最高的个体即为 问题的最优解。遗传算法是一种在策略空间直接搜索的算法，相对于直接遍历的方法， 提高了搜索效率。 (2) 蚁群算法 蚁群算法的基本思想来源于蚂蚁觅食时的路径寻优，以“信息素”作为路径长度的 量化指标，Agent 在搜索过程中不断获取当前路径上累积的“信息素”，并继续释放与该 路径长度有关的“信息素”，路径越短，则其上经过的 Agent 越多，累积的“信息素”也 越多，提高最短路径被选中的概率，从而形成正反馈机制，加速算法的搜索过程。采用 蚁群算法，有助于解决遗传算法在调度问题上的不稳定性，但是正反馈的出现，也可能 使算法出现停滞现象。因此，Agent 在选择路径时除了参考“信息素”，还要进行 Agent 间的交流，以避免算法进入局部最优。 (3) 粒子群算法 粒子群算法的基本思想来源于鸟类觅食时的搜寻路径优化，每只鸟在飞行时，按一 定权重结合自身历史最优位置和群体历史最优位置去确定目标位置，最终搜索到食物。 粒子群算法通过调节这一权重值，还可以达到前期趋向于探索，避免陷入局部最优，后 期再加速收敛的效果。相较于遗传算法，粒子群算法结构更加简单，收敛速度也更快。 (4) 强化学习 强化学习算法与以上几种算法不同，更加注重于 Agent 与环境的交互，其基本思路 来源于动物的学习过程：环境当前处在状态空间中的某一种状态，Agent 进行某一动作
13

中国铁道科学研究院硕士学位论文
以后，环境变为状态空间中的另一种状态，并给出奖励，Agent 根据奖励和当前环境的 状态，从动作空间中选择下一个动作，使奖赏累积达到最大。经过一段时间的学习后， 强化学习算法会建立状态与动作的映射关系，并存储于记忆库中[21]。当学习达到预期的 目标后，直接向记忆库输入状态，即可输出最优的动作。强化学习模型不需要对环境建 立精确的数学模型，降低了算法复杂度，另一方面，它与其他的监督学习算法又不同， 它的奖励信号由环境给出而不是“监督者”，因此能够适应外部环境的变化，尤其适用于 “监督者”也无法直接给出优化方向的问题。由于算法本身与模拟环境分离的较为彻底， 因此强化学习具有更强的适应性和通用性，是当今控制算法研究的热门方向。
3.2 算法适用性分析与算法选型 上述的几种算法中，前三种都有一个共同特点：它们都能够对问题进行数学建模，
并给出明确的优化目标函数和边界条件，然后直接从策略空间上进行搜索，通过群体搜 索的方式提高搜索效率，最终直接给出策略结果。这个共同特点也决定了此类算法的一 些局限性：在面对连续策略空间或高维离散空间时，由于空间不便给出或过于庞大，算 法可能会不稳定或效率不高，一方面可能无法求得最优解，另一方面求得最优解可能需 要很长的时间，在计划出现延误需要短时间内重新生成计划时，算法需要花费大量时间 重新搜索最优结果；对于不同的环境，算法也需要重新编写。强化学习算法则避开了这 些问题，在决策效率和适应性方面更具优势。
综合第二章对动车所行车调度业务的分析，强化学习算法更加适合解决这一问题。 但是，由于动车所行车调度问题，属于高维状态空间和大规模离散动作空间下的多步决 策问题，其奖励存在滞后，因此，本文将采用以强化学习为基础的深度强化学习算法。 下一章将对深度强化学习的原理进行详细的介绍。
3.3 本章小结 本章对现有常用于解决调度类问题的各种算法进行了介绍，并结合上一章所提出的
重点难点与需求设计，分析了各个算法对本文问题的适应性，并最终选定深度强化学习 算法作为本文的重点研究方向，完成了工程项目一般流程中的方案设计阶段。
14

深度强化学习算法介绍
4 深度强化学习算法介绍
深度强化学习的理论基础是深度学习和强化学习，为了能深入理解深度强化学习理 论，本章首先介绍深度学习和强化学习的理论基础，之后再逐步介绍深度强化学习的思 路与具体步骤。
4.1 深度学习 4.1.1 神经元模型
神经网络（neural network，NN）是一种仿照生物神经网络结构的算法模型，其最基 本组成单元是“神经元”[37]。在生物神经网络中，各神经元之间相互连接，并通过互相 传递化学物质来改变其他神经元的电位，当电位超过某个特定的阈值，神经元即被“激 活”。

图 4.1 生物神经元 Figure 4.1 Biological Neurons

在人工神经网络中，神经元模型式生物神经元的数学形式模型，如下图所示：

x1 1

x2 2

x3

3 4

i Sum

f

y

x4

图 4.2 人工神经元模型

Figure 4.2 Model of Artificial Neurons

其中，x1  x4 为来自其他神经元的输入，经过系数为1 ~4和偏置（阈值）为1 ~ 4
n
 的加权求和后，输入激活函数 f ( x ) ，最终输出 y  f ( i xi i ) 。激活函数将实数集映 i1

射到（0,1）空间中，作用是将多个输入信号的判断结果进行综合，最终决定该神经元是

15

中国铁道科学研究院硕士学位论文

否被激活，常见的激活函数如下图所示：

ReLu

Sigmoid

10

1

8

0.8

6

0.6

4

0.4

2

0.2

-15

-10

0

-5

0

0

5

10

15 -15

-10

-5

0

图 4.3 常用的激活函数

Figure 4.3 Common Activation Functions

5

10

15

4.1.2 单层神经网络

具有多个神经元的单层神经网络（又称感知器）如下图（a）所示，x1 ~ x4 四个维度 的输入量输入具有 5 个神经元的单层神经网络后，会产生五个输出 y1 ~ y5 ，由上文的神
经元模型结构可以分析，每一个神经元实际上是完成了一次线性分类任务，我们用“决

策分界”来描述分类效果，当数据维度为 2 时，其决策分界如下图（b）所示，是一条直

线，当维度为 3 时，决策分界是一个平面，当维度为 n 时，决策维度是一个 n-1 维的超

平面。因此，单层神经网络只能作为线性分类器使用。

x1

y1

x1

y2
x 2
y3
x3
y4

x4

y5

(a)

(b)

x2

图 4.4 单层神经网络

Figure 4.4 Neural Network with Single Layer

单层神经网络的训练也比较简单，给定训练集后只需要训练合适的权重值i 和偏置 值i 即可。对于更一般的情况，偏置值可以以一个权重值恒为-1 的偏置节点代替，以便

将权重值和偏置值统一进行训练：对于训练样例 ( x, y) ，当前神经元的输出为 yˆ ，则：

i i i

(4.1)

16

深度强化学习算法介绍

i (y yˆ)xi

(4.2)

其中 为训练的效率，又称“学习率”。

4.1.3 深度神经网络

深度神经网络（deep neural network，DNN），本质上就是多隐层神经网络。由上文

分析我们知道，想要完成非线性分类任务，即完成对非线性函数的拟合，需要增加神经

网络的层数，在层间完成多次数据空间的变换，完成输入和输出的非线性映射关系。但

是随着隐层数量的增加，神经网络的计算量和复杂度增长迅速，要求计算机具备极强的

运算能力，这也是多年以来制约人工神经网络发展的因素之一。

多层神经网络的拟合能力要比单层神经网络强大的多，其训练规则也更为复杂。误

差逆传播（error backpropagation，BP）算法是其中最成功的例子。下面我们以一个两层

（单隐层）神经网络为例说明 BP 算法如何进行训练。

给定训练样例集合为 D {(x1, y1),(x2, y2),...,(xm, ym)}, xi d , yi l ) ，即输入 d 维，

输出 l 维。现建立一个两层神经网络，为匹配训练样例的输入输出维度，神经网络输入

层神经元总个数为 d ，索引值为 i ；隐层神经元总个数为 q ，第 h 个神经元与输入层第 i

个神经元的连接权重值为ih，偏置值为  h ；输出层神经元总个数为 l ，索引值为 j，第 j

个神经元与隐层第 h 个神经元的连接权重值为h ，偏置值为 j ；激活函数使用 Sigmoid

函数

f

(x)



1 1 ex

，网络结构如下图所示：

ih

hj

x1

y1

x 2

y2

x3

y3

x4

y4

图 4.5 两层神经网络 Figure 4.5 Neural Network with Double Layers

隐层神经元的输入为：

隐层神经元的输出为：

d
 h  ih xi i1
bh  f (h   h )

(4.3) (4.4)

17

中国铁道科学研究院硕士学位论文

输出层神经元的输入为：

q
  j  hkbh h1

(4.5)

输出层神经元的输出为：

yˆ j  f ( j  j )

(4.6)

对训练样例 (xk , yk ) ，均方误差为：

 Ek



1 2

l

(

yˆ

k j

j 1



y

k j

)2

(4.7)

BP 算法采用类似于单层神经网络的参数更新方式进行更新估计，区别在于参数变化值

是基于梯度下降策略进行计算的：对于给定学习率 ，输出层与隐藏层的连接权重变化

值为：

hj





Ek hj

根据变量的逆推关系，有：

Ek



Ek



yˆ

k j





j

hj

yˆ

k j

 j

hj

由式(4.5)，有：

 j hj

 bh

激活函数为 Sigmoid 函数，则：

f (x)  f (x)(1 f (x))

由式(4.6)和式(4.7)，可得：

gj



 Ek

yˆ

k j



yˆ

k j

 j



(

yˆ

k j



y

k j

)

f

(

j



j

)



yˆ

k j

(1



yˆ

k j

)(

y

k j



yˆ

k j

)

由式(4.10)和式(4.12)及式(4.8)，可得：

hj  g jbh

同理可得：

其中：

 j  g j ih eh xi  h  eh

(4.8)
(4.9)
(4.10) (4.11)
(4.12)
(4.13) (4.14) (4.15) (4.16)

18

深度强化学习算法介绍

eh





Ek bh

 bh  h

 



l j 1

Ek  j

  j bh

f

( h

h)

l
  hj g j f (h   h ) j 1

l
  bh (1 bh ) hj g j j 1

(4.17)

以上为 BP 算法的数学推到过程，下面以伪代码的形式来说明 BP 算法的程序实现：

Input：training set D  {(xk , yk )}km1

target accumulated error E0 Define：learning rate 

Procedure：

Randomly initialize all the weights and bias of the network within (0,1) while true:

for all (xk , yk )  D do Calculate yˆk

Calculate gi

Calculate eh

Update hj ,ih and  j ,h

end for

 if

1 m

m k 1

Ek



E0

break

Output：all the weights and bias of the network

图 4.6 BP 算法

Figure 4.6 BP Algorithm

当神经网络的隐层数量继续增加时，误差在逆传播过程中可能会发散而使算法无法

收敛，因此，深度神经网络会采用更加复杂的训练过程，如无监督逐层训练、连接权共

享等策略来提高训练效率[38]。随着计算机浮点计算能力的不断增强，尤其是近几年依靠

GPU 计算能力的爆发式增强和新型神经网络结构的出现如卷积神经网络（CNN）、循环

神经网络（RNN）等，深度神经网络已经可以实现非常复杂的非线性分类器[39-42]。

19

中国铁道科学研究院硕士学位论文
4.2 强化学习
在第三章中，我们已经对强化学习算法进行了初步介绍，在本节中，我们将对强化 学习的理论基础进行详细的说明。
按照训练过程的不同，机器学习算法可以分为三类：监督学习（supervised learning， SL），无监督学习（unsupervised learning，UL）和强化学习。
监督学习的训练过程如同教师教授学生知识：“教师”事先对大量的样本进行标记 并构成训练集，以样本作为输入量，由“学生”（要训练的算法模型）计算并输出一个结 果，然后计算输出结果与标签的差别，并以此误差为依据修改算法中的参数，最终达到 误差最小的状态，上一节中通过训练神经网络实现非线性分类器的过程，就是典型的监 督学习过程。
与之对应的是无监督学习，就是所有的训练样本都没有标签，即训练过程中不存在 “教师”，因此，无监督学习是不能计算误差并进行误差反向传递的。无监督学习要解决 的另一类问题：聚类问题。将大量多特征的样本输入算法，算法对样本特征进行处理后， 找到某些样本在某些方面出现的“共性”，并按照算法“理解”出的“共性”对样本进行 分类。常见算法如 K 均值聚类算法、受限玻尔兹曼机等都属于无监督学习算法。
强化学习与以上两类算法都不同：强化学习要解决的问题是决策问题，即学习一种 算法，以环境的某一状态为输入，通过决策过程，输出一个最优的动作。实际上，强化 学习也可以理解为某种意义上的分类算法：它完成了不同的状态与不同的动作之间的映 射关系，确定哪些状态适合采取某一动作。需要注意的是，强化学习不存在带有标签的 训练样本和训练集，它的状态来自于当前的环境，并在执行动作后产生新的状态。强化 学习的另一特点是，它同样没有“教师”这一角色，算法开发者也不知道优化方向，但 是又不同于非监督学习，它由环境对每一步的动作给出单步奖励，算法通过优化累积奖 励，达到学习的目的。
4.2.1 MDP 基础
强化学习任务通常用马尔可夫决策过程（markov decision process，MDP）来描述。 首先我们引入马尔可夫性这一概念：动作发生后，如果系统的下一状态与系统状态的变 化历史无关，而只由当前状态和动作决定，则称这类系统具有无后效性，即马尔可夫性， 其公式表达为：P[xt1 | xt ]  P[xt1 | xk ], k  1,...,t 。适用于强化学习的系统，首先需要满足 马尔可夫性。
20

深度强化学习算法介绍

强化学习的基本流程如下图所示：Agent 处于环境 E 中， E 的所有可能状态 x构成 状态空间 X ，Agent 的所有可能发出的动作 a构成动作空间 A ，在某一时刻，Agent 观察 到环境所处的状态 xt 后，根据某种策略 从动作空间 X 中选择最优动作 a   (x) ，动作 a执行后，环境 E 的状态转变为其他状态的概率为 P ，获得的奖励为 R 。我们称这种决 策过程为马尔可夫决策过程[43-46]。

Agent

State Reward

Action

Environment
图 4.7 强化学习迭代过程 Figure 4.7 Iteration Process of Reinforcement Learning
智能体在环境中执行多步动作得到完整的执行轨迹，这条由多个状态-动作对组合而 成的轨迹称为马尔可夫链，如下图所示：

…

…

x0 a0 x1 x a

x a
图 4.8 马尔可夫链示意图 Figure 4.8 Diagram of Markov Chain
由 MDP 的定义，强化学习过程可以用元组 E  X , A, P, R 进行描述。在实际求解中，

Agent 常常需要进行多步动作才能达到某一目标，此时学习的目的是找到步数更少代价

更小的策略组合，因此，在计算累积奖赏时，还常常在单步奖励的基础上乘以该步数所 对应的折扣因子 。

4.2.2 有模型学习
在强化学习过程中，当动作执行后环境转移到某一状态的概率是已知的，该动作对 应的奖励也是已知的，并且 Agent 在学习过程中是已知这两个因素并且能够对其建立确 定的数学表达的，这样的强化学习我们称之为“有模型学习”（model-based learning）。 有模型学习有两个重要的步骤：策略评估和策略改进。

21

中国铁道科学研究院硕士学位论文

策略评估是对策略好坏进行量化评判的过程。现定义V (x)为“状态值函数”，代表

在状态 x时策略 的累积奖励，则：

Gt  Rt1   Rt2  ...   T 1RT

 VT (x) 


E

1 T

T t 1

rt

| x0



x ,

T 步累积奖励

V (x)  E Gt | x0  x，  折扣累积奖励

(4.18) (4.19)

定义 Q (x, a) 为“状态-动作值函数”，代表在状态 x 执行动作 a后，策略  的累积奖励，

则有：

 QT (x, a) 


E

1 T

T t 1

rt

| x0



x, a0



a ,T步累积奖励

 

Q

(x, a)



E

Gt

|

x0



x, a0



0，

 折扣累积奖励

(4.20)

由式(4.18)、(4.19)及状态-动作值函数的定义，可以推导出以下关系：

 VT (x)   (x, a)Q (x, a) aA

(4.21)

由于强化学习过程具有马尔可夫性，由当前状态和动作即可决定下一状态，因此状态值

函数可以以递归形式拆解，以T 步累积奖励为例：

 VT (x)  E

1 T

T
rt
t 1

| x0



x

 

E

1 T

r1



T 1 1 T T 1

T t2

rt

|

x0



x

   

 (x, a)

aA

xX

Pxa

x

  

1 T

Ra xx



T 1 T E

1 T 1

T -1 t 1

rt

|

x0



x

  

(4.22)

  =  (x, a)

aA

xX

Pa xx

1  T

Ra xx



T T

1VT1

(

x)

 

同理可推导T 步折扣累积奖励为：

    V (x)=  (x, a)

Pa x x

Ra x x



 V

( x)

aA

xX

(4.23)

根据以上迭代公式对状态空间的每个状态进行迭代，直至环境的初始状态，反过来从环

境初始状态出发迭代计算即可获得状态值函数。并由(4.21)可以推导出状态-动作值函数

的迭代过程为：

  QT


(x,

a)



Rxa



T 1 T

xX

Pa x x


aA

( x,

a)Q

( x,

a),

T 步累积奖励

  Q (x, a)  Rxa  

Pa xx

 (x, a)Q (x, a)，

 折扣累积奖励



xX

aA

(4.24)

式 (4.24)确定了状态值函数和状态-动作值函数的迭代关系，又称 Bellman 期望方程[47]。

22

深度强化学习算法介绍

策略改进是根据策略评估的结果，对策略进行优化的过程。强化学习的最终目标是

找到一个理想策略，是累积奖励最大：

  *  arg max V  (x)



xX

(4.25)

有时一个任务可能存在多个理想策略，设理想策略对应的状态值函数为V * ，称为最优值

函数，则：

x  X :V *(x)  V * (x)

(4.26)

此时策略的累积奖励为最大值，因此，对 Bellman 期望方程进行修改，将递归求和改为

求最优解：

 VT* ( x)




max aA

xX

Pa xx

1  T

Ra xx



T T

1VT1

(

x)

 

   V*(x)



max aA

xX

Pa xx

Ra xx

 V

( x)

(4.27)

将其概括为：

V *(x)  max Q* (x, a) aA

(4.28)

将式(4.27)代入式(4.21)可得：

 QT* (x, a)




1 T

Rxa



T 1 T xX

Pa xx

max
aA

QT*

-1

(

x,

a)

 Q* ( x,


a)



Rxa



xX

Pa xx

max
aA

QT*-1( x,

a)

(4.29)

相应的，式 (4.29)称为最优 Bellman 方程，它描述了学习过程中策略改进的方向：选择

当前最优动作。在数学上可证明V (x) 相对于策略优化方向是单调递增的，则在策略搜

寻过程中：
 (x)  arg max Q (x, a)
aA
当  与 相同时，状态值函数不在增长，此时的 即理想策略。

(4.30)

在策略评估和策略改进的基础上，我们可以总结有模型学习的策略迭代过程：从一

个随机初始化的策略开始，不断的进行策略评估与策略改进，直到策略满足式(4.30)时，

迭代结束并输出理想策略。而由状态值函数的单调性可知，策略改进的方向与值函数改

进的方向是相同的，因此，策略改进也可以变为值函数改进的形式，策略迭代的过程也

可以变为值迭代的形式：随机初始化一个策略，计算状态值函数进行策略评估后进行策

略改进，直到状态值函数不再增长或增长差值小于一个预先设定好的阈值，停止迭代，

此时的策略作为理想策略。改成值迭代的好处是，避免了在某些学习任务中，式(4.30)的

判断不便于进行准确的数学表达，可能在迭代过程中错过最接近的理想策略而无法及时

23

中断迭代。

中国铁道科学研究院硕士学位论文

4.2.3 无模型学习

在实际问题中，环境常常无法建模，状态转移函数和奖励函数等可能无法建立确定

的数学表达，解决这类无法建模的马尔可夫决策问题，是强化学习的精髓所在，此类学

习称为“无模型学习”（model-free learning）。

由于无法对环境建立确定的数学模型，按照有模型学习的思路进行学习时，首先要

面对的问题就是策略评估无法进行，在模型未知的时候，无法得知策略的状态转移概率

分布，状态的转移和奖励只能在动作实际执行了以后观察环境得出。另一方面，策略估

计和改进时用到的状态值函数将无法直接转换为状态-动作值函数，从而使迭代过程无

法顺利执行。一种解决思路是采用多次采样的方式，通过不断的执行动作和观察，并以

多次动作的平均累积奖励值作为累积奖励期望值的近似，这种方法称为“蒙特卡罗”方

法。

蒙特卡罗学习方法的基础是“采样”，Agent 从一个初试状态出发进行探索，并记录

完整的执行轨迹： x0 , a0 , r1, x1, a1, r2 ,..., xT 1, aT 1, rT ，对轨迹中每一对状态-动作，以其后

的奖励之和作为该状态-动作对的累积奖励，通过采集多条轨迹后，即可得到状态-动作

值函数的近似估计。其状态值函数更新方式为[48]：

  V

(x, a)



V

(x,

a)



1 N

Gt V (x, a)

  Q

(x,

a)



Q

(x,

a)



1 N

Gt  Q (x, a)

(4.31) (4.32)

这样的“探索-采样”方式，也带来了两个问题：一是当环境的状态或动作空间较大

时，采样无法全面反映环境的特性，因此蒙特卡罗算法仅适用于使用T 步累积奖励的有

限空间强化学习；二是在采样过程中，由于无法及时对策略进行改进，会导致策略选择

缺乏探索性而使采样效率降低，这个问题可以用   贪心策略解决：





(

x)



 (x)，

以概率1-

A中以均匀概率随机选取，以概率

(4.33)

蒙特卡罗学习方法的完整流程可以用如下伪代码进行描述：

Input：environment E action space A initial state x0 total step T

24

深度强化学习算法介绍

Procedure： Q(x, a)  0,count(x, a)  0, (x, a)  1 A( x)

for s=1,2,… do

Execute actions and record： x0 , a0 , r1, x1, a1, r2 ,..., xT 1, aT 1, rT

for t=0,1,…,T 1 do

T
 Gt   i1ri i t 1

Q( xt

,

at

)



Q(xt

,

at )  count(xt , at count(xt , at ) 1

)



Gt

count(xt , at )  count(xt , at ) 1

end for

foreach x in x0 , a0 , r1, x1, a1, r2 ,..., xT 1, aT 1, rT



(

x,

a)



 

arg

max
a

Q(

x,

a),

P 1

choose actionbeyond linear distribution, P  

end for
Output： 

图 4.9 蒙特卡罗算法 Figure 4.9 Monte Carlo Algorithm

蒙特卡罗方法需要 Agent 探索出每条完整轨迹后才能对策略进行一次更新，实质上

是一种离线学习算法，相比于使用动态规划方法的有模型学习，策略评估和改进效率低

了很多。从本质上进行分析，蒙特卡罗方法并没有很好的利用强化学习的马尔可夫性，

实际上探索出的完整轨迹中包含了多个连续的状态-动作对，而根据马尔可夫决策过程

的定义，这些状态-动作对可以拆分成为多个独立的状态-动作对，基于此诞生了现代强

化学习最核心的算法之一：时序差分算法（temporal difference，TD）。

现将T 步折扣累积奖励的学习任务一般化，可将状态值函数的更新过程改写为：

V

(x)



V

(x)



t

1  1

(Gt

 V

( x)),

t



0,1...,T

1

(4.34)

现令  1 ，对于步数不稳定的任务，我们令 为一较小的正数值，并可在数学上证
t 1

明 为常数时V 依旧代表奖励累积，于是，蒙特卡罗方法的状态值函数更新过程为：

V (x)  V (x) (Gt V (x))

(4.35)

实际上，蒙特卡罗方法不能实现单步更新是由于 Gt 值必须通过完整轨迹计算出，只要我

们依据马尔可夫决策过程由单步的参数对 Gt 值进行估计，即可完成策略的单步改进，此

25

中国铁道科学研究院硕士学位论文

处我们令：

Gt



Ra xx

 Vt (x)

(4.36)

并通过式(4.20)，可推导：

  Q (x, a)  Q (x, a)  

Ra x x





Q

( x,

a)



Q

(x,

a)

(4.37)

这就是 TD 算法的更新过程，由此我们给出完整的学习过程描述[49]：

Input：environment E action space A
initial state x0 reward discount  step size 
Procedure：

Q(x, a)  0, (x, a)  1 A( x)

x  x0, a    (x) for t  1, 2,...do
Execute action a，get next state x and reward r a    (x)

Q(x, a)  Q(x, a)  (r   Q(x, a)  Q(x, a))

Update   (x)

x  x, a  a

end for
Output： 

图 4.10 Sarsa 算法

Figure 4.10 Sarsa Algorithm

以上算法称为 Sarsa 算法，很明显，在 Sarsa 算法流程中，算法基于  (x) 生成了一个新

的样本，并基于这个样本更新了策略  (x) ，像这种评估与执行为同一策略的强化学习

算法，称为“On-Policy”算法，对 Sarsa 算法进行修改，使其评估策略与执行策略不为

同一个，即可给出 TD 算法的“Off-Policy”形式[50]：
Input：environment E action space A initial state x0 reward discount 
step size 
Procedure：

26

深度强化学习算法介绍
Q(x, a)  0, (x, a)  1 A( x)
x  x0 for t  1, 2,...do
a    (x) Execute action a，get next state x and reward r Q(x, a)  Q(x, a)   (r   max Q(x, a)  Q(x, a))
a
Update   (x)
x  x end for
Output： 
图 4.11 Q-learning 算法 Figure 4.11 Q-Learning Algorithm
以上算法称为 Q-Learning 算法。对比 Sarsa 算法可以发现，Q-learning 算法也是基于  (x)
生成样本，但是在更新 Q 值时没有直接利用这一样本，而是计算了最大收益，这是因为 生成样本的策略不一定是最优的[51]，所以，在更新策略时，将当前所选动作与历史评价 中的最好动作进行了比较与选择，并以最优动作的 Q 值完成对策略的更新。 “Off-Policy” 算法提高了对既有知识的利用效率，在深度强化学习中，这种思想在机器学习中得到了 更加广泛的应用，也使得 Q-learning 算法成为了强化学习算法中最重要的一个分支。
4.3 深度强化学习
由上一节的介绍可知，强化学习的 Q-learning 算法，是以状态-动作值 Q-value 为核 心的值迭代算法。Q-learning 算法应用于有限状态空间的强化学习任务时，通常是以表 格（Q-table）的形式表示并存储迭代的 Q-value，用以表达强化学习的学习成果，经过一 段时间的学习后，基本所有状态和动作被迭代到，Q-table 接近完备状态，此时将某一状 态输入 Q-table，可以找到该状态下各个动作对应的 Q-value，从中选取 Q-value 值最大 的一个，即为当前状态下的最优动作[52]。
但是，在解决实际问题时，状态空间或动作空间的规模或维度常常很大，例如强化 学习领域用来测试算法效果最常用的 Atari 游戏，其输入状态是原始图像数据（ 210160 像素的图片），假设图片为黑白图片，每个像素点由灰度值表达即可，每一个像素点有 256 种可能，那么一张图片的总状态空间为 256210160 ，这样大规模的状态空间不可能用 Q-table 进行存储，无论是存储成本还是使用时的查询成本都非常高。解决这个问题的方
27

中国铁道科学研究院硕士学位论文

法是值函数近似（value function approximation），即用一个函数：

f (x, a)  Q(x, a)

(4.38)

来对 Q-value 与状态-动作的映射关系进行拟合，替代有限的 Q-table。拟合函数拟合效果

的好坏直接决定了强化学习的结果，但是在面临大规模离散空间或连续空间时，函数拟

合也将无法直接进行。由上文对深度学习的介绍可知，深度神经网络在理论上可以对任

何函数进行拟合，因此，可以用强化学习迭代生成的样本和单步奖励生成训练集，训练

深度神经网络来拟合 Q(x, a) 。在这种思路的指引下，DeepMind 团队于 2013 年提出了深

度强化学习的概念，并提出了深度 Q 网络算法（Deep Q-Network，DQN）。由 Q-learning

算法可知，算法每一次迭代的核心是求：

Q* ( x,

a)



Ex

(r





max a

Q* ( x,

a))

(4.39)

现用一个带有参数 的深度神经网络来进行拟合：Q(x, a, )  Q*(x, a) ，在每一步迭代时

以梯度下降法对网络进行训练，式(4.7)的样本均方误差由损失函数 Li (i ) 代替：

Li (i )  Ex,a p ( yi  Q(x, a,i ))2 

yi



Ex

r



max a

Q

(

x,

a,i

1

)

 

损失函数在 上的梯度为：

(4.40) (4.41)

  i Li (i )  Ex,a p;x

 

r 

max a

Q(

x,

a,i1

)



Q(

x,

a,i

)

i

Q(

x,

a,

i

)

 

(4.42)

除了引入深度神经网络，DQN 算法还引入了另外两个机制来确保对深度神经网络

的训练能够高效和收敛：经验回放机制[53-54]和目标网络机制。

由于强化学习迭代过程实际上在每一个回合生成了一条完整的状态-动作轨迹，轨

迹中的每一个状态-动作对之间都具有一定的关联性，而由于每个回合起点的初始化策
略和   贪心策略的存在，每一条轨迹之间又可能存在较大差别，这会造成神经网络训

练的不稳定，即不收敛。考虑到强化学习过程具有马尔可夫性，每一个状态-动作对都可

以独立用做训练样本，基于此，DQN 算法将完整轨迹分割为多个独立的状态-动作对，

并建立样本库，每一次迭代产生的新样本加入到样本库中，当样本库达到一定的数量时，

每一次迭代从样本库中随机抽取部分样本用于策略评估和改进。

目标网络机制则是建立两个 Q 网络，其中一个为实时网络，负责每步迭代的策略选

择，每步迭代都更新一次；另一个为目标网络，负责存储强化学习已经学到的经验数据，

给出每步迭代时的最大历史 Q 值，即策略评估。目标网络每隔一定的步数与实时网络进

行一次同步，用当前学习的最新经验完成历史数据的更新。通过目标网络的延迟更新，

28

深度强化学习算法介绍

使得策略更新和策略评估由不同网络完成，进一步打破同一条轨迹中各个状态-动作对

间的相关性。

综合以上步骤，DQN 算法的一般流程如下[55]：

Input：environment E action space A initial state x0 reward discount 
Procedure：

Initialize replay buffer D Initialize action-value network Q with random weights 

Initialize target action-value network Qˆ with weights    

for episode  1, 2,M do

Initialize x1  x0 for t  1, 2,...T do

at



arg max Q(x, a), a

P 1

choose a random action, P  

Execute at and get rt , xt1 Store transition (xt , at , rt , xt1) in D Sample random minibatch of transitions (xj , aj , rj , xj1) from D

yi



rj ,if episodeterminates at step j 1

 rj





max a

Qˆ

(

x

j

1

,

a,

 ), otherwise

  Perform a gradient descent step on

yi

 Q(xj, aj , )

2
with respect to the network

parameters

Every C steps reset Qˆ  Q

end for end for Output：target action-value network Qˆ

图 4.12 DQN 算法 Figure 4.12 DQN Algorithm
在设计实时网络和目标网络的结构时，可以根据输入强化学习任务的维度和状态空

间大小进行适当选型，如状态空间是大规模高维离散空间，可在网络的前几层采用卷积

神经网络，通过卷积和池化对特征进行降维，如状态空间较小，可完全采用常规的多层

神经网络以减小复杂度[56]。

29

中国铁道科学研究院硕士学位论文
DQN 算法是深度强化学习方向的奠基算法，在此基础上，后续又发展出了 DoubleDQN、Duling-DQN、DPG、DDPG、A3C 等算法，并在大规模离散空间问题和连续空间 问题上取得了很好的应用效果，不过由于这些算法分支刚刚被提出不久，仍旧处于不断 探索的阶段。 4.4 本章小结
本章主要介绍深度学习、强化学习及深度强化学习相关的基础理论、公式推导，并 以伪代码形式展示了常用算法应用的一般流程。深度学习方面，介绍了神经元模型、单 层神经网络与多层神经网络的结构和训练过程；强化学习方面，介绍了 MDP 基础、基 于动态规划的有模型学习、基于蒙特卡罗算法和 TD 算法的无模型学习；深度强化学习 方面，介绍了如何结合深度学习和强化学习，并介绍了深度强化学习领域最基础的算法： DQN 算法。由于整个机器学习算法体系庞杂，分支众多，因此本章仅涉及到了本文所用 算法的基础理论。
30

动车所行车调度计划智能化算法设计
5 动车所行车调度计划智能化算法设计
由于每个动车所站场形状都不相同，因此在算法设计阶段，需要选定一个典型站场 图形，本文选定与第 2 章业务分析时一致的站场图形，如下图所示：
图 5.1 动车所站场示意图 Figure 5.1 Diagram of an EMU Depot
站场具备 2 个出入口，10 条存车线，5 条检修库线，1 条洗车线，一条镟轮线，能够模 拟大部分动车所的接发车和调车业务。
经过第 2 章的业务分析，第 3 章的算法适应性分析和第 4 章的理论基础，本文选定 基于深度强化学习的 DQN 算法作为解决动车所行车调度计划智能化的核心算法。 5.1 站场模拟环境设计
DQN 算法的主体迭代过程是强化学习，每一步迭代都是 Agent 与环境的交互，因 此首先对站场模拟环境进行设计，包含状态空间设计、动作空间设计和动作奖励设计。 5.1.1 状态空间设计
根据强化学习的一般流程，算法的输入量为环境的状态，因此设计模拟环境时需要 对状态空间进行设计。这一部分的设计可以参考现有人工编制调度计划时的流程：根据 各个动车组的接发车时间点、实时位置、检修需求编制接发车和调车计划。因此，站场 模拟环境的状态需要能够完整描述站场内所有动车组的接发车时间、位置、各股道和咽
31

中国铁道科学研究院硕士学位论文
喉的占用情况、检修需求和当前的完成情况。 环境的状态是随着时间推移不断发生变化的，虽然时间是连续变量，但是根据强化
学习的马尔可夫性，环境的状态变化轨迹可以以某一小段时间为单位分割为无数个离散 状态，这个分割单位一般为两次动作的最小时间间隔。考虑到动车所行车调度业务一般 以一天为一个大周期，因此将一天划分为 N 个时间点，并以 n  0,1,..., N 来代表各个时 刻，将动车组的接发车时间以整数的形式表达。动车所一钩接发车和调车计划用时一般 为 10~20 分钟，考虑到平行进路计划的存在，将时间间隔缩短为 5 分钟，考虑到动车所 作业多数接车作业集中在傍晚至午夜，多数发车作业集中在凌晨至第二天的上午，因此 选择每天的中午十二点作为每天的起点进行分割。
动车所站场有多个股道，站场内的动车组数量不确定。描述动车组位置状态有两种 方式，一种是以动车组为个体，每个动车组占用一个股道或半个咽喉，基于此可对站场 内各个股道和咽喉区进行编号，对动车组位置进行量化；另一种是以股道为个体，对各 动车组进行编号，无占用的股道车组编号为 0。从神经网络训练的角度看，由于样本的 各项特征在输入网络以前需要做归一化，后一种描述方式由于每天的动车组数量不确定， 会给归一化带来困难，因此选择第一种描述方式，每一个股道对应一个位置编号，咽喉 区比较特殊，在车辆走行进路中，咽喉区只有半个咽喉会被占用，为了模拟平行进路的 情况，需将咽喉区划分为上下两个部分并各自对应一个编号。位置相关的描述一共有四 个：接车线，发车线，当前位置，计划预订股道。其中计划预订股道比较特殊，由于状 态分割的时间为五分钟，一般会小于动车组的计划走行时间，所以需要设置一个预订股 道，用来描述当动车组走行过程（动车组的当前位置不在任何一个股道而是处于咽喉区） 的目的股道，一旦该股道被某列动车组预订即视为已被占用，其他以其为目的股道的计 划将无法执行。
本文示例的动车所车辆检修作业共有三种：清洗，检修，镟轮，三种作业分别由三 种作业线完成，每辆动车组的检修需求为三种作业的组合。为减少状态的特征数，将检 修需求和当前完成情况合并描述为作业剩余时间，即清洗作业剩余时间、检修作业剩余 时间和镟轮作业剩余时间，剩余时间大于 0 代表动车组有该项作业需求且作业未完成； 初始时剩余时间为 0 代表动车组没有该项作业需求；剩余时间减少为 0 代表该项作业已 完成。
综合上述分析，最终确定环境的状态以每列动车组为单位，共包含如下 10 项特征：
32

动车所行车调度计划智能化算法设计
图 5.2 动车组状态特征设计 Figure 5.2 Figure Design of the Status of EMU Train
当动车组数量为 N 时，状态可用一个 N 10 的矩阵表示。 5.1.2 动作空间设计
强化学习算法的输出量为合适的动作，因此，在设计环境时要对动作空间进行设计。 在动车所行车调度问题中，每一步要执行的调度计划即为动作。调度计划包含四部分： 要进行调度作业的动车组、调度作业计划时间、起始股道、目的股道。强化学习每步迭 代过程只能执行一次动作，即每次迭代只能执行一个调度作业计划，且根据模拟环境状 态空间设计时的时间离散化处理，迭代的步数就已经代表了该步调度作业计划的执行时 间；同样的，由模拟环境状态空间设计，每一个状态中都包含了动作组的实时位置，设 计环境的动作时，无需再指定计划作用于哪一列动车组，一旦计划的起始股道确定，该 计划的作用对象就是目前停靠在该股道的动车组。因此，模拟环境的动作只需包含起始 股道和目的股道两个参数，将站场中的各股道分别作为起始股道和目的股道进行排列组 合，即可得到模拟环境的完整动作空间。
整个站场共有 2 个出入口，10 条存车线，1 条洗车线，5 条检修线，1 条镟轮线， 共计 19 条股道，每一条股道都可以作为计划的起始股道或目的股道，在不考虑折返计
A 划的借助区段和如何经过咽喉区的情况下，排列组合就已经有 2  19 18  342 个动作， 19
根据既有研究经验，对于强化学习而言已经属于大规模动作空间，会造成训练难以进行， 需要对动作空间进行优化。
33

中国铁道科学研究院硕士学位论文
首先考虑到动车所行车调度业务的实际情况，当某些股道作为起始股道时，一些股 道是不能直接作为目的股道的，比如接车作业，起始股道为某一个出入口，接车计划的 目的股道必须为某一条存车线，目前动车所行车调度业务中通常不允许直接接车到检修 库，同理，发车作业的起始股道必须为某一条存车线，不允许从检修库发车到出入口， 通过这些原则，可以将一部分实际上无法执行的计划从动作空间中排除。调车作业方面， 当起始股道为某一条存车线时，其目的股道可以是洗车线或镟轮线或任意一条空闲的检 修库线；同理当起始股道为洗车线或镟轮线或某一条检修库线时，其目的股道可以是任 意一条空闲的存车线。按照这些实际作业原则， 可对动作空间进行精简。
其次，某些进路较长的计划，可以将其拆分为 2 步动作执行，虽然看似拆分是将一 个动作变成两个，但是由于进路组合的特殊性，动作空间的总动作数却是大幅减少的。 在进行过上一步的精简后，依旧能够拆分的长进路计划共有两种：一是折返计划，折返 计划又包含两种，一种为存车线到存车线的折返，需借助某一条股道作为牵出线，在经 过咽喉区时也有去程经过洗车线、返程经过洗车线和不经过洗车线三种选择，考虑到折
A 返计划的借助区段，共计 2  3 5  900 个动作；另一种为检修库线和镟轮线间的折返， 10
同样考虑到如何经过咽喉区和折返借助区段，共计 (5  5) 310  300 种动作。现将其以 折返点为分割点，拆分为两个单向的调车计划，这些单向动作在上一步的精简动作空间 中已完全包含。另一方面，单向的调车计划又分为存车线到检修线或镟轮线，检修线或 镟轮线到存车线两种，在考虑如何经过咽喉区的情况下，共计10 2 6  6 210  240 种动作。现将与洗车线平行的咽喉区段 18 作为一条辅助股道使用，并将单向调车计划 以辅助股道和洗车线为分割点再次进行拆分，一部分为存车线与洗车线或辅助股道间的 调车，另一部分为洗车线或辅助股道与检修线区的调车，两部分合计 10 2 2  2 6 2  64 种动作。
综上所述，模拟环境的实际动作空间为：接车计划 210  20 ，发车计划10 2  20 ， 调车计划10 2 2  2 6 2  64 ，共计 20  20  64  104 个动作。
5.1.3 动作奖励设计
在每一步迭代过程中，模拟环境需要当前的状态和执行的动作给出动作的奖励值， 这个奖励值是强化学习策略评估和改进的关键，因此，模拟环境的单步奖励设计，对于 强化学习的学习效率和学习方向有着至关重要的作用。在动车所行车调度问题中，车辆 的一般作业流程为接车、调车至检修线、完成检修作业后调车回存车线、发车。对于每
34

动车所行车调度计划智能化算法设计
一列动车组而言，能够按照规定的时间点完成发车作业，是动车所行车调度系统的最终 目标。因此，模拟环境发出奖励信号条件为：当动车组在规定时间点执行发车动作，且 此时动车组的状态为已具备发车条件。

5.2 特征归一化处理

站场模拟环境的状态中包含了多个特征，如动车组计划相关的位置和时间点信息，

虽然对其进行了离散化处理，但是各个特征的量纲不同，离散化的数值范围也不同，例

如以五分钟时间间隔对时间进行离散化，则当前时间的最大值为 24 60  5  288，而动

车组的当前位置共有 26 种。在这种情况下，如果直接将其作为样本数据对神经网络进

行训练，会造成样本中各特征对神经网络权重值的影响不平均，在采用梯度下降法训练

神经网络时，这种不平均会使下降方向变化更为剧烈，从而降低收敛速度甚至造成不收

敛。为了消除这种由各特征量纲不同带来的不均一性，需对样本进行数据预处理：归一

化（normalization）。归一化方法是采用一定的数学变换规则，将一定取值范围内的数据

映射到固定空间如（0,1）内，机器学习中常用的有以下几种归一化方法：线性函数归一

化、0 均值归一化和非线性归一化。

线性函数归一化公式为

xnorm



x  xmin xmax  xmin

，实质上是对特征做了一次线性映射，将特

征取值范围压缩至（0,1）或（-1,1）空间内，比较适合于特征分布为有限空间均匀分布

的情况，若 xmin 和 xmax 不稳定，则归一化效果会很差。

图 5.3 适用于线性归一化的数据分布示例图

Figure 5.3 Diagram of Data Distribution Applied to Liner Normalization

0

均值归一化公式为

xnorm



x 



，其中 

为原始数据的均值，

为原始数据的标准

差，数据经过 0 均值归一化处理后符合标准正态分布（均值为 0，标准差为 1），在分类、

聚类算法中，需要以距离来度量相似性时常用。

35

中国铁道科学研究院硕士学位论文
图 5.4 适用于 0 均值归一化的数据分布示例图 Figure 5.4 Diagram of Data Distribution Applied to 0-Mean Normalization
非线性函数归一化的映射函数为非线性函数，如对数函数、指数函数、正切函数等 等，常常适用于数据分布不均匀，如在某一小范围内比较集中但整体取值范围又较大的 情况。
图 5.5 适用于非线性归一化的数据分布示例图 Figure 5.5 Diagram of Data Distribution Applied to Nonlinear Normalization
考虑到站场模拟环境的各个特征均为正整数，且各数值间为均匀分布，因此采用线 性函数归一化的方法将各特征压缩到（0,1）空间内。 5.3 神经网络结构设计
在设计神经网络的结构之前，首先应对神经网络的输入量（站场模拟环境的状态） 规模和输出量（动作）规模进行估算，并基于此设计神经网络的层数、每一层神经元的 个数、网络的类型等等。
状态方面，现假设算法中只有一个神经网络，则输入量为某一时刻全场动车组的状 态，假设共有 10 列动车组，由上文的站场模拟环境状态空间设计，每列动车组共有 10 个特征，则输入量为一个1010 的矩阵。这个数据量规模相较于深度学习的图片分类等 任务而言，规模不算很大，因此不必采用卷积神经网络结构进行降维，直接采用常规类
36

动车所行车调度计划智能化算法设计
型的多层神经网络即可。 动作方面，由上文的动作空间设计，模拟环境的总动作数已经缩减为 104 个。DQN
算法中神经网络完成的是对 Q 值的模拟，最后神经网络的输出为各个动作的 Q 值组成 的一个矩阵，策略就是从矩阵中选取 Q 值最大的动作。对于单个神经网络而言，相当于 要输出一个 104 维的矩阵。为减小单个网络的复杂度，提升训练收敛的速率，需要对网 络结构进行优化。通过对动作空间中所有动作的考察，可对动作进行分类：接发车作业 发生在出入口和存车线间，都经过咽喉一；存车线与洗车线或辅助股道间的调车作业都 经过咽喉二；洗车线或辅助股道与检修线间的调车作业都经过咽喉三。因此，将整个站 场的调度业务按照咽喉分成三部分，每一部分由一个独立的神经网络拟合 Q 值，三个神 经网络的输入均为整个站场的状态，输出为该咽喉所对应的各个动作的 Q 值。相比于单 一神经网络，三个神经网络在以下方面更具优势： (1) 通过对动作空间的分割，降低了单个网络的输出维度，减少了各隐藏层所需的神经
元数，大大降低了单个网络的复杂度和训练难度。经过分割，网络一的输出维度为 210 2  40 ，网络二的输出维度为10 2 2  40 ，网络三的输出维度为 2 6 2  24 。 (2) 强化学习基于马尔可夫决策过程将时间点离散化，在每一步迭代动作完成后才跳入 下一时间点，因此单一网络的每一个时间点只能选择并执行单一动作。三个网络对 应的动作空间是各自独立的，采用三个网络以后，每一步迭代过程中各个网络都可 以执行一步动作并给出对应的 Q 值评估，这在调度业务中具有很重要的现实意义： 解决了站场内同时执行多个计划的问题。例如，在单一网络下，当最优动作为接车 作业时，所有调车作业都不会被选择到，而实际调度中，执行接车作业时只要咽喉 二和咽喉三是空闲的，是可以同时执行调车作业的。 (3) 三个网络的输入量都为整个站场的状态，但是在每次迭代过程中三个网络的输入量 并不相同，当网络一选择并执行某一动作，站场状态中对应车组的预订股道将变为 动作的目的股道，站场的状态转变为一个新的状态，此新状态将作为输入量输入网 络二，网络三同理。因此，三个网络之间的地位不是均等的，越靠后的网络其对应 的优先级越低。网络一和网络二的优先级体现在哪一个优先占用存车线，网络一选 择接车作业将某动车组接车至存车线 5，则网络二中以存车线 5 为目的股道的调车 作业将无法执行，在实际的动车所调度业务中，为了保证接发车正点，接发车计划 的优先级也应高于调车，因此网络一的优先级应最高。网络二和网络三的优先级体 现在哪一个优先占用洗车线或辅助股道，由于存车线的数量多于检修库线，而动车
37

中国铁道科学研究院硕士学位论文
所的最大容量为存车线全部停满，因此检修库线应在车辆完成检修任务后尽快出清， 调车作业中以检修线为起始股道的计划优先级最高，因此网络三的优先级应高于网 络二。

5.4 算法主体流程设计

算法基于 DQN 算法，主体流程也基本与 DQN 算法保持一致，只是将一个网络扩

充为三个网络，每一步迭代过程通过三个网络选择三个动作。动作选择方面，依旧采用

  贪心策略增加算法的探索性。下面以伪代码的形式给出算法的主体流程设计：

Input：environment E

action space A1, A2 A3
initial state x0 reward discount  Procedure：

Initialize replay buffer D1, D2, D3

Initialize action-value network Q1,Q2,Q3 with random weights 1,2,3

Initialize target action-value network

Qˆ1, Qˆ2 , Qˆ3

with

weights1



1,

 2

 2 ,3

 3

for episode  1, 2,M do

Initialize x1  x0 for t  1, 2,...T do

xt~1  xt ;

at ~1



 arg 

max
a

Q1 ( xt ~1 ,

a),

P  1  , a  A1 ;

choose a random action, P   , a  A1

Execute at~1 and get rt~1, xt~2 ;

at ~ 2



arg 

max
a

Q2 (xt~2 , a),

P  1  , a  A2 ;

choose a random action, P   , a  A2

Execute at~2 and get rt~2, xt~3 ;

at ~ 3



arg 

max
a

Q3 (xt~3, a),

P  1  , a  A3 ;

choose a random action, P   , a  A3

Execute at~3 and get rt~3, xt~4 ;

Change the current time of xt~2, xt~3, xt~4 and get xt~2, xt~3, xt~4 ;

Store transition (xt~1, at~1, rt~1, xt~2 ) in D1 ;

38

动车所行车调度计划智能化算法设计

Store transition (xt~2, at~2 , rt~2, xt~3) in D2 ;

Store transition (xt~3, at~3, rt~3, xt~4 ) in D3 ;

xt1  xt~4 ;

Sample random minibatch of transitions (xj , aj , rj , xj1) from D1 ;

yi



 rj ,if episodeterminates at step j 1

rj





max a

Qˆ1

(

x

j

1

,

a,1

)

,

otherwise

;

  Perform a gradient descent step on

yi  Q1(x j , a j ,1)

2
with

respect

to

the

network parameters1 ;

Sample random minibatch of transitions (xj , aj , rj , xj1) from D2 ;

yi



 rj ,if rj  

episodeterminates at step j 1

max a

Qˆ2

(

x

j

1

,

a,2



)

,

otherwise

;

  Perform a gradient descent step on

yi  Q2 (x j , a j ,2 )

2
with

respect

to

the

network parameters2 ;

Sample random minibatch of transitions (xj , aj , rj , xj1) from D3 ;

yi



 rj ,if rj  

episodeterminates at step j 1

max a

Qˆ3

(

x

j

1 ,

a,

3

)

,

otherwise

;

  Perform a gradient descent step on yi  Q3(x j , a j ,3) 2 with respect to the

network parameters3 ;

Every C steps reset Qˆ1  Q1, Qˆ2  Q2 , Qˆ3  Q3 ;

end for

end for

Output：target action-value network Qˆ1, Qˆ2 ,Qˆ3

图 5.6 算法主体流程 Figure 5.6 Main Flow of the Algorithm

注：按照上文神经网络结构设计部分三个网络的执行优先级，伪代码中角标为 2 的变量

代表网络三，角标为 3 的变量代表网络二。

需要特别注意的是，在网络一执行完以后，生成了一个当前时间未发生变化，但是

动车组位置和预订股道发生了变化的状态 xt~2 ，该状态作为下一网络的输入状态，xt~2 与 xt~2 的差别仅仅为状态特征中的当前时间发生了变化，增加了一个分割周期。按照算法 设计，在一步迭代中时间增加一个分割周期，而动作执行三步，也就意味着三个动作是

39

中国铁道科学研究院硕士学位论文
同时执行的，但是网络一的经验回放存储区中存储的状态-动作对的下一状态为 xt~2 ，而 不是所有动作都执行完成后的状态 xt~4 。这样做是由于 xt~4 不仅与网络一有关，还与网络 二和网络三有关，选择 xt~2 作为网络一动作执行后的结果，是通过 xt~2 来模拟当其他网络 不存在或其他网络输出为空动作的情况，通过这种方式来保证各网络对应的经验回放存 储区中存储的状态-动作对仅由对应的网络抉择而与其他网络保持独立。同样，状态 xt~2 中的当前时间与 xt~1 相同，在迭代过程中它是一个不存在的虚拟状态，它的作用是保证 了每次迭代过程中存储在存储区的起始状态的当前时间一致，同样保证了三个网络的独 立。
算法最终输出的为学习好的目标网络 Qˆ1, Qˆ2 ,Qˆ3 ，实际应用过程采用与学习过程类似 的结构：三个网络的执行顺序与学习过程保持相同，中间依旧用当前时间相同的辅助状 态连接，将初始状态输入网络依次得到三个动作，即得到当前时间点的最优计划，将学 习过程内层循环的策略选择方式由贪心策略改为确定性策略，并进行迭代，即可得到一 天内各个时间点的计划。 5.5 本章小结
本章以第四章介绍的深度强化学习相关理论为基础，对动车所行车调度计划智能化 编制算法进行了详细设计，按照算法设计的一般流程，从动车所模拟环境设计出发，首 先对算法的基础参数——状态空间、动作空间和奖励信号进行了设计，并依据动车所行 车调度业务的实际情况对状态空间和动作空间进行了精简；为算法选择了合适的特征归 一化方法，完成了站场模拟环境参数与 DQN 算法的适配；通过对状态空间规模的分析 选择了神经网络的类型，并创新性的引入三个连续的神经网络完成对动作空间的拆分， 减轻了单个网络的拟合和训练压力，同时将单步迭代的可执行动作数提升为三个；最后， 在上述设计的基础上，以伪代码的形式给出了算法完整的学习流程，为后续算法的实现 和验证打好了基础。
40

算法实现、仿真与验证
6 算法实现、仿真与验证
在上一章中，我们已经完成了算法框架的设计工作，本章将以上一章的设计为基础， 对算法进行实现，并通过编写仿真软件对算法进行运算和验证。
在算法实现和仿真软件设计的过程中，不仅仅要考虑如何以代码形式完成对上一章 算法的各种设计，还需要考虑软件工程化思想的应用，例如如何保持核心算法与业务逻 辑的分离、如何实现业务数据与业务逻辑的解耦、如何实现图形界面控件与业务逻辑和 数据的解耦与动态绑定等等。

6.1 算法实现

6.1.1 框架与编程语言选择

近几年随着 CPU 和 GPU 运算能力爆发式的增长，以多隐层神经网络为基础的深度

学习也发展迅速，一批深度学习框架的出现，更是将神经网络的构造与训练简单化、模

板化，对 GPU 运算的支持与优化也提高了框架的运算能力和训练效率。而且，这些框

架基本上都是开源框架，在开源社区如 Github 等的广大用户群支持下，各框架更新速度

都很快，对新硬件的支持也较为迅速。

目前已经得到成熟应用并且依旧在不断发展的框架主要有：TensorFlow，Caffe，

Keras，CNTK，MXNet，Torch，Theano 等，下表是这些开源框架在 Github 上的统计数

据对比： 框架名

表 6.1 深度学习框架对比 Table 6.1 Comparison of Deep Learning Frameworks

研发机构 主要支持语言 Stars

Forks

Contributors

TensorFlow Google

Python/C++

93209

59838

1383

Caffe

Berkeley

C++/Python

23306

14222

264

Keras

Keras-Team

Python

27160

9920

643

CNTK

Microsoft C++/C#/Python 14039

3725

173

MXNet

Apache Python/R/Scala 13412

4944

493

Torch

Facebook

Lua

7756

2249

133

Theano

MILA

Python

8029

2425

328

通过数据对比可以发现，TensorFlow 无论是在用户基数还是用户评价方面都占据了绝对

41

中国铁道科学研究院硕士学位论文
优势，而这种优势，一方面来源于其背后强大的 Google 及其研发团队，在人工智能领 域的累累硕果，如 AlphaGo、AlphaZero，另一方面则源于 TensorFlow 框架本身的优势。
TensorFlow 最大的优势在于其降低了构造和训练深度神经网络的门槛。在用户使用 方面，它以会话（session）为核心对象进行框架与用户代码的交互，只需要预先定义好 网络的结构和各基本参数，并在外部编写好训练迭代的主体流程即可，每一次迭代训练 的误差反向传播和神经网络参数调整都由框架自动完成[57]；另一方面，TensorFlow 框架 为尽可能的实现高性能，采用了 C++这一偏底层的语言进行实现，但是其支持最好的接 口语言为 Python， TensorFlow 框架本身支持 pip 方式的部署与调用，无论是在初次安 装配置还是后期应用方面，TensorFlow 都给深度学习带来了极大的便利性，也因此成为 深度学习框架中的佼佼者。
从数据统计中，我们还可以发现，几乎所有的主流深度学习框架都对 Python 语言提 供了支持。Python 是一种解释型的动态语言，没有变量类型声明等严格的语法限制，语 法简洁，入门难度低，目前在 web 后端开发、科学计算、爬虫、数据管理等方面形成了 较为完整的编程生态。Python 的另一大特点是与开源社区结合紧密，第三方库在数量和 质量上优势巨大，尤其是 Numpy、Pandas 等数学计算库，在矩阵处理、数学函数处理等 方面体验较好。在成为机器学习的首选语言后，随着近几年机器学习的蓬勃发展，Python 社区也越发活跃，各方面教程和参考资料数量巨大。
在综合考察多种框架和编程语言后，本文最终选定 TensorFlow 框架来完成神经网 络的搭建和训练，采用 Python 语言编写算法主体框架和与 TensorFlow 进行交互。 TensorFlow 版本为 1.5.0，Python 版本为 3.6.4。
6.1.2 硬件与编译环境选择
由于神经网络的复杂运算过程，深度学习算法通常对计算机硬件的计算能力有更高 的要求。CPU 可以做到很高的时钟频率，但是在并行计算方面受限于核心数量运算效率 不高，GPU 则弥补了这一缺陷。每个 GPU 包含了多个流处理单元，在 CUDA 技术的支 持下，利用高性能 GPU 进行大规模并行运算，无论在运算速度还是运算精度方面都更 具优势[58]。因此，目前主流的深度学习框架都已经支持了 GPU 运算，TensorFlow 框架 则直接推出了两种版本：CPU 版和 GPU 版，在同样规模的神经网络需求下，GPU 版的 运算速度比 CPU 快了一个数量级。因此，本文选定 TensorFlow-GPU 版，并选择 NVIDIA GTX1080ti 作为 GPU 为训练进行加速。NVIDIA 还专门为深度学习框架编写了 CUDA
42

算法实现、仿真与验证
驱动和 cuDNN（CUDA Deep Neural Network Library）库，专门为深度学习任务进行加 速。
编译环境方面，由于选定了 Python 语言，因此选用了 Python 编译器中目前评价最 好的 Pycharm。除了具备基本的代码编写与调试功能，Pycharm 还提供了 Python 语言的 自动补全、自动格式修正、解释器与 pip 包管理、Github 版本管理等等特色功能。
6.1.3 核心类结构设计
在设计深度强化学习核心算法时，我们利用 Python 语言面向对象的特性，将深度强 化学习中神经网络构造、训练部分的代码封装为独立的类：DRL_Brain。该类提供了神 经网络输入、输出、训练相关的方法并可被外部调用，而类本身的各成员变量和方法与 业务无关，对深度强化学习任务具有普适性，在这样的封装设计下，大大提高了后期对 学习算法进行单元测试和算法更换的便利性。核心类的结构图下图所示，下面对该类的 关键预定义参数和方法进行介绍。
DRL_Brain
-Learning_Rate -Reward_Discount -Memory_Size -Batch_Size -E_Greedy -Update_Step -Activation_Type -构造函数 -build_net( ) -store_trainsition( ) -choose_action( ) -learn( ) -save( ) -restore( )
图 6.1 DRL_Brain 类的 UML 图 Figure 6.1 UML Diagram of DRL_Brain Class
预定义参数为算法相关的一些变量，这些变量在算法运行过程中保持不变，因而以 全局变量的形式在类的外部进行定义，便于算法调优时的参数调整，这些预定义参数为：
学习率、迭代奖励折扣、存储区样本临界值、抽取样本个数、   贪心策略临界值、目
标网络更新步数临界值、网络激活函数的类型。 类的构造函数为类实例化时自动调用的函数，主要完成类的参数初始化和一些初始
化方法的调用。DRL_Brain 类的构造函数有四个参数：状态维度和三个网络对应的动作
43

中国铁道科学研究院硕士学位论文
维度。状态维度决定了算法中缓存状态的维度，动作维度则决定了三个网络的输出层的 神经元个数（即神经网络的输出维度）。构造函数对类的成员变量进行初始化，例如根据 状态维度以 tensorflow.placeholder 类型定义算法中缓存的当前状态、下一状态和该步的 奖励，初始化 Session 对象、Saver 对象等，并对神经网络对象、网络参数矩阵、目标网 络参数替换过程、网络损失函数、状态-动作对存储区等进行了定义和初始化。
build_net( )为神经网络构造方法，由于算法中存在三个神经网络，因此对神经网络 构造的过程进行了一次封装，方法的参数包括输入变量、各层的神经元个数、各隐藏层 是否为可训练层等等。方法内部主要为根据参数调用 tensorflow.layers.dense( )方法构造 网络的每一个隐藏层，并最终返回神经网络对象。
store_trainsition( )为状态-动作对保存方法，在算法主体流程中调用，参数为一个完 整的状态-动作对，作用是给每一步迭代产生的状态动作对添加索引并存入存储区中。
choose_action( )为动作选择方法，在算法主体流程中调用，参数为当前的状态，作
用是按照   贪心策略随机或根据网络计算出的 Q 值选择动作并返回。
learn( )为神经网络训练方法，在算法主体流程中调用，无外部输入参数。训练方法 首先判断存储区中的样本数量是否已达到预定义的存储区样本临界值，如果未达到则不 进行训练，达到了则按照预定义的抽取样本个数从存储区中随机抽取出一批样本对神经 网络进行训练。
save( )为神经网络参数保存方法。在神经网络完成训练后，需要调用该方法，将神 经网络各层的权重值、偏置值等按照 tensorflow 规定的格式存储到本地文本文件中，在 检验或实际使用该神经网络时，需要按照这些参数重新生成一个神经网络。
restore( )为神经网络重建方法。该方法与 save( )对应，执行该方法可以从本地文件 中读取神经网络的参数并完成重建，重建后的网络可以继续训练、进行检验或实际使用。
6.2 仿真软件设计 为便于对深度学习算法进行测试和验证，在第五章站场模拟环境的基础上，对站场
模拟环境进行了编程实现，并编写了一套带有站场图形界面的完整仿真软件。
6.2.1 框架选择 编程语言与上一步的算法编程实现保持一致，选用 Python 语言。Python 语言常用
的 GUI 框架有 TKinter 和 PyQt。TKinter 是 Python 原生的 GUI 框架，界面设计全部都
44

算法实现、仿真与验证
由 Python 代码进行编写和调节。PyQt 则是一个第三方工具包，将 Python 语言和 Qt 库 进行了融合。Qt 库是目前 C++等语言界面设计最强大的库之一，拥有完善的控件体系和 图形化的界面设计器，能够实现“所见即所得”的编程方式。考虑到界面美观性和开发 效率，仿真软件采用 PyQt 框架。
利用 PyQt 框架进行 GUI 设计的一般流程是：首先在 QtDesigner 中采用控件拖拽方 式调整界面的元素的命名和各项属性，保存为 ui 文件后，由 PyUIC 工具转换为 Python 的一个类，在 Python 代码里直接实例化这个类后就可以进行界面加载。
6.2.2 基础类结构设计
上一节已经对深度强化学习的“大脑”进行了编程实现，在仿真软件中将主要对站 场模拟环境和强化学习的迭代主循环流程进行实现，考虑到仿真软件对不同站场的适应 性，实现软件内交互的核心数据对象与业务逻辑流程的分离，对核心数据对象进行了封
装，并在软件初始化时生成并存储在对象字典中。下面首先对仿真软件的各个基础类进 行设计：
(1) EMUTrain
EMUTrain
-trainID -arrivePosition -departurePosition -arriveTime -DepartureTime -remainingWashingTime -remainingLathingTime -remainingRepairTime -currentPosition -currentTime -orderPosition -movingStage -missionCompleted -构造函数 -getTrainStatus( )
图 6.2 EMUTrain 类的 UML 图 Figure 6.2 UML Diagram of EMUTrain Class
由站场模拟环境的状态空间设计，站场的状态以动车组为单位，每列动车组有 10 个 特征，且动车组的总数量不确定，因此不适于用数组或列表来表达环境的状态，现将动 车组单独封装为一个类，在站场初始化时，根据动车组总数量和各项特征参数实例化多 个对象并存入主类的动车组字典中。该类除了包含动车组本身的 10 个特征以外，还包
45

中国铁道科学研究院硕士学位论文
含了三个辅助特征：动车组 ID，用于动车组的索引；动车组移动阶段（TrainMovingStage 类型），用于算法迭代中记录动作执行情况；检修需求是否已完成（bool 类型），用于算 法迭代中记录检修需求完成情况。此外，该类还包含了一个方法：getTrainStatus( )，调 用该方法时通过判断动车组当前位置、检修需求剩余时间等特征返回动车组当前的车组 状态（TrainStatus 类型）。
(2) StationPosition StationPosition
-positionID -currentTrain -isOrdered -positionType -trainIDLabel -washingTimeLabel -repairingTimeLabel -lathingTimeLabel -构造函数 -updateLabelStatus( )
图 6.3 StationPosition 类的 UML 图 Figure 6.3 UML Diagram of StationPosition Class
在状态空间设计中，动车组位置由编号来表达。但是对于仿真软件而言，编号表达 无法体现各个位置的区别，算法中与位置相关的判断如果直接与位置编号挂钩，那么一 旦站场形状发生变化，比如存车线多了一条，则整个判断逻辑将失效。实际上，动车组 在站场中的位置可以划分为几类，同类的位置地位是平等的。于是，将动车组位置也单 独封装为类，并增加以下成员变量来辅助表达与该位置有关的信息：位置 ID，用于位置 的索引；当前动车组（EMUTrain 类型），是当前位置所停放动车组对象的引用；位置是 否被预定（bool 类型），用于指示当前位置是否被某一动作预定，如果被预定，则视为 该位置已被占用，在算法流程中不能作为目的股道；位置类型（PositionType 类型），用 于指示当前位置的类型信息。
(3) StepAction
46

算法实现、仿真与验证
StepAction
-actionID -startPosition -targetPosition -throatNeeded -构造 函数 -updateLabelStatus( )
图 6.4 StepAction 类的 UML 图 Figure 6.4 UML Diagram of StepAction Class
在动作空间设计中，动作由起始股道和目的股道两个参数确定。而根据上文的算法 设计，由于每步迭代时间点前进的长度小于动作的执行用时，因此迭代过程中会出现动 车组位于某一咽喉的上半部分或下半部分的状态，为了解决咽喉区的占用冲突，仿真软 件需要在执行动作时知道动作需要占用的咽喉区，基于此，该类共包含如下成员变量： 动作 ID，用于动作的索引；起始股道（StationPosition 类型），指示当前动作的起始股道； 目的股道（StationPosition 类型），指示当前动作的目的股道；占用咽喉（StationPosition 类型），指示当前动作需要占用的咽喉区。
(4) PositionType
PositionType:Enum
-OutofStation = 0 -ArriveOrDepartureSection = 1 -ParkingSection = 2 -WashingSection = 3 -RepairSection = 4 -LathingSection = 5 -Throat = 6 -FreeThroatSection = 7 图 6.5 PositionType 枚举类的 UML 图 Figure 6.5 UML Diagram of PositionType Class
枚举类，用于保存动车组位置的类型，共有站外、出入口、存车线、洗车线、检修 库线、镟轮线、咽喉区、辅助股道 8 种类型。
(5) TrainStatus
47

中国铁道科学研究院硕士学位论文
TrainStatus:Enum
-OutofStation = 0 -JustArrived = 1 -IsWashing = 2 -IsRepairing = 3 -IsLathing = 4 -ReadyToLeave = 5 -IsMoving = 6 -TaskCompleted = 7 -InProgress = 8 -IsLeaving = 9 -IsUsingFreeThroat = 10 图 6.6 TrainStatus 枚举类的 UML 图 Figure 6.6 UML Diagram of TrainStaus Class
枚举类，保存动车组状态的类型。动车组状态是根据环境状态中各动车组的特征进 行分析得出的实时状态，在算法中与动作结合判断并给出动作的奖励信号，按照动车组 当前位置类型的不同进行划分与判断：位于出入口外时，状态为站外；位于出入口时， 状态有待接车和已发车两种；位于存车线时，状态有待作业和待发车两种；位于洗车线、 镟轮线、检修库线时，作业中和作业已完成；位于咽喉区时，状态为在进路中；位于辅 助股道时，状态为使用辅助股道。
(6) TrainMovingStage
TrainMovingStage:Enum
-NotMoving = 0 -FirstStage = 1 -SecongStage = 2 图 6.7 TrainMovingStage 枚举类的 UML 图 Figure 6.7 UML Diagram of TrainMovingStage Class
枚举类，保存动车组移动阶段，共有静止、移动第一阶段、移动第二阶段三种状态。 动车组在股道上停稳为静止，处在进路中时，由于动作执行所需时间大于迭代前进时间， 需要将走行过程划分为两个阶段，从起始股道移入咽喉区为第一阶段，从咽喉区移入目 的股道为第二阶段。
6.2.3 软件界面设计
仿真软件的界面基于 PyQt5 框架设计实现，用于展示站场形状以及迭代过程中各动 车组的编号、实时位置、检修需求完成情况、迭代步数、总奖励等信息，以及完成学习 模式、验证模式的切换和既有神经网络训练的读入等功能。
48

算法实现、仿真与验证
图 6.8 仿真软件界面 Figure 6.8 GUI of Simulation Software
图中各位置上如果有动车组停放，以绿色矩形框代替，矩形框由 4 个 Label 控件组成， 分别是车组号、剩余洗车时间、剩余检修时间、剩余镟轮时间，控件命名由 Label 类型 名+所属位置代号组成，如“label_WT_5”代表停放在位置 5 的动车组的剩余洗车时间。 6.2.4 核心类结构设计
StationEnvironment 类为仿真软件的核心类，用于实现模拟环境运行和强化学习的 循环迭代。下面对该核心类的关键预定义参数、关键成员变量和方法进行说明。
49

中国铁道科学研究院硕士学位论文
StationEnvironment
-EMUTrainInfoDir -ActionInfoDir -PositionInfoDir -Max_Episodes -Max_Steps -trainDic -lathingTimeLabel -trainDic -positionDic -actionDic -currentTime -totalReward -normalizedState -isTrainingCompleted -brain -构造函数 -reset( ) -step( ) -train( ) -executeAction( ) -moveAllTrain( ) -addCurrentTime( ) -generateNormalizedState( ) 图 6.9 StationEnvironment 类的 UML 图 Figure 6.9 UML Diagram of StationEnvironment Class
核心类的关键预定义参数有两个：迭代外循环的回合数和内循环的步数。在学习前
期，由于知识的稀疏性，强化学习能学到的知识较少，此时应设置较大的内循环步数和
较小的外循环回合数，较大的内循环步数可以保证每个回合的学习算法能够“走”的足
够“远”，增加算法的探索性，而较小的外循环回合数则控制了本次学习的总时间，便于
及时调整如学习率、奖励折扣等学习相关的参数，而在学习的后期，可以适当增大外循
环数提高学习的效率。
关键成员变量有：动车组字典，站场位置字典，动作字典，当前时间，当前累积总
奖励，深度强化学习算法核心类对象，以及程序运行中用到的各种全局标志变量等等。
构造函数主要完成一些全局变量的初始化，比如界面刷新的计时器、用于训练的新
线程、深度强化学习算法核心类 DRL_Brain 的实例化等等。在变量初始化这一方面，为
了提高仿真软件对不同软件和不同初始状态的适应性，将初始状态、模拟环境动作空间、
模拟环境位置信息分别存入 3 个配置文件中，并在构造函数中进行文件信息的读取。
EMUTrainInfo.txt 文件为初始状态配置，文件中每一行为一列动车组的各项特征，第 1
列为车组 ID，后续 10 列对应模拟环境状态空间设计中动车组的 10 个特征。ActionInfo.txt
50

算法实现、仿真与验证
文件为动作空间配置，每一行为一个动作，共 4 列，分别是动作 ID，起始股道编号，目 的股道编号，动作占用的咽喉区编号。PositionInfo.txt 为位置信息配置，每一行为一个 位置，共 2 列，第 1 列为位置编号，第 2 列为位置类型编号，该编号对应了基础类 PositionType 类中的枚举值。通过配置文件读取和核心数据对象的封装，仿真软件实现 了业务数据与业务逻辑的分离，增强了算法对不同站场的适应性。
reset( )为模拟环境重置方法。强化学习每一回合迭代的初始，需要将环境状态重置 为初始状态，重置方法需要在外循环中调用多次，因此将环境重置初始化的部分流程单 独编写为一个重置方法。在该方法中，首先将累计总奖励、当前时间清零，并将动车组 字典、位置字典、动作字典全部清空。之后，根据构造函数中读取的三个配置文件的信 息，重新实例化各个位置、动车组、动作的对象并存入字典。为了实现界面控件与后台 代码的解耦，在位置字典生成时，利用 QtWidgets.QWidget.findChild( )方法，直接查找该 位置对应的 4 个 Label 控件并在每一个位置对象中添加控件的引用。在这样的设计下， 仿真软件完成了图形界面控件的动态绑定，从而实现了界面与业务逻辑的解耦。如果对 站场图形进行调整，比如增加一条存车线，只需要用 QtDesigner 和 PyUIC 重新绘制生 成一个界面类，并调整位置信息配置和动作空间配置即可，无需对代码进行改动。
step( )为动作的执行方法。该方法的参数为动作 ID，方法根据动作 ID 在动作字典 中查找动作对象作为依据修改站场中各动车组的状态特征，并依据当前状态和动作判断 给出该动作的奖励信号。
train( )为强化学习的主循环流程，是章节 5.4 的代码实现。迭代过程需要较长时间， 为防止界面的主线程被阻塞，该方法由辅助线程执行。外循环对应强化学习回合数，每 一个回合初始调用 reset( )方法重置环境后开始执行内循环。内循环对应每一回合迭代的 步数，每一步首先由动车组字典生成站场当前状态并进行特征归一化处理，调用 DRL_Brain.choose_action( )方法计算最佳动作 ID，由 step( )方法执行并返回奖励值，再 次根据动车组字典生成新的站场状态并进行特征归一化处理，之后调用 DRL_Brain.store_trainsition( )存储状态-动作对，最后调用 DRL_Brain.learn( )方法完成神 经网络的训练。所有外循环执行结束后，调用 DRL_Brain.save( )方法对训练结果进行保 存。
下面以 UML 图的形式给出各类的主要方法之间的调用关系：
51

中国铁道科学研究院硕士学位论文

startNewTraining_clicked()
brain=DRLBrain() brain.__init__()
environment.train()

Event banding

Start
environment=StationEnvironment() environment .__init__() environment .show()

i<Max_Episode True
False brain.save()
i+=1
brain.sess=tensorflow.Session() brain.realtimeNet=brain.build_net() brain.targetNet=brain.build_net() brain.trainOp=brain.build_trainOp() brain.replaceOp=brain.build_replaceOp()
brain.sess.run(trainOp)

environment .reset()
False j<Max_Step True
action=brain.choose_action(currentState) nextState,reward=environment.step(action)
brain.store_transition(currentState ,action,reward,nextState) brain.learn()

Steps>replaceMax False

True

brain.sess.run(trainOp)

j+=1

图 6.10 各类和方法调用关系 UML 图 Figure 6.10 UML Diagram of Call Relationship between Classes and Methods

6.3 算法结果与分析
按照本章前两节的算法设计，深度强化学习的核心已经被单独封装为 DRL_Brain 类，该类的实现与业务完全无关，具备普适性。因此，算法的编程实现和仿真软件的编 写完成后，首先对 DRL_Brain 类进行了单元测试，以验证深度强化学习算法本身的有 效性。

52

算法实现、仿真与验证
6.3.1 算法核心类的单元测试 单元测试是在软件全面测试前对软件的最小可测试单元进行测试[59]。对于面向对象
类的编程语言，一般最小可测试单元的粒度都在类一级，所以首先对 DRL_Brain 类和 模拟环境进行测试。
基于 TensorFlow 框架编写的学习算法可以用 TensorBoard 工具生成网络结构图，用 于展示网络结构、参数和算法中各过程的执行顺序与数据交互过程。因此，首先用 TensorBoard 工具生成了网络结构图，整体与部分关键元素的结构如下图所示：
图 6.11 TensorBoard 生成的整体结构图 Figure 6.11 Diagram of Main Structure Generated by TensorBoard
53

中国铁道科学研究院硕士学位论文
图 6.12 TensorBoard 生成的某一个网络的结构图 Figure 6.12 Diagram of Network Structure Generated by TensorBoard
图 6.13 TensorBoard 生成的某一网络的某一层的结构图 Figure 6.13 Diagram of Layer Structure of some Network Generated by TensorBoard
通过对 TensorBoard 的结果分析，DRL_Brain 类的结构与之前的深度强化学习核心算法 设计一致。
按照上文的算法设计和实现，DRL_Brain 类是深度强化学习算法的主要代码实现， 并且与具体业务无关，因此，在进行单元测试时可以选用简单的学习任务来测试该类的
54

算法实现、仿真与验证
学习效果。本次单元测试选择了强化学习领域中测试离散动作空间最经典的测试用例： 迷宫实验，如下图所示：
图 6.14 迷宫游戏示意图 Figure 6.14 Diagram of Maze Game
红色方块为 Agent，可以在 4  4 方格中自由移动，当 Agent 移动到黄色圆圈位置时获得 奖励 1，移动到黑色方块获得惩罚 1（奖励-1），训练的目的是寻找起始点到黄色圆圈的 最短路径，同时保证 Agent 不会走入黑色方块位置。环境的状态为 Agent 的横纵坐标， 动作共有上、下、左、右四种。该环境已有很多开源的实现例子，因此本次测试直接调 用了一个开源库作为强化学习任务的模拟环境。
测试一共迭代了 1000 个回合，每个回合不限步数，当 Agent 成功走入黄色圆圈或 黑色方块时此回合终止，每 300 步更新一次目标网络，每次学习抽取的样本数 batch_size 为 32，记忆库容量 memory_size 为 500。程序记录每一步迭代时的损失函数值和每个回 合完成预定目标所需的步数，实验结果如下：
图 6.15 损失函数值 Figure 6.15 Loss Function
55

中国铁道科学研究院硕士学位论文
图 6.16 每回合完成任务所需步数 Figure 6.16 Steps to Complete the Task in Every Episodes
单看每步的损失函数值，可以发现其一直在波动。按照监督学习的规律来看，损失函数 的值如果一直呈现下降趋势，意味着算法的计算值与真实值的差距逐渐减小，算法呈现 收敛趋势，一直波动则表明算法不收敛。但是结合每一回合要达到目标的步数可以发现， 在学习早期完成目标所需步数一直在波动，后来稳定到 4-5 步，算法在迭代了 250 个回 合左右时已经完成了该学习任务，并收敛到了最优路径，证明了算法的收敛性。考虑到 强化学习的训练过程，损失函数值的波动，恰恰体现了强化学习与监督学习在训练过程 方面最明显的区别：强化学习通过与环境的交互，一直在产生新的样本，而神经网络只 能基于既有样本进行拟合。由于本次学习设定的 batch_size 和 memory_size 都较小，在
学习的中后期记忆库中的状态-动作对已经基本被替换为最优状态，这时基于   贪心策
略随机生成的新动作可能会和记忆库中的记忆有较大差别，抽取的 batch 中若包含了新 样本，可能会使损失函数的值显著增大，当新样本加入记忆库后，后续迭代过程神经网 络基于新的训练集进行了拟合，因此损失函数值又逐渐下降。强化学习通过产生新样本 的方式，来提高神经网络的泛化能力，减少深度神经网络过拟合情况的发生。而常规的 监督学习是基于固定样本集的，必须采用其他的调整方法如提前终止训练、Dropout 方 法剔除部分神经元等来达到这一目的。
通过以上单元测试，DRL_Brain 类的学习效果得以验证。
56

算法实现、仿真与验证
6.3.2 单车组情况验证 在 DRL_Brain 类学习效果已经验证的基础上，需要对模拟环境进行单元测试。在
这一步中，选取动车所行车调度最简单的情况：单列动车组，作为环境的初始状态对仿 真软件的运行和对站场环境的模拟功能进行测试。
在编译器中以调试模式运行仿真环境，界面可以正常加载，没有异常事件抛出，通 过断点调试可以确认各配置文件读取正常，仿真软件运行正常。之后对单列动车组的情 况进行 1000 个回合的迭代学习，却出现了不收敛的情况，例如其中某一个神经网络的 学习结果如下：
图 6.17 每回合完成任务所需步数 Figure 6.17 Steps to Complete the Task in Every Episodes
回顾模拟环境的设计过程可以发现，为了降低神经网络的输出维度，在设计网络结构时 将动作空间划分为三类，并用三个神经网络进行对应，而奖励信号只有动车组能按时发 车这一种情况，所有奖励为 1 的动作都是发车对应的动作，因此理论上只有网络一的记 忆库中存在奖励为 1 的状态-动作对，由于网络二和网络三无法获取到奖励，获取不到收 敛方向，因此无法完成学习任务，由随机策略生成的状态-动作链中只有很小的概率会出 现按时发车的情况的，对于网络一而言奖励信号依旧非常稀少，同样无法收敛。
基于此，现对模拟环境的奖励规则进行细化，增加惩罚信号，并使部分奖励信号与
57

中国铁道科学研究院硕士学位论文
三个网络形成对应关系，通过奖励细化的方式提高学习前期的收敛速度。奖励与惩罚规 则如下：
(1) 进路不能排通的情况： 进路不能排通的动作奖励值都为-1：起始股道当前车组为空；动作所需咽喉被占用 或被预订；目的股道被占用或被预订；除此之外，为解决相邻网络的起始股道相同的情 况，起始股道当前车组的预订股道不为空，视为车组已在其他进路中，故也视为当前动 作的进路不能排通。 (2) 进路可以排通的情况： 进路可以排通的动作需要结合当前状态进行判断，可以以起始股道和目的股道的类 型将动作对应到三个网络，按照网络执行优先级顺序分别为：

起始股道 出入口 出入口 出入口 存车线 存车线 存车线 存车线

表 6.2 网络一对应动作奖励规则 Table 6.2 Reward Rules of the First Network

目的股道

车组状态

时间条件

存车线

待接车

接车时间>=当前时间

存车线

待接车

接车时间<当前时间

存车线

已发车

无

出入口

待作业

无

出入口

待发车

发车时间>当前时间

出入口

待发车

发车时间=当前时间

出入口

待发车

发车时间<当前时间

奖励值 1
t / T -1 -1
t / T 1
t / T

起始股道 洗车线 洗车线 洗车线 检修库 检修库 检修库

表 6.3 网络三对应动作奖励规则 Table 6.3 Reward Rules of the Third Network

目的股道

车组状态

时间条件

检修库

作业中

无

检修库

作业已完成

剩余检修时间>0

检修库

作业已完成

剩余检修时间=0

洗车线

作业中

无

洗车线

作业已完成

剩余洗车时间>0

洗车线

作业已完成

剩余洗车时间=0

58

奖励值 -1 1 -1 -1 1 -1

算法实现、仿真与验证

洗车线

镟轮线

作业中

无

-1

洗车线

镟轮线

作业已完成

剩余镟轮时间>0

1

洗车线

镟轮线

作业已完成

剩余镟轮时间=0

-1

镟轮线

洗车线

作业中

无

-1

镟轮线

洗车线

作业已完成

剩余洗车时间>0

1

镟轮线

洗车线

作业已完成

剩余洗车时间=0

-1

辅助股道 检修库 使用辅助股道

剩余检修时间>0

1

辅助股道 检修库 使用辅助股道

剩余检修时间=0

-1

检修库 辅助股道

作业中

无

-1

检修库 辅助股道 作业已完成

剩余洗车时间>0

-1

检修库 辅助股道 作业已完成

剩余洗车时间=0

1

辅助股道 镟轮线 使用辅助股道

剩余镟轮时间>0

1

辅助股道 镟轮线 使用辅助股道

剩余镟轮时间=0

-1

镟轮线 辅助股道

作业中

无

-1

镟轮线 辅助股道 作业已完成

剩余洗车时间>0

-1

镟轮线 辅助股道 作业已完成

剩余洗车时间=0

1

起始股道 存车线 存车线 存车线 洗车线 洗车线
洗车线
洗车线

表 6.4 网络二对应动作奖励规则 Table 6.4 Reward Rules of the Second Network

目的股道

车组状态

时间条件

洗车线

待作业

剩余洗车时间>0

洗车线

待作业

剩余洗车时间=0

洗车线

待发车

无

存车线 存车线 存车线 存车线

作业中 作业已完成 作业已完成 作业已完成

无
剩余检修时间=0 且剩余镟 轮时间=0
剩余检修时间>0 且检修库 线全部被占用或预订
剩余镟轮时间>0 且镟轮线 被占用或预订

奖励值 1 -1 -1 -1 1
1
1

59

中国铁道科学研究院硕士学位论文

除去以上三种情况外的所

洗车线

存车线

作业已完成

-1

有情况

剩余检修时间>0 且检修库

存车线 辅助股道

待作业

1

有空闲

剩余镟轮时间>0 且镟轮线

存车线 辅助股道

待作业

1

空闲

除去以上两种情况外的所

存车线 辅助股道

待作业

-1

有情况

存车线 辅助股道

待发车

无

-1

剩余检修时间=0 且剩余镟

辅助股道 存车线 使用辅助股道

1

轮时间=0

剩余检修时间>0 且检修库

辅助股道 存车线 使用辅助股道

1

线全部被占用或预订

剩余镟轮时间>0 且镟轮线

辅助股道 存车线 使用辅助股道

1

被占用或预订

除去以上三种情况外的所

辅助股道 存车线 使用辅助股道

-1

有情况

在进行以上奖励规则细化后再次进行训练，结果如下：

图 6.18 每回合完成任务所需步数 Figure 6.18 Steps to Complete the Task in Every Episodes
60

算法实现、仿真与验证

可见算法已经基本收敛，后期的一些跳动是由于   贪心策略增强了算法的探索性。在

学习完成后，以单个动车组作为输入状态对学习效果进行测试。本次测试的动车组初始

状态为： 接车线

表 6.5 单车组测试初始状态 Table 6.5 Initial Status of the Single-Train Test

发车线

接车时间

发车时间

剩余洗车时间

出入口 1

出入口 1

60(17:00)

228(7:00)

6(30min)

剩余镟轮时间 剩余检修时间 当前位置

当前时间

预订股道

0

36(180min)

站外 0

0

0

在完成训练以后，将初始状态直接输入算法，最终给出的全天计划为：

表 6.6 单车组测试结果 Table 6.6 Result of the Single-Train Test

时间点

起始股道

目的股道

动车组

占用咽喉

60(17:00)

出入口 1

存车线 6

0 号车

咽喉 4

63(17:15)

存车线 6

洗车线 17

0 号车

咽喉 15

70(17:50)

洗车线 17

检修库 25

0 号车

咽喉 19

107(20:55)

检修库 25

辅助股道 18

0 号车

咽喉 20

109(21:05)

辅助股道 18

存车线 7

0 号车

咽喉 15

228(7:00)

存车线 7

出入口 1

0 号车

咽喉 3

通过对计划的分析，该版计划能够按时完成接车与发车作业，并完成了动车组的各

项检修需求，算法能够完成动车所行车调度的基本任务。

6.3.3 多车组情况验证
对于多车组的情况，测试用例应尽量与动车所实际作业习惯保持一致，即接车作业 集中在傍晚到午夜，发车作业集中在凌晨到第二天上午，只有部分动车组需要进行检修 等作业，另一部分动车组仅完成存车作业即可。下面给出一个测试用例的初始状态，共 包含 8 列动车组：

61

中国铁道科学研究院硕士学位论文

接车线 出入口 1 剩余镟轮时间
0
接车线 出入口 2 剩余镟轮时间
0
接车线 出入口 1 剩余镟轮时间
0
接车线 出入口 1 剩余镟轮时间 36(180min)
接车线 出入口 2 剩余镟轮时间
0
接车线 出入口 1 剩余镟轮时间
0

表 6.7 多车组测试初始状态 Table 6.7 Initial Status of the Multiple-Train Test
0 号动车组

发车线

接车时间

发车时间

出入口 1

60(17:00)

228(7:00)

剩余检修时间 当前位置

当前时间

36(180min)

站外 0

0

1 号动车组

发车线

接车时间

发车时间

出入口 2

61(17:05)

228(7:05)

剩余检修时间 当前位置

当前时间

36(180min)

站外 0

0

2 号动车组

发车线

接车时间

发车时间

出入口 1

66(17:30)

216(6:00)

剩余检修时间 当前位置

当前时间

0

站外 0

0

3 号动车组

发车线

接车时间

发车时间

出入口 1

72(18:00)

234(7:30)

剩余检修时间 当前位置

当前时间

0

站外 0

0

4 号动车组

发车线

接车时间

发车时间

出入口 2

84(19:00)

228(7:00)

剩余检修时间 当前位置

当前时间

36(180min)

站外 0

0

5 号动车组

发车线

接车时间

发车时间

出入口 1

93(19:45)

240(8:00)

剩余检修时间 当前位置

当前时间

36(180min)

站外 0

0

62

剩余洗车时间 6(30min) 预订股道 0
剩余洗车时间 6(30min) 预订股道 0
剩余洗车时间 6(30min) 预订股道 0
剩余洗车时间 0
预订股道 0
剩余洗车时间 0
预订股道 0
剩余洗车时间 6(30min) 预订股道 0

算法实现、仿真与验证

6 号动车组

接车线

发车线

接车时间

发车时间 剩余洗车时间

出入口 2

出入口 2

98(20:10)

234(7:30)

0

剩余镟轮时间 剩余检修时间 当前位置

当前时间

预订股道

0

0

站外 0

0

0

7 号动车组

接车线

发车线

接车时间

发车时间 剩余洗车时间

出入口 2

出入口 2

120(22:00)

252(9:00)

0

剩余镟轮时间 剩余检修时间 当前位置

当前时间

预订股道

0

0

站外 0

0

0

在多车组的情况下，强化学习的任务目标变成了寻找能够保证所有动车组完成各项

检修作业需求并按时发车的最少步数的计划排布，任务的复杂度相较单车组情况有所提

高，因此达到收敛的回合数也大大提高。在训练完成后，将以上初始状态直接输入算法，

最终给出的全天计划为：

表 6.8 多车组测试结果 Table 6.8 Result of the Multiple-Train Test

时间点

起始股道

目的股道

动车组

占用咽喉

60(17:00)

出入口 1

存车线 9

0 号车

咽喉 3

61(17:05)

出入口 2

存车线 8

1 号车

咽喉 4

63(17:15)

存车线 9

洗车线 17

0 号车

咽喉 15

64(17:20)

存车线 8

辅助股道 18

1 号车

咽喉 16

66(17:30)

出入口 1

存车线 11

2 号车

咽喉 3

66(17:30)

辅助股道 18

检修库 25

1 号车

咽喉 19

71(17:55)

洗车线 17

检修库 21

0 号车

咽喉 19

72(18:00)

出入口 1

存车线 9

3 号车

咽喉 3

74(18:10)

存车线 11

洗车线 17

2 号车

咽喉 15

75(18:20)

存车线 9

辅助股道 18

3 号车

咽喉 15

77(18:30)

辅助股道 18

镟轮线 26

3 号车

咽喉 20

82(18:50)

洗车线 17

存车线 9

2 号车

咽喉 15

63

中国铁道科学研究院硕士学位论文

84(19:00)

出入口 2

存车线 8

4 号车

咽喉 4

87(19:15)

存车线 8

辅助股道 18

4 号车

咽喉 16

89(19:25)

辅助股道 18

检修库 22

4 号车

咽喉 20

93(19:45)

出入口 1

存车线 10

5 号车

咽喉 4

96(20:00)

存车线 10

洗车线 17

5 号车

咽喉 16

98(20:10)

出入口 2

存车线 12

6 号车

咽喉 4

104(20:40)

洗车线 17

检修库 23

5 号车

咽喉 19

105(20:50)

检修库 25

洗车线 17

1 号车

咽喉 19

109(21:05)

检修库 21

辅助股道 18

0 号车

咽喉 19

111(21:15)

辅助股道 18

存车线 7

0 号车

咽喉 15

114(21:30)

洗车线 17

存车线 10

1 号车

咽喉 16

114(21:30)

镟轮线 26

辅助股道 18

3 号车

咽喉 20

116(21:40)

辅助股道 18

存车线 5

3 号车

咽喉 15

120(22:00)

出入口 2

存车线 14

7 号车

咽喉 4

127(22:35)

检修库 22

辅助股道 18

4 号车

咽喉 20

129(22:45)

辅助股道 18

存车线 8

4 号车

咽喉 16

142(23:50)

检修库 23

辅助股道 18

5 号车

咽喉 19

144(0:00)

辅助股道 18

存车线 11

5 号车

咽喉 15

216(6:00)

存车线 9

出入口 1

2 号车

咽喉 3

228(7:00)

存车线 7

出入口 1

0 号车

咽喉 3

229(7:05)

存车线 10

出入口 2

1 号车

咽喉 4

234(7:30)

存车线 5

出入口 1

3 号车

咽喉 3

237(7:45)

存车线 12

出入口 2

6 号车

咽喉 4

240(8:00)

存车线 11

出入口 1

5 号车

咽喉 3

241(8:05)

存车线 8

出入口 2

4 号车

咽喉 4

252(9:00)

存车线 14

出入口 2

7 号车

咽喉 4

通过分析输出的结果，可以发现算法能够根据站场状态，自动编制整版行车调度计

划；通过对该版计划的分析，可以发现计划完成了所有动车组的各项作业需求，并按时

64

算法实现、仿真与验证
完成了发车作业，且动车组单项作业完成后在进路可以排通的情况下被尽快调离了作业 线，验证了网络优先级的设置和奖励规则设置的正确性。经过多次调整输入状态并测试， 算法都能够编制合适的调度计划，且通过对整版计划的分析可以发现，算法给出的计划 已经是能够保证所有动车组正点发车所需步数最少（相当于行车调度作业勾数最少）的 计划排布方式，在完成任务步数上已经达到了最优，验证了强化学习的策略优化过程， 即通过学习总结出能够完成既定多步任务所需步数最少的策略。 6.4 本章小结
本章主要介绍基于 DQN 的动车所行车调度智能化算法及仿真软件的代码实现。代 码充分利用 Python 语言、TensorFlow 框架和 PyQt 框架，并结合软件工程化的思想，首 先实现了深度强化学习算法单元，实现了算法与业务的分离，以便于后续仿真软件与其 他算法的兼容。然后，对仿真软件进行了设计实现。最后，对基于 DQN 的动车所行车 调度智能化算法的执行结果进行了分析，验证了算法的有效性。
65

中国铁道科学研究院硕士学位论文
66

7 总结与展望

总结与展望

7.1 总结
得益于计算机运算能力的增强和深度神经网络的发展，深度学习将人工智能的发展 推向了一个新的高度。以强化学习为核心，深度学习为工具的深度强化学习，极大地推 动了控制决策智能化算法的进步。
本文是深度强化学习算法在工程应用领域的一次探索与实践，并首次将机器学习算 法应用于铁路行车调度领域，文中的设计方案和设计思想可为以后的研究提供参考。本 文按照工程项目研发的一般流程，首先对动车所行车调度业务进行了详细分析和算法需 求设计，在完成算法选型后，提出了一种基于深度强化学习的动车所行车调度计划编制 智能化算法，并对算法和仿真软件进行了设计、编程实现与实验。通过分析实验结果， 可以得出如下结论： (1) 深度强化学习核心算法可以较好地完成学习任务；仿真软件可以正常运行并对站场
环境进行模拟，仿真软件设计中的工程化思想得到验证。 (2) 基于深度强化学习的动车所行车调度计划智能化编制算法，能够基于动车所的当前
状态和各动车组的检修作业需求，自动编制合适的接发车和调车计划，且通过对计 划的分析，整版计划已经达到了完成任务理论上最少的行车调度作业勾数，解决了 本文所提出的动车所行车调度业务的 NP-Hard 问题和站场设备的时空占用相容性问 题，算法的可行性和正确性得到验证。 (3) 在多车组的情况下，算法虽然需要较长时间的学习才能收敛到理想状态，但在完成 学习以后，算法能够在很短的时间内根据当前状态生成整版行车调度计划，解决了 本文所提出的意外状况下的计划动态生成问题。

7.2 展望
受科研能力和时间所限，本文仍旧存在一些不足，在以下方面还可以进行进一步的 优化研究： (1) 学习效率。算法中虽然采用了奖励细化规则来加速学习前期的收敛速度，但是为了
让算法达到非常理想的收敛状态，仍需要较长的训练时间，且这个时间会随着站场 规模的扩大而增长。 (2) 硬件利用率。强化学习的样本集通常较小，训练过程中 GPU 使用率不高，造成了硬

67

中国铁道科学研究院硕士学位论文
件计算能力的浪费。可以尝试使用多进程或多实例异步训练（如 A3C 算法）技术提 高硬件计算能力的利用率，从而提升整体训练效率。 (3) 对站场特殊业务需求的适配。由于动车所行车调度业务的特殊性，有些计划的执行 可能会存在特殊需求，如某些检修作业需要由特定检修库线完成等等，对于这些“特 例”，如何做到算法的适配和提高适配效率，也是今后应当考虑的问题。
68

参考文献
参考文献
[1]付紫彪.动车段(所)控制集中系统进路控制的智能优化研究[D].中国铁道科学研究院,2016. [2]安琪. 动车组运用计划的优化方法研究[D].中国铁道科学研究院,2013. [3]国 家 发 展 改 革 委 员 会 . 《 中 长 期 铁 路 网 规 划 》 [EB/OL]. 发 改 基 础 [2016]1536 号 ,[2016-07-
13].http://www.ndrc.gov.cn/zcfb/zcfbghwb/201607/t20160720_813863.html. [4]国家发展改革委员会.《铁路“十三五”发展规划》[EB/OL].发改基础[2017]1996 号,[2017-11-
20].http://www.ndrc.gov.cn/gzdt/201711/t20171124_867822.html. [5]曹桂均,张华.动车基地调度集中系统研究[J].中国铁路,2012(04):55-59. [6]杨勇.铁路调度集中系统的现状与发展策略[J].上海铁道科技,2008(01):97-98. [7]崔艳萍,庄河.国内外铁路调度集中系统的差异性分析[J].铁道运输与经济,2013,35(04):16-21. [8]张 锐 . 国 内 外 铁 路 信 号 现 状 、 差 距 对 比 和 我 国 铁 路 信 号 发 展 方 向 的 思 考 [J]. 铁 道 标 准 设
计,2004(07):117-120. [9]李凯. 高速铁路调度指挥系统结构分析[D].西南交通大学,2015. [10]王瑞斌. 高速铁路调度指挥安全保障体系研究[D].中国铁道科学研究院,2014. [11]张 惟 皎 , 史 天 运 , 陈 彦 . 动 车 运 用 所 存 车 线 运 用 方 案 优 化 模 型 与 算 法 [J]. 中 国 铁 道 科
学,2013,34(01):121-125. [12]曹 桂 均 , 闫 石 . 动 车 段 ( 所 ) 集 中 控 制 系 统 作 业 进 路 方 案 冲 突 检 测 的 方 法 [J]. 中 国 铁 道 科
学,2016,37(02):106-113. [13]陈韬,吕红霞,潘金山,赵敬勇.动车所通过能力计算模型与方法研究[J].铁道学报,2016,38(09):9-17. [14]郭 小 乐 , 宋 瑞 , 黎 浩 东 , 陈 胜 波 , 刘 星 材 . 动 车 运 用 所 调 车 作 业 计 划 编 制 优 化 [J]. 中 国 铁 道 科
学,2016,37(01):117-123. [15]王家喜,林柏梁,王忠凯,李建.动车所调车作业计划优化模型及求解算法[J].交通运输系统工程与信
息,2016,16(06):122-127+141. [16]王忠凯. 动车组运用检修计划优化方法的研究[D].中国铁道科学研究院,2012. [17]童 佳 楠 , 聂 磊 , 贺 振 欢 . 基 于 遗 传 算 法 的 动 车 运 用 所 一 级 检 修 作 业 计 划 优 化 [J]. 铁 道 运 输 与 经
济,2016,38(08):59-65. [18]王忠凯,史天运,张惟皎,王辉. 动车组调车计划智能辅助编制平台的研究[A]. 中国智能交通协会.
第十届中国智能交通年会优秀论文集[C].中国智能交通协会:,2015:8. [19]陈钇似. 基于机器学习的游戏智能系统研究与应用[D].电子科技大学,2017.
69

中国铁道科学研究院硕士学位论文
[20]李晨溪,曹雷,张永亮,陈希亮,周宇欢,段理文.基于知识的深度强化学习研究综述[J].系统工程与电 子技术,2017,39(11):2603-2613.
[21]宋超峰. 基于平均型强化学习算法的动态调度方法的研究[D].天津大学,2006. [22]赵冬斌,邵坤,朱圆恒,李栋,陈亚冉,王海涛,刘德荣,周彤,王成红.深度强化学习综述:兼论计算机围
棋的发展[J].控制理论与应用,2016,33(06):701-717. [23]姜新猛. 基于 TensorFlow 的卷积神经网络的应用研究[D].华中师范大学,2017. [24]Mnih V, Kavukcuoglu K, Silver D, et al. Playing Atari with Deep Reinforcement Learning[J]. Computer
Science, 2013. [25]Van Hasselt H, Guez A, Silver D. Deep Reinforcement Learning with Double Q-learning[J]. Computer
Science, 2015. [26]Silver D, Lever G, Heess N, et al. Deterministic policy gradient algorithms[C]// International Conference
on International Conference on Machine Learning. JMLR.org, 2014:387-395. [27]Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement learning[J]. Computer
Science, 2015, 8(6):A187. [28]Mnih V, Badia A P, Mirza M, et al. Asynchronous Methods for Deep Reinforcement Learning[J]. 2016. [29]刘全,翟建伟,章宗长,钟珊,周倩,章鹏,徐进.深度强化学习综述[J].计算机学报,2018,41(01):1-27. [30]曹桂均. 编组站综合自动化系统控制技术及其扩展应用的研究[D].中国铁道科学研究院,2013. [31]张雪松,马亮.基于约束规划的编组站阶段作业计划优化研究[J].铁路计算机应用,2012,21(09):1-
4+9. [32]黄康. 基于粗糙集理论的行车指挥知识获取与决策的应用研究[D].铁道部科学研究院,2003. [33]张英贵,雷定猷,刘明翔.铁路车站股道运用排序模型与算法[J].中国铁道科学,2010,31(02):96-100. [34]林 炳 跃 , 曹 桂 均 , 寇 亚 洲 . 动 车 段 ( 所 ) 集 中 控 制 系 统 自 动 进 路 指 令 的 展 示 及 调 整 [J]. 中 国 铁
路,2016(02):42-45. [35]中国铁路总公司.动车段(所)控制集中系统暂行技术条件[S].北京:中国铁路总公司运输局.2016 [36]曹 桂 均 , 刘 青 . 在 动 车 运 用 所 实 现 动 车 组 位 置 自 动 追 踪 及 调 度 集 中 的 解 决 方 案 [J]. 中 国 铁
路,2013(03):51-54. [37]张辉. 基于深度强化学习的主动人脸感知技术研究[D].山东大学,2017. [38]周志华. 机器学习[M]. 清华大学出版社, 2016. [39]王雨辰. 基于深度学习的图像识别与文字推荐系统的设计与实现[D].北京交通大学,2017. [40]刘全,翟建伟,钟珊,章宗长,周倩,章鹏.一种基于视觉注意力机制的深度循环 Q 网络模型[J].计算机
70

参考文献 学报,2017,40(06):1353-1366. [41]曹东岩. 基于强化学习的开放领域聊天机器人对话生成算法[D].哈尔滨工业大学,2017. [42]丁乐乐. 基于深度学习和强化学习的车辆定位与识别[D].电子科技大学,2016. [43]邱立威. 深度强化学习在视频游戏中的应用[D].华南理工大学,2015. [44]刘晓平,杜琳,石慧.基于 Q 学习的任务调度问题的改进研究[J].图学学报,2012,33(03):11-16. [45]王超,郭静,包振强.改进的 Q 学习算法在作业车间调度中的应用[J].计算机应用,2008,28(12):32683270. [46]陈雪江. 基于强化学习的多机器人协作机制研究[D].浙江工业大学,2004. [47]Sutton R S,Barto A G. Reinforcement Learning: An Intro duction. Cambridge, USA: MIT Press,1998. [48]Baldeaux J, Platen E. Monte Carlo and Quasi-Monte Carlo Methods[J]. Acta Numerica, 2013, 7(2):1-49. [49]Mcclure S M, Berns G S, Montague P R. Temporal prediction errors in a passive learning task activate human striatum.[J]. Neuron, 2003, 38(2):339-346. [50]Li Y. Deep Reinforcement Learning: An Overview[J]. 2017. [51]Watkins C J C H, Dayan P. Technical Note: Q-Learning[J]. Machine Learning, 1992, 8(3-4):279-292. [52]顾明珠. 基于深度强化学习的大规模自适应服务组合问题研究[D].东南大学,2017. [53]石征锦,王康.深度强化学习在 Atari 视频游戏上的应用[J].电子世界,2017(16):105-106+109. [54]Singh S P, Sutton R S. Reinforcement learning with replacing eligibility traces[M]. Kluwer Academic Publishers, 1996. [55]Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540):529. [56]Lample G, Chaplot D S. Playing FPS Games with Deep Reinforcement Learning[J]. 2016. [57]Abadi M. TensorFlow: learning functions at scale[J]. Acm Sigplan Notices, 2016, 51(9):1-1. [58]李宁. 基于机器学习算法的 IGBT 模块故障预测技术研究[D]. 北京交通大学, 2017. [59]郑人杰. 《软件工程——实践者的研究方法》[J]. 计算机教育, 2007, No.(2s):80-80.
71

中国铁道科学研究院硕士学位论文
72

附录 1 作者简历及科研成果清单
附录 1 作者简历及科研成果清单

一、作者简历 姓名：曹达 性别：男 民族：汉 出生年月：1992-01-01 籍贯：辽宁省沈阳市 2011.09-2015.06 北京航空航天大学 飞行器设计与工程专业 工学学士 2015.09-2018.06 中国铁道科学研究院 交通信息工程及控制专业 工学硕士 二、攻读学位期间科研成果

一、参加科研项目

序号 1 2

项目名称
动车段（所）控制集中系统 关键数据智能测试技术的研
究 动车段（所）集中控制仿真
测试系统的研究

项目来源
院基金课题 所基金课题

项目下达编号
1651TH6804 1652TH0704

本人承担的任务
软件测试 软件测试

二、发表学术论文

序号

发 表 论 文 名 称

《道岔融雪控制系统典型问题
1
分析与优化设计》

发表时间 2017.11.30

发表刊物、会议 名称
《铁道标准设 计》

ISSN 刊 物 号 或 SCI、EI、ISTP 检 索号
1004-2954

作者排名 1

三、获得各类科技奖项

序号

获奖项目

获奖等级

获奖时间

一种基于统计的动车段/所接发

1

专利

2016.03.18

车作业股道自动分配方法

动车段/所检修计划正点率的统

2

专利

2016.04.12

计方法

证书号/ 专利号

颁奖部门

201 6101582 09X 2016102 248602

国家知识产权局 国家知识产权局

（说明：此表如一页未填完可续页）

73

中国铁道科学研究院硕士学位论文
74

附录 2 学位论文数据集
附录 2 学位论文数据集

作者姓名

曹达

关键词（3 个）
动车所，行车调度， 深度强化学习

密级（ 内部、公 开）
公开

学位授予单位

学位授予单位代码

学号 中图分类号
U292.13 学位类别

S1523

UDC
656. 2

论文选题来源 （国家级、省部级、自
选）
自选

学位级别（博士、硕士）

中国铁道科学研究院

83801

学术学位

硕士

前置学位授予单位 前置学位授予时间 前置学位证书编号

前置学位授予专业

北京航空航天大学

2015.07

1000642015000928

飞行器设计与工程

论文中文题目

论文英文题名

论文语种

基于深度强化学习的动车所行车调度计划编 制智能化研究

Research on the Intelligentization of Train Dispatching Plan Generating for the EMU Depot Based on Deep Reinforcement Learning

中文

培养单位名称

中国铁道科学研究院 培养单位地址 北京市海淀区大柳树路 2 号

培养单位代码

83801

邮编

100081

导师姓名

张雪松

导师职称

研究员

答辩委员会主席

沃华欧

论文评阅人

答辩委员会成员

曹玉，曹桂均，臧永 立，张华

曹玉，沃华欧，张华

电子版论文提交格式 论文总页数

推荐格式：MSword 或 PDF 85 页

75

